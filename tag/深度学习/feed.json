{
    "version": "https://jsonfeed.org/version/1",
    "title": "千里稻花应秀色 • All posts by \"深度学习\" tag",
    "description": "blogs by SSR",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/2022/04/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/",
            "url": "http://example.com/2022/04/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/",
            "title": "PyTorch深度学习实践",
            "date_published": "2022-04-01T03:40:59.000Z",
            "content_html": "<h1 id=\"pytorch深度学习实践\"><a class=\"markdownIt-Anchor\" href=\"#pytorch深度学习实践\"></a> PyTorch深度学习实践</h1>\n<h2 id=\"第1章-深度学习基础\"><a class=\"markdownIt-Anchor\" href=\"#第1章-深度学习基础\"></a> 第1章 深度学习基础</h2>\n<h3 id=\"11-人工智能-机器学习与深度学习\"><a class=\"markdownIt-Anchor\" href=\"#11-人工智能-机器学习与深度学习\"></a> 1.1 人工智能、机器学习与深度学习</h3>\n<h4 id=\"111-人工智能简介\"><a class=\"markdownIt-Anchor\" href=\"#111-人工智能简介\"></a> 1.1.1 人工智能简介</h4>\n<p>人工智能：是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。<br />\n人工智能的目的：就是让计算机能够像人一样思考。<br />\n<strong>人工智能、机器学习与深度学习的关系如下图：</strong></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403154446384.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403154446384.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403154446384\" /><br />\n<strong>人工智能三种形态：</strong></p>\n<ul>\n<li>\n<p>弱人工智能：单个方面的人工智能。目前，主流科研集中在弱人工智能上。并且一般认为这一研究领域已经取得可观的成就。</p>\n</li>\n<li>\n<p>强人工智能：使机器学习人的理解、学习和执行任务的能力，不仅能真正推理和解决问题的智能机器，还具有知觉的或自我意识。</p>\n</li>\n<li>\n<p>超人工智能：超越人类智慧并且将人类智慧延展的智能体系，各方面都可以比人类强。</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403154713334.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403154713334.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403154713334\" /></p>\n</li>\n</ul>\n<h4 id=\"112-机器学习简介\"><a class=\"markdownIt-Anchor\" href=\"#112-机器学习简介\"></a> 1.1.2 机器学习简介</h4>\n<p>机器学习的广义概念：是指从已知数据中获得规律，并利用规律对未知数据进行预测的方法。<br />\n机器学习可用于：自然语言处理、图像识别、生物信息学以及风险预测等，已在工程学、经济学以及心理学等多个领域。<br />\n机器学习是一种统计学习方法，机器人和计算机等机器需要使用大量数据进行学习，从而提取出所需的信息。<br />\n机器学习的任务，就是要在基于大数据量的基础上，发掘其中蕴含并且有用的信息。<br />\n** 机器学习的分类**</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403155001955.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403155001955.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403155001955\" /><br />\n机器学习主要分为有监督学习（也称监督学习）和无监督学习两种。<br />\n监督学习需要为机器提供一组标记数据。有监督学习通过训练，从标记数据中提取通用信息或特征信息， 以此得到预测模型。<br />\n监督学习的两种主要类型是分类和回归。<br />\n无监督学习两种主要类型是聚类和降维。</p>\n<h4 id=\"113-深度学习简介\"><a class=\"markdownIt-Anchor\" href=\"#113-深度学习简介\"></a> 1.1.3 深度学习简介</h4>\n<p><strong>深度学习定义：</strong><br />\n就是一种利用深度人工神经网络来进行自动分类、预测和学习的技术。<br />\n深度学习的本质就是一个深层神经网络。深度学习的基本思想就是对堆叠多个层，将上一层的输出作为下一层的输入，逐步实现对输入信息的分级表达，让程序从中自动学习深入、抽象的特征。尤其值得注意的是“深度学习减少了人为干预，而这恰恰保留了数据客观性，因此可以提取出更加准确的特征”。深度学习的训练过程：</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403155216197.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403155216197.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403155216197\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403155316397.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403155316397.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403155316397\" /></p>\n<p><strong>机器学习与深度学习流程对比</strong><br />\n深度学习和传统机器学习在流程上的差异：深度学习算法可以从数据中学习更加复杂的特征表达，使得最后一步权重学习变得更加简单且有效。</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403155625784.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403155625784.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403155625784\" /><br />\n<strong>深度学习发展历史</strong></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403160006153.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403160006153.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403160006153\" /></p>\n<h3 id=\"12-深度学习的三大核心要素\"><a class=\"markdownIt-Anchor\" href=\"#12-深度学习的三大核心要素\"></a> 1.2 深度学习的三大核心要素</h3>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403155909130.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403155909130.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403155909130\" /></p>\n<p><strong>大数据</strong></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403160223573.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403160223573.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403160223573\" /></p>\n<p><strong>深度网络架构</strong><br />\n所谓的深度网络架构，就是整个网络体系的构建方式和拓扑连接结构.<br />\n目前最常用的有4种：</p>\n<ul>\n<li>全连接网络FC、</li>\n<li>卷积神经网络CNN、</li>\n<li>循环神经网络RNN</li>\n<li>生成对抗网络GAN。</li>\n</ul>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403160247799.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403160247799.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403160247799\" /></p>\n<p><strong>高性能的计算力</strong><br />\nGPU</p>\n<h3 id=\"13-神经元与深度神经网络\"><a class=\"markdownIt-Anchor\" href=\"#13-神经元与深度神经网络\"></a> 1.3 神经元与深度神经网络</h3>\n<p><strong>神经元模型</strong><br />\n人体的神经元包含树突、细胞核和轴突。</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403160529805.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403160529805.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403160529805\" /></p>\n<h3 id=\"14-神经网络中常用的激励函数\"><a class=\"markdownIt-Anchor\" href=\"#14-神经网络中常用的激励函数\"></a> 1.4 神经网络中常用的激励函数</h3>\n<p>如果每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合。<br />\n<strong>激活函数</strong><br />\n激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。</p>\n<ol>\n<li>Sigmoid 函数</li>\n</ol>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403160821953.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403160821953.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403160821953\" /></p>\n<ol start=\"2\">\n<li>tanh 函数</li>\n</ol>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403160903804.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403160903804.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403160903804\" /></p>\n<ol start=\"3\">\n<li>ReLU函数</li>\n</ol>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403160951910.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403160951910.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403160951910\" /></p>\n<ol start=\"4\">\n<li>指数线性单元 ELU函数</li>\n</ol>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403161029538.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403161029538.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403161029538\" /></p>\n<ol start=\"5\">\n<li>Leaky ReLU 渗漏型整流线性单元激活函数</li>\n</ol>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403161144933.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403161144933.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403161144933\" /></p>\n<ol start=\"6\">\n<li>Maxout 函数</li>\n</ol>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403161216101.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403161216101.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403161216101\" /></p>\n<h3 id=\"15-深度学习的优势\"><a class=\"markdownIt-Anchor\" href=\"#15-深度学习的优势\"></a> 1.5 深度学习的优势</h3>\n<ol>\n<li>不用再提取特征</li>\n<li>处理线性不可分的能力强。</li>\n</ol>\n<h3 id=\"16-常见的深度学习框架\"><a class=\"markdownIt-Anchor\" href=\"#16-常见的深度学习框架\"></a> 1.6 常见的深度学习框架</h3>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403161359277.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403161359277.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403161359277\" /></p>\n<ol>\n<li>PyTorch<br />\n2017年1月Facebook开源Torch库的Python版本（由Lua语言编写）。<a href=\"https://pytorch.org/\">https://pytorch.org/</a></li>\n<li>TensorFlow<br />\n2015年9月谷歌大脑开源的深度学习框架</li>\n</ol>\n<h3 id=\"练习\"><a class=\"markdownIt-Anchor\" href=\"#练习\"></a> 练习</h3>\n<h2 id=\"第2章-深度学习框架pytorch的安装\"><a class=\"markdownIt-Anchor\" href=\"#第2章-深度学习框架pytorch的安装\"></a> 第2章 深度学习框架PyTorch的安装</h2>\n<h3 id=\"21-pytorch-介绍\"><a class=\"markdownIt-Anchor\" href=\"#21-pytorch-介绍\"></a> 2.1 PyTorch 介绍</h3>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403162522814.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403162522814.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403162522814\" /></p>\n<h3 id=\"22-windows下pytorch-深度学习环境的配置\"><a class=\"markdownIt-Anchor\" href=\"#22-windows下pytorch-深度学习环境的配置\"></a> 2.2 Windows下PyTorch 深度学习环境的配置</h3>\n<p><strong>安装Python</strong><br />\n<strong>pip安装torch</strong></p>\n<ol>\n<li>登录PyTorch官网安装</li>\n</ol>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403162709713.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403162709713.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403162709713\" /></p>\n<ol start=\"2\">\n<li>点击Get Started</li>\n</ol>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403162730246.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403162730246.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403162730246\" /></p>\n<p>3．复制命令到终端执行</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install torch==1.8.0+cpu torchvision==0.9.0+cpu torchaudio===0.8.0 -f https://download.pytorch.org/whl/torch_stable.html </span><br></pre></td></tr></table></figure>\n<p>4．安装torchvision<br />\npip3 install torchvision</p>\n<h3 id=\"23-linux下pytorch-深度学习环境的配置\"><a class=\"markdownIt-Anchor\" href=\"#23-linux下pytorch-深度学习环境的配置\"></a> 2.3 Linux下PyTorch 深度学习环境的配置</h3>\n<p>同上</p>\n<h3 id=\"24-pytorch开发工具\"><a class=\"markdownIt-Anchor\" href=\"#24-pytorch开发工具\"></a> 2.4 PyTorch开发工具</h3>\n<p>PyCharm不多说，或者Notebook</p>\n<h2 id=\"第3章-pytorch基础\"><a class=\"markdownIt-Anchor\" href=\"#第3章-pytorch基础\"></a> 第3章 PyTorch基础</h2>\n<h3 id=\"31张量是什么\"><a class=\"markdownIt-Anchor\" href=\"#31张量是什么\"></a> 3.1张量是什么</h3>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403163251961.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403163251961.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403163251961\" /></p>\n<p>PyTorch处理的最基本操作对象就是张量，张量的英文是Tensor，表示的是一个多维的矩阵。</p>\n<p>零阶张量就是一个数，一阶张量就是向量，二阶张量就是一般的矩阵，多阶张量就相当于一个多维的数组。</p>\n<p><strong>张量的3个属性：</strong></p>\n<ol>\n<li>阶（rank）：维数。</li>\n<li>形状（shape）： 行和列的数目。</li>\n<li>类型（type）： 元素的数据类型。</li>\n</ol>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403163432216.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403163432216.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403163432216\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403163524553.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403163524553.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403163524553\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403163540349.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403163540349.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403163540349\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403163553803.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403163553803.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403163553803\" /></p>\n<h3 id=\"32-tensor的创建\"><a class=\"markdownIt-Anchor\" href=\"#32-tensor的创建\"></a> 3.2 Tensor的创建</h3>\n<p><strong>创建给定元素值的Tensor</strong><br />\n【例3.1】假设要创建一个32位浮点数的Tensor,其值是矩阵[[-1,-2],[3,4],[5,6]]，</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\">a = torch.FloatTensor([[-<span class=\"number\">1</span>, -<span class=\"number\">2</span>], [<span class=\"number\">3</span>, <span class=\"number\">4</span>], [<span class=\"number\">5</span>, <span class=\"number\">6</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;a:&#x27;</span>, a)</span><br><span class=\"line\">b = torch.Tensor([[-<span class=\"number\">1</span>, -<span class=\"number\">2</span>], [<span class=\"number\">3</span>, <span class=\"number\">4</span>], [<span class=\"number\">5</span>, <span class=\"number\">6</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;b:&#x27;</span>, b)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403163813264.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403163813264.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403163813264\" /></p>\n<p>【例3.2】查看的Tensor的尺寸,属性类型和元素数量，</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> t</span><br><span class=\"line\">b = t.Tensor([[<span class=\"number\">1</span>, <span class=\"number\">2</span>], [<span class=\"number\">3</span>, <span class=\"number\">4</span>], [<span class=\"number\">5</span>, <span class=\"number\">6</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(b)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;b.size(): &quot;</span>, b.size())</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;b.shape:  &quot;</span>, b.shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;b.type(): &quot;</span>, b.<span class=\"built_in\">type</span>())</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;b.dtype:  &quot;</span>, b.dtype)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;b.numel():&quot;</span>, b.numel())</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403163921660.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403163921660.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403163921660\" /></p>\n<p>【例3.3】可以在创建时给Tensor直接赋值，也可以先创建一个未赋值的Tensor。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> t</span><br><span class=\"line\">c = t.FloatTensor(<span class=\"number\">3</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">d = t.Tensor(<span class=\"number\">3</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(c)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(d)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403164104717.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403164104717.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403164104717\" /></p>\n<p>【例3.4】创建一个和给定的Tensor形状一样的新Tensor</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> t</span><br><span class=\"line\">d = t.Tensor(<span class=\"number\">3</span>,<span class=\"number\">2</span>) </span><br><span class=\"line\">e = t.Tensor(d.size())</span><br><span class=\"line\"><span class=\"built_in\">print</span>(e)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403164156560.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403164156560.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403164156560\" /></p>\n<p>其他创建方法</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a = torch.zeros(<span class=\"number\">2</span>,<span class=\"number\">3</span>)  <span class=\"comment\">#返回全零Tensor</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;a:&quot;</span>,a)</span><br><span class=\"line\">b = torch.ones_like(a)   <span class=\"comment\">#返回shape和参数一样的全1Tensor, zeros_like类似</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;b:&quot;</span>,b)</span><br><span class=\"line\">c = torch.arange(<span class=\"number\">0</span>, <span class=\"number\">10</span>, <span class=\"number\">1</span>)  <span class=\"comment\">#torch.arange(start=0, end, step=1)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;c:&quot;</span>,c)</span><br><span class=\"line\">e = torch.full((<span class=\"number\">2</span>,<span class=\"number\">3</span>), <span class=\"number\">2</span>)  <span class=\"comment\">#初始化指定值的Tensor</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;e:&quot;</span>,e)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403164303006.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403164303006.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403164303006\" /></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#************随机初始化***********</span></span><br><span class=\"line\">a = torch.rand((<span class=\"number\">2</span>,<span class=\"number\">3</span>))   <span class=\"comment\">#初始化为[0,1)内的均匀分布随机数</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;a:&quot;</span>,a)</span><br><span class=\"line\">b = torch.rand_like(a)  <span class=\"comment\">#初始化为[0,1)内的均匀分布随机数，不过shape与参数相同</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;b:&quot;</span>,b)</span><br><span class=\"line\">torch.randn(<span class=\"number\">2</span>,<span class=\"number\">3</span>)  <span class=\"comment\">#返回标准正态分布(0,1)的随机数</span></span><br><span class=\"line\">c = torch.normal(torch.randn(<span class=\"number\">2</span>,<span class=\"number\">3</span>), torch.randn(<span class=\"number\">2</span>,<span class=\"number\">3</span>))  <span class=\"comment\">#torch.normal(mean, std, out=None)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;c:&quot;</span>,c)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403164422605.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403164422605.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403164422605\" /></p>\n<h3 id=\"33-tensor的调整形状操作\"><a class=\"markdownIt-Anchor\" href=\"#33-tensor的调整形状操作\"></a> 3.3 Tensor的调整形状操作</h3>\n<p>【例3.5】创建一个一阶张量，长度为6，元素为[0,1,2,3,4,5],使用torch.view()函数将其调整为二阶张量2*3的Tensor。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> t</span><br><span class=\"line\">a = t.arange(<span class=\"number\">0</span>,<span class=\"number\">6</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;原&quot;</span>,a)</span><br><span class=\"line\">b = a.view(<span class=\"number\">2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;新&quot;</span>,b)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(b.shape)</span><br><span class=\"line\">b = a.view(<span class=\"number\">3</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">b[<span class=\"number\">2</span>][<span class=\"number\">1</span>] =<span class=\"number\">10</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;b = &quot;</span>,b)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;a = &quot;</span>,a)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## 注：通过tensor.view()方法可以调整tensor的形状,但必须保证调整前后元素总数一致。view不会修改自身的数据,不会修改原形状，返回的新tensor与源tensor共享内存。</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403164752807.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403164752807.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403164752807\" /></p>\n<p>【例3.6】改变形状的其他方法torch.resize_(), torch.reshape()。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> t</span><br><span class=\"line\">a = t.arange(<span class=\"number\">0</span>,<span class=\"number\">6</span>); <span class=\"built_in\">print</span>(<span class=\"string\">&quot;a = &quot;</span>,a)</span><br><span class=\"line\">b = a.view(<span class=\"number\">2</span>,<span class=\"number\">3</span>) ;  <span class=\"built_in\">print</span>(<span class=\"string\">&quot;b = &quot;</span>,b,<span class=\"string\">&quot;\\n&quot;</span>)</span><br><span class=\"line\">f = a.reshape(<span class=\"number\">2</span>,<span class=\"number\">3</span>); b[<span class=\"number\">1</span>][<span class=\"number\">2</span>] =<span class=\"number\">10</span>;</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;f = &quot;</span>,f); </span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;a = &quot;</span>,a,<span class=\"string\">&quot;\\n&quot;</span>)</span><br><span class=\"line\">f = t.reshape(a,(<span class=\"number\">2</span>,<span class=\"number\">3</span>));</span><br><span class=\"line\">c = a.resize_(<span class=\"number\">2</span>,<span class=\"number\">3</span>); <span class=\"built_in\">print</span>(<span class=\"string\">&quot;c = &quot;</span>,c)</span><br><span class=\"line\">d = a.resize_(<span class=\"number\">1</span>,<span class=\"number\">3</span>); </span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;d = &quot;</span>,d); <span class=\"built_in\">print</span>(<span class=\"string\">&quot;a = &quot;</span>,a)</span><br><span class=\"line\">e = a.resize_(<span class=\"number\">3</span>,<span class=\"number\">3</span>); <span class=\"built_in\">print</span>(<span class=\"string\">&quot;e = &quot;</span>,e)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## resize_()会修改a的shape，reshape()返回修改后的tensor，但不会更改a的shape</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403164956841.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403164956841.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403164956841\" /></p>\n<h3 id=\"34-tensor的运算\"><a class=\"markdownIt-Anchor\" href=\"#34-tensor的运算\"></a> 3.4 Tensor的运算</h3>\n<p>【例3.7】 对 Tensor的加、减、乘、除、取绝对值操作。</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403165048053.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403165048053.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403165048053\" /></p>\n<h3 id=\"35-tensor的比较操作\"><a class=\"markdownIt-Anchor\" href=\"#35-tensor的比较操作\"></a> 3.5 Tensor的比较操作</h3>\n<p>Tensor常用的比较函数有很多，比如</p>\n<ul>\n<li>torch.equal( )、</li>\n<li>torch.eq( )、</li>\n<li><a href=\"http://torch.gt\">torch.gt</a>( )、</li>\n<li><a href=\"http://torch.lt\">torch.lt</a>( )、</li>\n<li><a href=\"http://torch.ge\">torch.ge</a>( )、</li>\n<li>torch.le( )、</li>\n<li><a href=\"http://torch.ne\">torch.ne</a>( )、</li>\n<li>torch.topk( )、</li>\n<li>torch.sort( )<br />\n等等，这里简单介绍几个。</li>\n</ul>\n<p><strong>torch.equal( )</strong></p>\n<p>若两个Tensor具有相同的形状和元素，则返回True,否则返回False。</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403165258051.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403165258051.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403165258051\" /></p>\n<p><strong><a href=\"http://torch.gt\">torch.gt</a>( )</strong></p>\n<p>逐元素比较input和other， 若input元素严格大于other元素，则返回True,否则返回False。</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403165425079.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403165425079.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403165425079\" /></p>\n<h3 id=\"36-tensor的数理统计操作\"><a class=\"markdownIt-Anchor\" href=\"#36-tensor的数理统计操作\"></a> 3.6 Tensor的数理统计操作</h3>\n<p>Tensor中的求最小值、最大值、均值、累加、累积等的操作比如：torch.min( )、torch.max( )、torch.mean( )等。<br />\n<strong>torch.max( )、torch.mean( )</strong></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403165541364.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403165541364.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403165541364\" /></p>\n<h3 id=\"37-tensor与numpy的互相转换操作\"><a class=\"markdownIt-Anchor\" href=\"#37-tensor与numpy的互相转换操作\"></a> 3.7 Tensor与Numpy的互相转换操作</h3>\n<p>NumPy(Numerical Python) 是 Python 语言的一个扩展程序库，支持大量的维度数组与矩阵运算，此外也针对数组运算提供大量的数学函数库。它提供了一个多维数组（ndarray）数据类型，以及关于多维数组的操作，NumPy 已经成为其他大数据和机器学习模块的基础。</p>\n<p>Tensor类似于Numpy的ndarray，但ndarray 不支持GPU运算，而Tensor支持。</p>\n<p>Tensor与Numpy可以方便的互相转换。</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403165654101.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403165654101.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403165654101\" /></p>\n<h3 id=\"38-tensor-的降维和增维操作\"><a class=\"markdownIt-Anchor\" href=\"#38-tensor-的降维和增维操作\"></a> 3.8 Tensor 的降维和增维操作</h3>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#关于Tensor维度的操作有很多，</span></span><br><span class=\"line\"><span class=\"comment\">#比如张量降维 </span></span><br><span class=\"line\">torch.squeeze( )</span><br><span class=\"line\"><span class=\"comment\">#张量增维   </span></span><br><span class=\"line\">torch.unsqueeze( )</span><br><span class=\"line\"><span class=\"comment\">#张量拼接   </span></span><br><span class=\"line\">torch.cat( )</span><br><span class=\"line\"><span class=\"comment\">#张量扩大   </span></span><br><span class=\"line\">torch.Tensor.expand( )</span><br><span class=\"line\"><span class=\"comment\">#张量缩小   </span></span><br><span class=\"line\">torch.Tensor.narrow( )</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403165848913.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403165848913.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403165848913\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403165905983.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403165905983.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403165905983\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403165930249.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403165930249.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403165930249\" /></p>\n<h3 id=\"39-tensor-的裁剪操作\"><a class=\"markdownIt-Anchor\" href=\"#39-tensor-的裁剪操作\"></a> 3.9 Tensor 的裁剪操作</h3>\n<p>torch.clamp()对Tensor中的元素进行范围过滤，不符合条件的可以把它变换到范围内部（边界）上，常用于梯度裁剪（gradient clipping），即在发生梯度离散或者梯度爆炸时对梯度的处理。<br />\ntorch.clamp(input,min,max ,out=None) <img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403170024050.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403170024050.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403170024050\" /></p>\n<h3 id=\"310-tensor-的索引操作\"><a class=\"markdownIt-Anchor\" href=\"#310-tensor-的索引操作\"></a> 3.10 Tensor 的索引操作</h3>\n<p>Tensor支持与numpy.ndarray类似的索引操作，下面通过一些例子讲解常用的索引操作。如无特殊说明，索引出来的结果与原Tensor共享内存，即修改一个，另一个也会跟着改。</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403170150497.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403170150497.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403170150497\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403170213848.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403170213848.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403170213848\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403170246190.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403170246190.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403170246190\" /></p>\n<h3 id=\"311-把tensor-移到gpu上去\"><a class=\"markdownIt-Anchor\" href=\"#311-把tensor-移到gpu上去\"></a> 3.11 把Tensor 移到GPU上去</h3>\n<p>PyTorch提供了一个名为cuda( )的简单函数，将张量从CPU复制到GPU上。</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403170314792.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220403170314792.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220403170314792\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404101826313.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404101826313.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404101826313\" /></p>\n<h3 id=\"自动求导\"><a class=\"markdownIt-Anchor\" href=\"#自动求导\"></a> 自动求导</h3>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404101901457.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404101901457.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404101901457\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404101923121.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404101923121.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404101923121\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404102331503.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404102331503.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404102331503\" /></p>\n<ul>\n<li>autograd.grad()函数</li>\n<li>backward()函数</li>\n</ul>\n<h3 id=\"功能模块-数据集\"><a class=\"markdownIt-Anchor\" href=\"#功能模块-数据集\"></a> 功能模块  数据集</h3>\n<p>在处理任何机器学习问题之前都需要数据读取，并进行预处理。<br />\nPyTorch 提供了很多工具使得数据的读取和预处理变得很容易。<br />\n想让PyTorch能读取我们自己的数据，首先要了解pytroch读取图片的机制和流程。<br />\n主要使用以下两个类：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.utils.data.Dataset</span><br><span class=\"line\">torch.utils.data.DataLoader </span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>第一步：定义一个子类，继承Dataset类， 重写__len()__, __getitem()__ 方法。</p>\n<p>第二步： 实例化一个对象，对数据集进行变换</p>\n<p>第三步： 使用DataLoader进行包装，可视化等</p>\n<h3 id=\"功能模块-神经网络工具箱-nnmodel\"><a class=\"markdownIt-Anchor\" href=\"#功能模块-神经网络工具箱-nnmodel\"></a> 功能模块   神经网络工具箱 nn.model</h3>\n<p>使用torch.nn包中的工具来构建神经网络，构建一个神经网络需要以下几步：<br />\n定义神经网络的权重,搭建网络结构<br />\n遍历整个数据集进行训练</p>\n<ul>\n<li>将数据输入神经网络</li>\n<li>计算loss      正向传播</li>\n<li>计算网络权重的梯度       反向传播</li>\n<li>更新网络权重weight = weight - learning_rate * gradient</li>\n</ul>\n<p>在PyTorch里面编写神经网络，所有的层结构和损失函数都来自于torch.nn, 所有的模型构建都是从基类nn.Module 继承的，于是有了搭建结构的模板。</p>\n<p>全连接示例：<br />\n<img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404103227411.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404103227411.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404103227411\" /><br />\n卷积神经网络结构：</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404103405123.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404103405123.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404103405123\" /></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">net_name</span> (<span class=\"params\">nn.Module</span>) :</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self,in_ch=<span class=\"number\">3</span>, out_ch=<span class=\"number\">3</span>, features=<span class=\"number\">6</span></span>):</span></span><br><span class=\"line\"><span class=\"built_in\">super</span>(net_name, self).__init_()</span><br><span class=\"line\">self.cnn=nn.Sequential(</span><br><span class=\"line\">\t\tnn.Conv2d(in_channels=in_ch,out_channels=<span class=\"number\">6</span>,<span class=\"number\">1</span>),</span><br><span class=\"line\">\t\tnn.Conv2d(in_channels=<span class=\"number\">6</span>, out_channels=<span class=\"number\">6</span>, kernel_size=<span class=\"number\">3</span>),</span><br><span class=\"line\">\t\tnn.Conv2d(in_channels=<span class=\"number\">6</span>, out_channels=out_ch, <span class=\"number\">3</span>)  ) </span><br><span class=\"line\">     <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, x</span>) :</span></span><br><span class=\"line\">\tx = self.cnn(x)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">criterion = nn. CrossEntropyLoss()</span><br><span class=\"line\">criterion = nn. MSELoss()</span><br><span class=\"line\">loss = criterion(output, target)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"功能模块-优化器torchoptim\"><a class=\"markdownIt-Anchor\" href=\"#功能模块-优化器torchoptim\"></a> 功能模块   优化器torch.optim</h3>\n<p>在机器学习或者深度学习中，我们需要通过修改参数使得损失函数最小化(或最大化)，优化算法就是一种调整模型参数更新的策略。<br />\n优化算法分为两大类:</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404103644221.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404103644221.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404103644221\" /></p>\n<ul>\n<li>SGD</li>\n<li>ASGD</li>\n<li>Adadelta</li>\n<li>Adagrad</li>\n<li>Adam</li>\n<li>AdamW</li>\n<li>Adamax</li>\n<li>SparseAdam</li>\n<li>RMSprop</li>\n<li>Rprop</li>\n<li>LBFGS</li>\n</ul>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404103957915.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404103957915.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404103957915\" /></p>\n<ol>\n<li>optimizer. zeros() #需要先将梯度归零</li>\n<li>loss . backward()#反向传播，自动求导得到每个参数的梯度，</li>\n<li>optimizer . step()#最后就可以通过梯度做-一步参数更新。</li>\n</ol>\n<h3 id=\"计算机视觉工具包torchvision\"><a class=\"markdownIt-Anchor\" href=\"#计算机视觉工具包torchvision\"></a> 计算机视觉工具包：torchvision</h3>\n<p>主要包含以下三部分：</p>\n<ul>\n<li>models：提供深度学习中各种经典网络结构及预训练好的模型，包括AlexNet、VGG系列、ResNet系列、Inception系列等。</li>\n<li>datasets：提供常用的数据集下载，设计上都是继承torch.utils.data.Dataset，主要包括MNIST、CIFAR10/100、ImageNet、COCO等。</li>\n<li>transform：提供常用的数据预处理操作，主要包括对Tensor及PIL Image对象的操作。transforms中涵盖了大部分对Tensor和PIL Image的常用处理。</li>\n</ul>\n<p><strong>对PIL Image的操作包括：</strong><br />\n（1）Scale：调整图片尺寸，长宽比保持不变。<br />\n（2）CenterCrop、RandomCrop： 裁剪图片。<br />\n（3）Pad：填充。<br />\n（4）ToTensor：将PIL Image对象转成Tensor，会自动将[0, 255] 归一化至[0, 1]。<br />\n<strong>对Tensor的操作包括：</strong><br />\n（1）Normalize：标准化（减均值，除以标准差）。<br />\n（2）ToPILImage：将Tensor转为PIL Image对象。</p>\n<h2 id=\"第4章-线性回归和逻辑回归\"><a class=\"markdownIt-Anchor\" href=\"#第4章-线性回归和逻辑回归\"></a> 第4章 线性回归和逻辑回归</h2>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404104637284.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404104637284.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404104637284\" /></p>\n<h3 id=\"41-正向学习过程\"><a class=\"markdownIt-Anchor\" href=\"#41-正向学习过程\"></a> 4.1　正向学习过程</h3>\n<ul>\n<li>样本由输入层传入第一层layer，经第一层每个节点计算，每个节点得到一个输出，其输出继续作为下一层的输入，向前传播，直到输出层输出预测的结果。</li>\n<li>初次正向传播会先初始化网络的权值，得到的输出值并不一定正确值。</li>\n</ul>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404104800893.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404104800893.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404104800893\" /></p>\n<h3 id=\"42-反向调整过程\"><a class=\"markdownIt-Anchor\" href=\"#42-反向调整过程\"></a> 4.2　反向调整过程</h3>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404105059576.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404105059576.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404105059576\" /></p>\n<p>梯度<br />\n<img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404105139227.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404105139227.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404105139227\" /></p>\n<p><strong>梯度下降法：</strong><br />\n梯度下降法是最常用的神经网络优化算法。<br />\n若将代价函数简单可视化，代价函数相当于一个崎岖不平的盆地，有高峰也有低谷（最小值）。梯度下降的目标是取得最小值，每次沿着最陡峭的方向（梯度反方向），下降一定的距离（步长）。</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404105210801.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404105210801.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404105210801\" /></p>\n<p>梯度下降的步长不是一直不变的，当下降接近底部的时候，需要调整步子的大小，小心试探。当步子太大时，容易跨过最低点，在底部来回震荡。步子过小，下降速度会较慢。<br />\n在梯度下降过程中，节点i和j之间连接的权重$$W_{ji}$$的更新如下：其中η为学习速率，用于控制步长的变化。</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404105431331.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404105431331.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404105431331\" /></p>\n<p>在梯度下降法调优中，影响较大的三个因素为步长、初始值和归一化。</p>\n<ol>\n<li>步长：又称学习率，决定了梯度下降迭代过程中每一步沿梯度负方向前进的长度。也就是上述所说的沿最陡峭的位置走的那一步的长度。</li>\n<li>初始值：随机选取的值，当损失函数是非凸函数时，找到的可能是局部最优解，此时需要多测试几次，从局部最优解中找出最优解。当损失函数是凸函数时，得到的解就是最优解。</li>\n<li>归一化：若不进行归一化，会导致收敛速度很慢，从而形成“之”字形的路线。</li>\n</ol>\n<p><strong>反向传播的问题</strong></p>\n<ul>\n<li>梯度消失：由于sigmod函数在趋于无限大时，梯度会逐渐消失，随着传播深度的增加（如7层以上），残差传播到底层时已经变得太小，梯度的幅度也会急剧减小，导致浅层神经元的权重更新非常缓慢，无法有效进行学习。深层模型也就变成了前几层几乎固定，只能调节后几层的浅层模型，形成梯度弥散（vanishing gradient）。</li>\n<li>局部最优：深层模型的每个神经元都是非线性变换，代价函数是高度非凸函数，与浅层模型的目标函数不同。所以采用梯度下降的方法容易陷入局部最优。</li>\n</ul>\n<p><strong>优化算法的改进</strong></p>\n<ul>\n<li>激活函数选择</li>\n<li>梯度剪切、正则</li>\n<li>Batch Normalization</li>\n<li>残差结构</li>\n</ul>\n<h3 id=\"43-线性回归实现\"><a class=\"markdownIt-Anchor\" href=\"#43-线性回归实现\"></a> 4.3  线性回归实现</h3>\n<h4 id=\"一元线性回归\"><a class=\"markdownIt-Anchor\" href=\"#一元线性回归\"></a> 一元线性回归</h4>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404111102699.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404111102699.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404111102699\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404111132863.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404111132863.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404111132863\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404111223055.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404111223055.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404111223055\" /></p>\n<p>本节主要实现基于pytorch的线性回归实现：torch.nn.Linear( )</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, in_features: <span class=\"built_in\">int</span>, out_features: <span class=\"built_in\">int</span>, bias: <span class=\"built_in\">bool</span> = <span class=\"literal\">True</span></span>) -&gt; <span class=\"literal\">None</span>:</span></span><br><span class=\"line\">\t<span class=\"built_in\">super</span>(Linear, self).__init__()</span><br><span class=\"line\">\tself.in_features = in_features</span><br><span class=\"line\">\tself.out_features = out_features</span><br><span class=\"line\">\tself.weight = Parameter(torch.Tensor(out_features, in_features))</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> bias:</span><br><span class=\"line\">\t\tself.bias = Parameter(torch.Tensor(out_features))</span><br><span class=\"line\">\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\tself.register_parameter(<span class=\"string\">&#x27;bias&#x27;</span>, <span class=\"literal\">None</span>)</span><br><span class=\"line\">\tself.reset_parameters()</span><br><span class=\"line\">\t</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">reset_parameters</span>(<span class=\"params\">self</span>) -&gt; <span class=\"literal\">None</span>:</span></span><br><span class=\"line\">\tinit.kaiming_uniform_(self.weight, a=math.sqrt(<span class=\"number\">5</span>))</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> self.bias <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">\t\tfan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)</span><br><span class=\"line\">\t\tbound = <span class=\"number\">1</span> / math.sqrt(fan_in)</span><br><span class=\"line\">\t\tinit.uniform_(self.bias, -bound, bound)</span><br><span class=\"line\">\t\t</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, <span class=\"built_in\">input</span>: Tensor</span>) -&gt; Tensor:</span></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> F.linear(<span class=\"built_in\">input</span>, self.weight, self.bias)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404111512891.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404111512891.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404111512891\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404111603174.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404111603174.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404111603174\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404111629157.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404111629157.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404111629157\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404111639109.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404111639109.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404111639109\" /></p>\n<p>迭代的次数为200次。模型训练最终Loss到0.27（200）</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404111659851.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404111659851.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404111659851\" /></p>\n<p>model.eval()表示，只进行预测，不进行参数更新</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404111735145.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404111735145.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404111735145\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404111902334.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404111902334.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404111902334\" /></p>\n<h4 id=\"多元线性回归\"><a class=\"markdownIt-Anchor\" href=\"#多元线性回归\"></a> 多元线性回归</h4>\n<p>原始函数：$$𝑦=4𝑥<sup>3+3𝑥</sup>2+2𝑥+1$$</p>\n<p>假设拟合函数：$$𝑦=𝑤_3 𝑥^3+𝑤_2 𝑥^2+𝑤_1 𝑥+𝑏$$</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404114003109.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404114003109.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404114003109\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404114313746.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404114313746.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404114313746\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404114320946.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404114320946.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404114320946\" /></p>\n<p>迭代的终止条件los&lt;1e-3。模型训练大约100batch。</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404114347966.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404114347966.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404114347966\" /></p>\n<h3 id=\"44-逻辑回归实现\"><a class=\"markdownIt-Anchor\" href=\"#44-逻辑回归实现\"></a> 4.4  逻辑回归实现</h3>\n<p>逻辑回归是一种广义的回归模型，其与线性回归有着很多相似之处，模型的形式基本相同，都是 y = xw + b。<br />\n<strong>逻辑回归的损失函数-Cross Entropy Loss</strong></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404114552517.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404114552517.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404114552517\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404114617142.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404114617142.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404114617142\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404114647082.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404114647082.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404114647082\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404114700130.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404114700130.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404114700130\" /></p>\n<h2 id=\"第5章-全连接神经网络\"><a class=\"markdownIt-Anchor\" href=\"#第5章-全连接神经网络\"></a> 第5章 全连接神经网络</h2>\n<h3 id=\"理论和定理\"><a class=\"markdownIt-Anchor\" href=\"#理论和定理\"></a> 理论和定理</h3>\n<p>在机器学习中，有一些非常有名的理论或定理，对理解深度学习的内在特性非常有帮助．<br />\nPAC 学习理论：可能近似正确（ Probably Approximately Correct ， PAC ）学习理论<br />\n没有免费午餐定理：“具体问题具体分析”<br />\n奥卡姆剃刀原理:： “如无必要，勿增实体”<br />\n丑小鸭定理：特征筛选标准的重要性<br />\n归纳偏置（贝叶斯称为先验prior）:对问题的假设</p>\n<h3 id=\"51-全连接神经网络fc\"><a class=\"markdownIt-Anchor\" href=\"#51-全连接神经网络fc\"></a> 5.1 全连接神经网络（FC）</h3>\n<p>全连接神经网络的准则很简单：神经网络中除输入层之外的每个节点都和上一层的所有节点有连接。</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404193841852.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404193841852.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404193841852\" /></p>\n<h3 id=\"52多分类问题\"><a class=\"markdownIt-Anchor\" href=\"#52多分类问题\"></a> 5.2多分类问题</h3>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404193858760.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404193858760.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404193858760\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404194556867.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404194556867.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404194556867\" /></p>\n<h3 id=\"53-softmax与交叉熵\"><a class=\"markdownIt-Anchor\" href=\"#53-softmax与交叉熵\"></a> 5.3 softmax与交叉熵</h3>\n<p>softmax函数，又称归一化指数函数。<br />\n它是二分类函数Sigmoid在多分类上的推广，目的是将多分类的结果以概率的形式展现出来。</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404194658395.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404194658395.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404194658395\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404194712158.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404194712158.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404194712158\" /></p>\n<p>交叉熵是用来衡量两个概率分布的距离（也可以叫差别)。</p>\n<p>交叉熵数值越小说明两个概率分布越接近。<br />\n概率分布：即[0.1，0.5，0.2，0.1，0.1]，每个类别的概率都在0-1，且加起来为1。<br />\n若有两个概率分布p(x)和q(x)，它们的交叉熵为：<img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404194819323.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404194819323.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404194819323\" /></p>\n<p>假设有一个三分类的问题，某样本的真实分类标签是p=(1,0,0),<br />\n某个模型经过Softmax函数之后预测结果，若<br />\nq=(0.5,0.4, 0.1)，那么它们的交叉熵为<br />\nH(p,q) =-(1*log0.5+0*log0.4+0*log0.1)~0.3</p>\n<p>q=(0.8,0.1, 0.1)，那么它们的交叉熵为<br />\nH(p,q) =-(1*log0.8+0*log0.1+0*log0.1)~0.1</p>\n<p>由于后者更小，所以第二个模型参数性能更好</p>\n<h3 id=\"54-计算机视觉工具包torchvision\"><a class=\"markdownIt-Anchor\" href=\"#54-计算机视觉工具包torchvision\"></a> 5.4 计算机视觉工具包：torchvision</h3>\n<p>torchvision主要包含以下三部分：<br />\n<strong>models</strong>：提供深度学习中各种经典网络结构及预训练好的模型，包括AlexNet、VGG系列、ResNet系列、Inception系列等。</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404195042091.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404195042091.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404195042091\" /></p>\n<p>**datasets：**提供常用的数据集下载，设计上都是继承torch.utils.data.Dataset，主要包括MNIST、CIFAR10/100、ImageNet、COCO等。</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404195205464.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404195205464.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404195205464\" /></p>\n<p>**transform：**提供常用的数据预处理操作，主要包括对Tensor及PIL Image对象的操作。<br />\ntransforms中涵盖了大部分对Tensor和PIL Image的常用处理。</p>\n<p>对PIL Image的操作包括：<br />\n（1）Scale：调整图片尺寸，长宽比保持不变。<br />\n（2）CenterCrop、RandomCrop、RandomResizedCrop： 裁剪图片。<br />\n（3）Pad：填充。<br />\n（4）ToTensor：将PIL Image对象转成Tensor，会自动将[0, 255]归一化至[0, 1]。</p>\n<p>对Tensor的操作包括：<br />\n（1）Normalize：标准化（减均值，除以标准差）。<br />\n（2）ToPILImage：将Tensor转为PIL Image对象。</p>\n<p>torchvision还提供了两个常用的函数。<br />\nmake_gri()，它能将多张图片拼接成一个网格中。<br />\nsave_img()，它能将Tensor保存成图片。</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404195456263.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404195456263.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404195456263\" /></p>\n<h3 id=\"55-用全连接神经网络实现多分类\"><a class=\"markdownIt-Anchor\" href=\"#55-用全连接神经网络实现多分类\"></a> 5.5 用全连接神经网络实现多分类</h3>\n<p><strong>简易神经网络搭建</strong><br />\n我们从最简单的网络建起，<br />\n然后建立一个加入激励函数的网络，<br />\n最后建立一个加入批标准化函数的网络。</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404195818463.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404195818463.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404195818463\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404195834465.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404195834465.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404195834465\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404195841479.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404195841479.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404195841479\" /></p>\n<p><strong>批标准化函数</strong></p>\n<p>BN的作用：<br />\n（1）加快训练速度，减少了对学习率的要求，可以使用很大的学习率或者较小的学习率，算法也能够快速训练。<br />\n（2）增加模型的稳定性，有效减少梯度消失/爆炸，提高训练精度。<br />\n（3）BN具有轻微的正则化效果，在某些情况下可以取消 dropout 和 L2 正则项参数,或者采取更小的 L2 正则项约束参数。<br />\n（4）减少了人为选择参数的过程（权重的初始化方式，正则化方式的超参数的选择，学习率等）。</p>\n<p>BN的缺陷：<br />\n（1）无法使用小batch进行训练，小batch的均值和方差可能与整体训练样本偏差很大。<br />\n（2）无法在RNN等网络中使用。</p>\n<p><strong>全连接识别MNIST手写数字</strong></p>\n<p>MNIST数据集是一个非常出名的数据集，基本上很多网络都将其作为一个测试的标准，其来自美国国家标准与技术研究所，National Institute of Standards and Technology (NIST)。训练集（training set） 由来自250个不同人手写的数字构成，其中50% 是高中学生，50% 来自人口普查局 （the Census Bureau）的工作人员，一共有 60000 张图片。 测试集（testing set）也是同样比例的手写数字数据，一共有10000张图片。图5.11就是数据集中的一些数字图片。<img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404200526211.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404200526211.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404200526211\" /></p>\n<p>第一步：导入要用的包、定义超参数，比如训练中每批多少图片（batch_size），学习率（learning_rate），迭代次数num epoches。</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404201041749.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404201041749.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404201041749\" /></p>\n<p>第二步：下载MNIST数据集，同时对数据进行标准化预处理。</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404201122823.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404201122823.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404201122823\" /></p>\n<p>第三步：定义带有激活函数和批标准化的网络。</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404201128112.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404201128112.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404201128112\" /></p>\n<p>第四步：导入网络、定义损失函数和优化方法。</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404201137337.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404201137337.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404201137337\" /></p>\n<p>第五步：训练网络模型，</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404201143166.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404201143166.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404201143166\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404201200527.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404201200527.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404201200527\" /></p>\n<p>第六步：测试网络</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404201210092.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404201210092.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404201210092\" /></p>\n<p>训练误差和泛化误差：模型应关注降低泛化误差</p>\n<ul>\n<li>模型在训练数据集上表现出的误差叫做训练误差</li>\n<li>在任意一个测试数据样本上表现出的误差的期望值叫做泛化误差</li>\n</ul>\n<ol>\n<li>欠拟合：模型⽆法得到较低的训练误差。</li>\n<li>过拟合：模型的训练误差远小于它在测试数据集上的误差。</li>\n</ol>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404202232978.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404202232978.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404202232978\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404202248190.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404202248190.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404202248190\" /></p>\n<p>防止过拟合的方法:</p>\n<ul>\n<li>更多数据more data</li>\n<li>数据扩充 data argumentation</li>\n<li>降低模型复杂度：正则化regularization，浅层shallow</li>\n<li>Dropout</li>\n<li>早停技术early stopping</li>\n</ul>\n<p><strong>Dropout</strong><br />\n如何使用Dropout？在训练DNN网络的过程中，对于每一个神经元，以p的概率被随机的drop out，也就是将其值置零。这样，在该轮前传和反传的过程中，该神经元将失去作用，相当于不存在，如下图所示。<img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404202453198.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404202453198.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404202453198\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404202505450.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404202505450.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404202505450\" /></p>\n<p><strong>early stop</strong></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404202557446.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404202557446.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404202557446\" /></p>\n<p>原理</p>\n<ul>\n<li>将数据分为训练集和验证集</li>\n<li>每个epoch结束后（或每N个epoch后)： 在验证集上获取测试结果，随着epoch的增加，如果在验证集上发现测试误差上升，则停止训练；</li>\n<li>将停止之后的权重作为网络的最终参数。</li>\n</ul>\n<h2 id=\"第6章-卷积神经网络\"><a class=\"markdownIt-Anchor\" href=\"#第6章-卷积神经网络\"></a> 第6章 卷积神经网络</h2>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404205153425.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404205153425.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404205153425\" /></p>\n<h3 id=\"61深度前馈网络\"><a class=\"markdownIt-Anchor\" href=\"#61深度前馈网络\"></a> 6.1深度前馈网络</h3>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404205239026.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404205239026.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404205239026\" /></p>\n<p>在前馈神经网络中，各神经元分别属于不同的层。每一层的神经元可以接受到前一层的神经元信号，并产生信号输出到下一层。</p>\n<p>对于具有线性输出层和至少一个使用“挤压”性质的激活函数的隐藏层组成的前馈神经网络，只要其隐藏层神经元的数量足够，它可以以任意精度来近似任何从一个定义在实数空间中的有界闭集函数。</p>\n<h3 id=\"62-全连接网络和卷积神经网络\"><a class=\"markdownIt-Anchor\" href=\"#62-全连接网络和卷积神经网络\"></a> 6.2 全连接网络和卷积神经网络</h3>\n<p>整个网络中无反馈，信号从输入层向输出层单向传播，可用一个有向无环图表示。</p>\n<p>[3,4,4,1]<br />\n参数量=3x4+4x4+4x1=32</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404205737989.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220404205737989.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220404205737989\" /></p>\n<p>用全连接前馈网络来处理图像时，会存在两个问题：</p>\n<ul>\n<li>权重矩阵的参数非常多</li>\n<li>局部不变性特征不易提取</li>\n</ul>\n<h3 id=\"63卷积神经网络原理\"><a class=\"markdownIt-Anchor\" href=\"#63卷积神经网络原理\"></a> 6.3卷积神经网络原理</h3>\n<p><strong>卷积神经网络（ Convolutional Neural Network ， CNN 或 ConvNet ）</strong></p>\n<ul>\n<li>卷积神经网络最早主要用来处理图像信息．</li>\n<li>包含卷积计算且具有深度结构的前馈神经网络</li>\n<li>具有特征学习能力，能够按其阶层结构对输入信息进行平移不变分类。</li>\n<li>一种具有<strong>局部连接、权重共享</strong>等特性的深层前馈神经网络．<br />\nCNN主要应用：计算机视觉，图像和视频分析的各种任务上。近年来，卷积神经网络也应用到自然语言处理和推荐系统等领域。</li>\n</ul>\n<p><strong>生物学上感受野 Receptive Field</strong><br />\n在神经网络中，感受野的定义是：<br />\n卷积神经网络的每一层输出的特征图（Feature map）上的像素点在原图像上映射的区域大小。</p>\n<p>卷积神经网络的三个思想根源如下：</p>\n<ul>\n<li>局部性<img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405105331848.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405105331848.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405105331848\" /></li>\n<li>相同性 <img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405105342750.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405105342750.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405105342750\" /></li>\n<li>不变性<img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405105353372.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405105353372.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405105353372\" /></li>\n</ul>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405105411483.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405105411483.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405105411483\" /></p>\n<p><strong>一维卷积</strong></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405110442299.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405110442299.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405110442299\" /></p>\n<p><strong>二维卷积</strong></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405110512919.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405110512919.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405110512919\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405110540328.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405110540328.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405110540328\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405110835904.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405110835904.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405110835904\" /></p>\n<p><strong>互相关运算</strong><br />\n虽然卷积层得名于卷积（convolution）运算，但我们通常在卷积层中使用更加直观的互相关（cross-correlation）运算。在二维卷积层中，一个二维输⼊数组和⼀个二维核（kernel）数组通过互相关运算输出⼀个二维数组。</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405111008943.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405111008943.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405111008943\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405111110855.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405111110855.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405111110855\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405111204796.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405111204796.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405111204796\" /></p>\n<p><strong>卷积种类</strong><br />\n在卷积的标准定义基础上，还可以引入卷积核的<strong>滑动步长S和零填充P</strong>来增加卷积的多样性，可以更灵活地进行特征抽取。</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405111258416.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405111258416.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405111258416\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405111331494.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405111331494.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405111331494\" /></p>\n<p><strong>小结</strong></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405111425744.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405111425744.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405111425744\" /></p>\n<h3 id=\"64-典型的卷积神经网络\"><a class=\"markdownIt-Anchor\" href=\"#64-典型的卷积神经网络\"></a> 6.4 典型的卷积神经网络</h3>\n<p>卷积神经网络一般由<strong>卷积层、汇聚层和全连接层</strong>构成。<br />\n用卷积来代替全连接<img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405112138900.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405112138900.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405112138900\" /></p>\n<p><strong>卷积层</strong></p>\n<ul>\n<li>卷积层的每个神经元只与输入数据的一个局部区域连接，因此过滤器（卷积核）提取到的是图像的局部特征。</li>\n<li>卷积层的作用是提取局部区域的特征，不同的卷积核相当于不同的特征提取器．</li>\n<li>通常将神经元组织为三维结构的神经层，其大小为高度 M× 宽度 N× 深度 D ，由 D 个 M × N 大小的特征映射构成.</li>\n<li>特征映射（ Feature Map ）为一幅图像（一组特征映射）在经过卷积提取到的特征,每个特征映射可以作为一类抽取的图像特征．</li>\n</ul>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405112344984.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405112344984.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405112344984\" /><br />\n如果是灰度图像，就是有一个特征映射，输入层的深度 D = 1 ；<br />\n如果是彩色图像，分别有 RGB 三个颜色通道的特征映D=3.</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405112510903.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405112510903.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405112510903\" /></p>\n<p><strong>感受野计算</strong><br />\n<img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405112603153.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405112603153.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405112603153\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405112614518.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405112614518.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405112614518\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405113100510.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405113100510.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405113100510\" /></p>\n<p><strong>卷积层实现</strong><br />\n在卷积层中要设定的参数：<br />\n1.滤波器（卷积核）的长，宽，深度<br />\n2.步长<br />\n3.边界填充</p>\n<p><strong>torch.nn.Conv2d()函数</strong><br />\n<img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405113201652.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405113201652.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405113201652\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405113347988.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405113347988.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405113347988\" /></p>\n<p><strong>池化层</strong></p>\n<ul>\n<li>最大池化（max-pooling）</li>\n<li>平均池化（mean-pooling）</li>\n</ul>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405113503684.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405113503684.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405113503684\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405113543768.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405113543768.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405113543768\" /></p>\n<p>池化层的作用有：</p>\n<ol>\n<li>特征降维，避免过拟合</li>\n<li>空间不变性</li>\n<li>减少参数，降低训练难度。</li>\n</ol>\n<h3 id=\"第二部分-经典的卷积神经网络\"><a class=\"markdownIt-Anchor\" href=\"#第二部分-经典的卷积神经网络\"></a> 第二部分  经典的卷积神经网络</h3>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405114153967.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405114153967.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405114153967\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405114209637.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405114209637.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405114209637\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405114221325.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405114221325.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405114221325\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405114245941.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405114245941.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405114245941\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405114310902.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405114310902.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405114310902\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405114325056.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405114325056.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405114325056\" /></p>\n<h3 id=\"65-经典cnn-lenet5\"><a class=\"markdownIt-Anchor\" href=\"#65-经典cnn-lenet5\"></a> 6.5 经典CNN-LeNet5</h3>\n<p>LeNet神经网络由深度学习三巨头之一的Yan LeCun在1998年提出，他同时也是卷积神经网络 (CNN，Convolutional Neural Networks)之父。LeNet的实现确立了CNN的结构，自那时起，CNN的最基本的架构就定下来了：卷积层、池化层、全连接层。</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405114403469.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405114403469.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405114403469\" /><br />\nLenNet-5共有7层（不包括输入层），每层都包含不同数量的训练参数。<br />\n主要有2个卷积层、2个下抽样层（池化层）、3个全连接层3种连接方式。</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405114514425.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405114514425.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405114514425\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405114531231.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405114531231.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405114531231\" /></p>\n<h3 id=\"66-经典cnn-vggnet\"><a class=\"markdownIt-Anchor\" href=\"#66-经典cnn-vggnet\"></a> 6.6 经典CNN- VGGNet</h3>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405114607371.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405114607371.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405114607371\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405114634206.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405114634206.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405114634206\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405114646310.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405114646310.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405114646310\" /></p>\n<p><strong>VGGNet特点</strong></p>\n<ul>\n<li>小卷积核。卷积核全部替换为3x3（极少用了1x1）；</li>\n<li>小池化核。相比AlexNet的3x3的池化核，VGG全部为2x2的池化核；</li>\n<li>层数更深特征图更宽。基于前两点外，由于卷积核专注于扩大通道数、池化专注于缩小宽和高，使得模型架构上更深更宽的同时，计算量的增加放缓；</li>\n<li>全连接转卷积。网络测试阶段将训练阶段的三个全连接替换为三个卷积，测试重用训练时的参数，使得测试得到的全卷积网络因为没有全连接的限制，因而可以接收任意宽或高度的输入</li>\n<li>VGG耗费更多计算资源</li>\n</ul>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405115020964.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405115020964.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405115020964\" /></p>\n<p>实验表明 当层数达到一定数量比如20层左右以上，性能反而下降，小于这个层数会呈增长趋势</p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405115045479.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405115045479.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405115045479\" /></p>\n<h3 id=\"67-经典cnn-resnet\"><a class=\"markdownIt-Anchor\" href=\"#67-经典cnn-resnet\"></a> 6.7 经典CNN-ResNet</h3>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405115059897.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405115059897.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405115059897\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405115113194.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405115113194.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405115113194\" /></p>\n<p><img src=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405115123374.png\" class=\"lazyload placeholder\" data-srcset=\"PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/image-20220405115123374.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20220405115123374\" /></p>\n<h3 id=\"小结\"><a class=\"markdownIt-Anchor\" href=\"#小结\"></a> 小结</h3>\n<ul>\n<li>卷积的原理：卷积运算和互相关运算</li>\n<li>典型卷积神经网络：卷积层，池化层，全连接层</li>\n<li>经典卷积神经网络：LeNet,VGG,ResNet</li>\n<li>常用数据集：MNIST  CIFAR10/100 ImageNet</li>\n</ul>\n<h2 id=\"第7章-循环神经网络\"><a class=\"markdownIt-Anchor\" href=\"#第7章-循环神经网络\"></a> 第7章 循环神经网络</h2>\n<h2 id=\"第8章-生成式对抗网络\"><a class=\"markdownIt-Anchor\" href=\"#第8章-生成式对抗网络\"></a> 第8章 生成式对抗网络</h2>\n",
            "tags": [
                "深度学习"
            ]
        }
    ]
}