{
    "version": "https://jsonfeed.org/version/1",
    "title": "千里稻花应秀色 • All posts by \"tensorflow笔记\" category",
    "description": "blogs by SSR",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/2021/11/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/TensorFlow%E7%AC%94%E8%AE%B0/Ch1/",
            "url": "http://example.com/2021/11/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/TensorFlow%E7%AC%94%E8%AE%B0/Ch1/",
            "title": "第一讲 神经网络计算",
            "date_published": "2021-11-08T07:02:54.000Z",
            "content_html": "<h1 id=\"实验环境准备\"><a class=\"markdownIt-Anchor\" href=\"#实验环境准备\">#</a> 实验环境准备</h1>\n<ul>\n<li>Anoconda3.7</li>\n<li>TensorFlow2.1</li>\n<li>PyCharm</li>\n</ul>\n<h2 id=\"安装anoconda\"><a class=\"markdownIt-Anchor\" href=\"#安装anoconda\">#</a> 安装 Anoconda</h2>\n<p>官网下载安装，勾选添加到环境变量。</p>\n<h2 id=\"安装tensorflow21\"><a class=\"markdownIt-Anchor\" href=\"#安装tensorflow21\">#</a> 安装 TensorFlow2.1</h2>\n<p>1 打开安装好的 Anoconda Prompt</p>\n<p><img src=\"image-20211211133411024.png\" class=\"lazyload placeholder\" data-srcset=\"image-20211211133411024.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20211211133411024\"></p>\n<p>2 创建一个环境</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create -n TF2.1 python=3.7</span><br><span class=\"line\">y</span><br></pre></td></tr></table></figure>\n<p>3 进入新创建的环境</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda activate TF2.1</span><br></pre></td></tr></table></figure>\n<p>4 安装所需软件</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\">安装英伟达SDK10.1版本</span></span><br><span class=\"line\">conda install cudatoolkit=10.1</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\">安装英伟达深度学习软件包7.6版本</span></span><br><span class=\"line\">conda install cudann=7.6</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\">以上两条是为了支持N卡GPU，如果报错，暂时跳过</span></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\">安装TensorFlow，指定版本2.1</span></span><br><span class=\"line\">pip install tensorflow==2.1</span><br></pre></td></tr></table></figure>\n<p>5 验证安装</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python</span><br><span class=\"line\">import tensorflow as tf </span><br><span class=\"line\">tf.__version__</span><br></pre></td></tr></table></figure>\n<h2 id=\"安装pycharm\"><a class=\"markdownIt-Anchor\" href=\"#安装pycharm\">#</a> 安装 PyCharm</h2>\n<p>官网下载安装</p>\n<h2 id=\"新建工程\"><a class=\"markdownIt-Anchor\" href=\"#新建工程\">#</a> 新建工程</h2>\n<ul>\n<li>打开 PyCharm，新建工程，添加创建好的环境 TF2.1</li>\n<li>右键工程文件夹选择在文件管理器中打开（open in explorer）</li>\n<li>将课程代码拷贝进入工程目录</li>\n</ul>\n<p><img src=\"image-20211211150007347.png\" class=\"lazyload placeholder\" data-srcset=\"image-20211211150007347.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20211211150007347\"></p>\n<p><img src=\"image-20211211150025257.png\" class=\"lazyload placeholder\" data-srcset=\"image-20211211150025257.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20211211150025257\"></p>\n<h1 id=\"神经网络计算\"><a class=\"markdownIt-Anchor\" href=\"#神经网络计算\">#</a> 神经网络计算</h1>\n<p>本讲目标：学会神经网络计算过程，使用基于 TF2 原生代码搭建你的第一个的神经网络训练模型</p>\n<ul>\n<li>当今人工智能主流方向 —— 连接主义</li>\n<li>前向传播</li>\n<li>损失函数（初体会）</li>\n<li>梯度下降（初体会）</li>\n<li>学习率（初体会）</li>\n<li>反向传播更新参数</li>\n<li>Tensorflow2 常用函数</li>\n</ul>\n<h2 id=\"11-人工智能三学派\"><a class=\"markdownIt-Anchor\" href=\"#11-人工智能三学派\">#</a> 1.1 人工智能三学派</h2>\n<p>人工智能：让机器具备人的思维和意识。<br>\n<strong>人工智能三学派：</strong></p>\n<ul>\n<li>\n<p>行为主义：基于控制论，构建感知 - 动作控制系统。<br>\n（控制论，如平衡、行走、避障等自适应控制系统）</p>\n</li>\n<li>\n<p>符号主义：基于算数逻辑表达式，求解问题时先把问题描述为表达式，再求解表达式。<br>\n（可用公式描述、实现理性思维，如专家系统）</p>\n</li>\n<li>\n<p>连接主义：仿生学，模仿神经元连接关系。本课重点<br>\n（仿脑神经元连接，实现感性思维，如神经网络）</p>\n</li>\n</ul>\n<h2 id=\"12-基于连结主义的神经网络设计过程\"><a class=\"markdownIt-Anchor\" href=\"#12-基于连结主义的神经网络设计过程\">#</a> 1.2 基于连结主义的神经网络设计过程</h2>\n<p><img src=\"image-20211211140449793.png\" class=\"lazyload placeholder\" data-srcset=\"image-20211211140449793.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20211211140449793\"></p>\n<p>用计算机仿出神经网络连接关系，让计算机具备感性思维。</p>\n<p><img src=\"image-20211211140517854.png\" class=\"lazyload placeholder\" data-srcset=\"image-20211211140517854.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20211211140517854\"></p>\n<ul>\n<li>准备数据：采集大量 “特征 / 标签” 数据</li>\n<li>搭建网络：搭建神经网络结构</li>\n<li>优化参数：训练网络获取最佳参数（反传）</li>\n<li>应用网络：将网络保存为模型，输入新数据，输出分类或预测结果（前传）</li>\n</ul>\n<h2 id=\"13-tf2\"><a class=\"markdownIt-Anchor\" href=\"#13-tf2\">#</a> 1.3 TF2</h2>\n<p>TensorFlow2</p>\n<ul>\n<li>2019 年 3 月 Tensorflow2.0 测试版发布</li>\n<li>2019 年 10 月 Tensorflow2.0 正式版发布</li>\n<li>2020 年 1 月 Tensorflow2.1 发布</li>\n</ul>\n<p><strong>张量</strong></p>\n<ul>\n<li>张量（Tensor）：多维数组（列表）阶：张量的维数</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>维数</th>\n<th>阶</th>\n<th>名字</th>\n<th>例子</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0-D</td>\n<td>0</td>\n<td>标量 scalar</td>\n<td>s=123</td>\n</tr>\n<tr>\n<td>1-D</td>\n<td>1</td>\n<td>向量 vector</td>\n<td>v=[1,2,3]</td>\n</tr>\n<tr>\n<td>2-D</td>\n<td>2</td>\n<td>矩阵 matrix</td>\n<td>m=[[1,2,3],[4,5,6],[7,8,9]]</td>\n</tr>\n<tr>\n<td>3-D</td>\n<td>n</td>\n<td>张量 tensor</td>\n<td>t=[[[…]]]</td>\n</tr>\n</tbody>\n</table>\n<p><strong>数据类型</strong></p>\n<ul>\n<li>\n<p>tf.int, tf.float……<br>\ntf.int 32, tf.float32, tf.float64</p>\n</li>\n<li>\n<p>tf.bool<br>\ntf.constant([True, False])</p>\n</li>\n<li>\n<p>tf.string<br>\ntf.constant(“Hello, world!”)</p>\n</li>\n</ul>\n<p><strong>如何创建一个 Tensor</strong></p>\n<ul>\n<li>tf 创建一个张量<br>\n tf.constant (张量内容，dtype = 数据类型 (可选))</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflowas tf</span><br><span class=\"line\">a=tf.constant([<span class=\"number\">1</span>,<span class=\"number\">5</span>],dtype=tf.int64)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(a)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(a.dtype)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(a.shape)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>将 numpy 的数据类型转换为 Tensor 数据类型<br>\n tf. convert_to_tensor (数据名，dtype = 数据类型 (可选))</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflowas tf</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpyas np</span><br><span class=\"line\">a = np.arange(<span class=\"number\">0</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\">b = tf.convert_to_tensor( a, dtype=tf.int64 )</span><br><span class=\"line\"><span class=\"built_in\">print</span>(a)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(b)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>\n<p>创建全为 0 的张量<br>\n tf. zeros (维度)</p>\n</li>\n<li>\n<p>创建全为 1 的张量<br>\n tf. ones (维度)</p>\n</li>\n<li>\n<p>创建全为指定值的张量<br>\n tf. fill (维度，指定值)</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">维度：</span></span><br><span class=\"line\"><span class=\"string\">一维直接写个数</span></span><br><span class=\"line\"><span class=\"string\">二维用[行，列]</span></span><br><span class=\"line\"><span class=\"string\">多维用[n,m,j,k……]</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">a = tf.zeros([<span class=\"number\">2</span>, <span class=\"number\">3</span>])</span><br><span class=\"line\">b = tf.ones(<span class=\"number\">4</span>)</span><br><span class=\"line\">c = tf.fill([<span class=\"number\">2</span>, <span class=\"number\">2</span>], <span class=\"number\">9</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(a)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(b)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(c)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>\n<p>生成正态分布的随机数，默认均值为 0，标准差为 1<br>\ntf. random.normal (维度，mean = 均值，stddev = 标准差)</p>\n</li>\n<li>\n<p>生成截断式正态分布的随机数<br>\n tf. random.truncated_normal (维度，mean = 均值，stddev = 标准差)<br>\n 在 tf.truncated_normal 中如果随机生成数据的取值在（μ-2σ，μ+2σ）之外则重新进行生成，保证了生成值在均值附近。<br>\nμ：均值，σ：标准差</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">d = tf.random.normal([<span class=\"number\">2</span>, <span class=\"number\">2</span>], mean=<span class=\"number\">0.5</span>, stddev=<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(d)</span><br><span class=\"line\">e = tf.random.truncated_normal([<span class=\"number\">2</span>, <span class=\"number\">2</span>], mean=<span class=\"number\">0.5</span>, stddev=<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(e)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>生成均匀分布随机数 [minval, maxval)<br>\n tf. random. uniform (维度，minval = 最小值，maxval = 最大值)</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">f = tf.random.uniform([<span class=\"number\">2</span>, <span class=\"number\">2</span>], minval=<span class=\"number\">0</span>, maxval=<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(f)</span><br></pre></td></tr></table></figure>\n<h2 id=\"14-tf2常用函数\"><a class=\"markdownIt-Anchor\" href=\"#14-tf2常用函数\">#</a> 1.4 TF2 常用函数</h2>\n<ul>\n<li>强制 tensor 转换为该数据类型</li>\n</ul>\n<p>tf.cast (张量名，dtype = 数据类型)</p>\n<ul>\n<li>计算张量维度上元素的最小值</li>\n</ul>\n<p>tf.reduce_min (张量名)</p>\n<ul>\n<li>计算张量维度上元素的最大值</li>\n</ul>\n<p>tf.reduce_max (张量名)</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x1 = tf.constant([<span class=\"number\">1.</span>, <span class=\"number\">2.</span>, <span class=\"number\">3.</span>],dtype=tf.float64)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x1)</span><br><span class=\"line\">x2 = tf.cast(x1, tf.int32)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x2)</span><br><span class=\"line\"><span class=\"built_in\">print</span> (tf.reduce_min(x2), tf.reduce_max(x2))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">理解axis</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">在一个二维张量或数组中，可以通过调整</span></span><br><span class=\"line\"><span class=\"string\">axis 等于0或1 控制执行维度。</span></span><br><span class=\"line\"><span class=\"string\">axis=0代表跨行（经度，down)，而axis=1代表跨列（纬度，across)</span></span><br><span class=\"line\"><span class=\"string\">如果不指定axis，则所有元素参与计算。</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"image-20211211150529913.png\" class=\"lazyload placeholder\" data-srcset=\"image-20211211150529913.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20211211150529913\"></p>\n<ul>\n<li>计算张量沿着指定维度的平均值</li>\n</ul>\n<p>tf.reduce_mean (张量名，axis = 操作轴)</p>\n<ul>\n<li>计算张量沿着指定维度的和</li>\n</ul>\n<p>tf.reduce_sum (张量名，axis = 操作轴)</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=tf.constant( [ [ <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>],[ <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>] ] )</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(tf.reduce_mean( x ))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(tf.reduce_sum( x, axis=<span class=\"number\">1</span> ))</span><br></pre></td></tr></table></figure>\n<ul>\n<li>tf.Variable()</li>\n</ul>\n<p>tf.Variable () 将变量标记为 “可训练”，被标记的变量会在反向传播中记录梯度信息。神经网络训练中，常用该函数标记待训练参数。<br>\ntf.Variable (初始值)<br>\nw = tf.Variable(tf.random.normal([2, 2], mean=0, stddev=1))</p>\n<p><strong>TensorFlow 中的数学运算</strong></p>\n<ul>\n<li>对应元素的四则运算：tf.add，tf.subtract，tf.multiply，tf.divide</li>\n</ul>\n<p>只有维度相同的张量才可以做四则运算</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a = tf.ones([<span class=\"number\">1</span>, <span class=\"number\">3</span>])</span><br><span class=\"line\">b = tf.fill([<span class=\"number\">1</span>, <span class=\"number\">3</span>], <span class=\"number\">3.</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(a)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(b)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(tf.add(a,b))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(tf.subtract(a,b))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(tf.multiply(a,b))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(tf.divide(b,a))</span><br></pre></td></tr></table></figure>\n<ul>\n<li>平方、次方与开方： tf.square，tf.pow，tf.sqrt</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a = tf.fill([<span class=\"number\">1</span>, <span class=\"number\">2</span>], <span class=\"number\">3.</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(a)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(tf.<span class=\"built_in\">pow</span>(a, <span class=\"number\">3</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(tf.square(a))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(tf.sqrt(a))</span><br></pre></td></tr></table></figure>\n<ul>\n<li>矩阵乘：tf.matmul</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">tf.matmul(矩阵1，矩阵2)</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">a = tf.ones([<span class=\"number\">3</span>, <span class=\"number\">2</span>])</span><br><span class=\"line\">b = tf.fill([<span class=\"number\">2</span>, <span class=\"number\">3</span>], <span class=\"number\">3.</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(tf.matmul(a, b))</span><br></pre></td></tr></table></figure>\n<ul>\n<li>tf.data.Dataset.from_tensor_slices</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">切分传入张量的第一维度，生成输入特征/标签对，构建数据集</span></span><br><span class=\"line\"><span class=\"string\">data = tf.data.Dataset.from_tensor_slices((输入特征, 标签))</span></span><br><span class=\"line\"><span class=\"string\">（Numpy和Tensor格式都可用该语句读入数据）</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">features = tf.constant([<span class=\"number\">12</span>,<span class=\"number\">23</span>,<span class=\"number\">10</span>,<span class=\"number\">17</span>])</span><br><span class=\"line\">labels = tf.constant([<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>])</span><br><span class=\"line\">dataset = tf.data.Dataset.from_tensor_slices((features, labels))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(dataset)</span><br><span class=\"line\"><span class=\"keyword\">for</span> element <span class=\"keyword\">in</span> dataset:</span><br><span class=\"line\">  <span class=\"built_in\">print</span>(element)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>tf.GradientTape</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">with结构记录计算过程，gradient求出张量的梯度</span></span><br><span class=\"line\"><span class=\"string\">with tf.GradientTape( ) as tape:</span></span><br><span class=\"line\"><span class=\"string\">  若干个计算过程</span></span><br><span class=\"line\"><span class=\"string\">grad=tape.gradient(函数，对谁求导)</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">with</span> tf.GradientTape( ) <span class=\"keyword\">as</span> tape:</span><br><span class=\"line\">  w = tf.Variable(tf.constant(<span class=\"number\">3.0</span>))</span><br><span class=\"line\">  loss = tf.<span class=\"built_in\">pow</span>(w,<span class=\"number\">2</span>)</span><br><span class=\"line\">grad = tape.gradient(loss,w)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(grad)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>enumerate</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">enumerate是python的内建函数，它可遍历每个元素(如列表、元组</span></span><br><span class=\"line\"><span class=\"string\">或字符串)，组合为：索引 元素，常在for循环中使用。</span></span><br><span class=\"line\"><span class=\"string\">enumerate(列表名)</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">seq = [<span class=\"string\">&#x27;one&#x27;</span>, <span class=\"string\">&#x27;two&#x27;</span>, <span class=\"string\">&#x27;three&#x27;</span>]</span><br><span class=\"line\"><span class=\"keyword\">for</span> i, element <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(seq):</span><br><span class=\"line\">  <span class=\"built_in\">print</span>(i, element)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>tf.one_hot</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">独热编码（one-hot encoding）：在分类问题中，常用独热码做标签，标记类别：1表示是，0表示非.</span></span><br><span class=\"line\"><span class=\"string\">tf.one_hot (待转换数据, depth=几分类)</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">classes = <span class=\"number\">3</span></span><br><span class=\"line\">labels = tf.constant([<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">2</span>]) <span class=\"comment\"># 输入的元素值最小为0，最大为2</span></span><br><span class=\"line\">output = tf.one_hot( labels, depth=classes )</span><br><span class=\"line\"><span class=\"built_in\">print</span>(output)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<ul>\n<li>tf.nn.softmax</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">tf.nn.softmax(x) 使输出符合概率分布</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">y = tf.constant ( [<span class=\"number\">1.01</span>, <span class=\"number\">2.01</span>, -<span class=\"number\">0.66</span>] )</span><br><span class=\"line\">y_pro = tf.nn.softmax(y)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;After softmax, y_pro is:&quot;</span>, y_pro)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>assign_sub</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">赋值操作，更新参数的值并返回。</span></span><br><span class=\"line\"><span class=\"string\">调用assign_sub前，先用 tf.Variable 定义变量 w 为可训练（可自更新）。</span></span><br><span class=\"line\"><span class=\"string\">w.assign_sub (w要自减的内容) </span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">w = tf.Variable(<span class=\"number\">4</span>)</span><br><span class=\"line\">w.assign_sub(<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(w)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>tf.argmax</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">返回张量沿指定维度最大值的索引tf.argmax (张量名,axis=操作轴)</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">test = np.array([[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>], [<span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>], [<span class=\"number\">5</span>, <span class=\"number\">4</span>, <span class=\"number\">3</span>], [<span class=\"number\">8</span>, <span class=\"number\">7</span>, <span class=\"number\">2</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(test)</span><br><span class=\"line\"><span class=\"built_in\">print</span>( tf.argmax (test, axis=<span class=\"number\">0</span>)) <span class=\"comment\"># 返回每一列（经度）最大值的索引</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>( tf.argmax (test, axis=<span class=\"number\">1</span>)) <span class=\"comment\"># 返回每一行（纬度）最大值的索引</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"15-神经网络实现鸢尾花分类\"><a class=\"markdownIt-Anchor\" href=\"#15-神经网络实现鸢尾花分类\">#</a> 1.5 神经网络实现鸢尾花分类</h2>\n<h3 id=\"数据集介绍\"><a class=\"markdownIt-Anchor\" href=\"#数据集介绍\">#</a> 数据集介绍</h3>\n<p>共有数据 150 组，每组包括花萼长、花萼宽、花瓣长、花瓣宽 4 个输入特征。同时给出了，这一组特征对应的鸢尾花类别。类别包括 Setosa Iris（狗尾草鸢尾），Versicolour Iris（杂色鸢尾），Virginica Iris（弗吉尼亚鸢尾）三类，分别用数字 0，1，2 表示。</p>\n<p><img src=\"image-20211212172907509.png\" class=\"lazyload placeholder\" data-srcset=\"image-20211212172907509.png\" srcset=\"https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg\" alt=\"image-20211212172907509\"></p>\n<h3 id=\"准备数据\"><a class=\"markdownIt-Anchor\" href=\"#准备数据\">#</a> 准备数据</h3>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#数据集读入</span></span><br><span class=\"line\"><span class=\"comment\">#从sklearn包datasets 读入数据集：</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> datasets</span><br><span class=\"line\">x_data = datasets.load_iris().data 返回iris数据集所有输入特征</span><br><span class=\"line\">y_data = datasets.load_iris().target 返回iris数据集所有标签</span><br><span class=\"line\"><span class=\"comment\">#数据集乱序</span></span><br><span class=\"line\">np.random.seed(<span class=\"number\">116</span>) <span class=\"comment\"># 使用相同的seed，使输入特征/标签一一对应</span></span><br><span class=\"line\">np.random.shuffle(x_data)</span><br><span class=\"line\">np.random.seed(<span class=\"number\">116</span>)</span><br><span class=\"line\">np.random.shuffle(y_data)</span><br><span class=\"line\">tf.random.set_seed(<span class=\"number\">116</span>)</span><br><span class=\"line\"><span class=\"comment\">#数据集分出永不相见的训练集和测试集</span></span><br><span class=\"line\">x_train = x_data[:-<span class=\"number\">30</span>]</span><br><span class=\"line\">y_train = y_data[:-<span class=\"number\">30</span>]</span><br><span class=\"line\">x_test = x_data[-<span class=\"number\">30</span>:]</span><br><span class=\"line\">y_test = y_data[-<span class=\"number\">30</span>:]</span><br><span class=\"line\"><span class=\"comment\">#配成[输入特征，标签]对，每次喂入一小撮（batch）</span></span><br><span class=\"line\">train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(<span class=\"number\">32</span>)</span><br><span class=\"line\">test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(<span class=\"number\">32</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"搭建网络\"><a class=\"markdownIt-Anchor\" href=\"#搭建网络\">#</a> 搭建网络</h3>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#定义神经网路中所有可训练参数</span></span><br><span class=\"line\">w1 = tf.Variable(tf.random.truncated_normal([ <span class=\"number\">4</span>, <span class=\"number\">3</span> ], stddev=<span class=\"number\">0.1</span>, seed=<span class=\"number\">1</span>))</span><br><span class=\"line\">b1 = tf.Variable(tf.random.truncated_normal([ <span class=\"number\">3</span> ], stddev=<span class=\"number\">0.1</span>, seed=<span class=\"number\">1</span>))</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"参数优化\"><a class=\"markdownIt-Anchor\" href=\"#参数优化\">#</a> 参数优化</h3>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#嵌套循环迭代，with结构更新参数，显示当前loss</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(epoch): <span class=\"comment\">#数据集级别迭代</span></span><br><span class=\"line\">  <span class=\"keyword\">for</span> step, (x_train, y_train) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(train_db): <span class=\"comment\">#batch级别迭代</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> tf.GradientTape() <span class=\"keyword\">as</span> tape: <span class=\"comment\"># 记录梯度信息</span></span><br><span class=\"line\">      前向传播过程计算y</span><br><span class=\"line\">      计算总loss</span><br><span class=\"line\">    grads = tape.gradient(loss, [ w1, b1 ])</span><br><span class=\"line\">    w1.assign_sub(lr * grads[<span class=\"number\">0</span>]) <span class=\"comment\">#参数自更新</span></span><br><span class=\"line\">    b1.assign_sub(lr * grads[<span class=\"number\">1</span>])</span><br><span class=\"line\">  <span class=\"built_in\">print</span>(<span class=\"string\">&quot;Epoch &#123;&#125;, loss: &#123;&#125;&quot;</span>.<span class=\"built_in\">format</span>(epoch, loss_all/<span class=\"number\">4</span>))</span><br></pre></td></tr></table></figure>\n<h3 id=\"测试效果\"><a class=\"markdownIt-Anchor\" href=\"#测试效果\">#</a> 测试效果</h3>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 计算当前参数前向传播后的准确率，显示当前acc</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> x_test, y_test <span class=\"keyword\">in</span> test_db:</span><br><span class=\"line\">y = tf.matmul(h, w) + b <span class=\"comment\"># y为预测结果</span></span><br><span class=\"line\">y = tf.nn.softmax(y) <span class=\"comment\"># y符合概率分布</span></span><br><span class=\"line\">pred = tf.argmax(y, axis=<span class=\"number\">1</span>) <span class=\"comment\"># 返回y中最大值的索引，即预测的分类</span></span><br><span class=\"line\">pred = tf.cast(pred, dtype=y_test.dtype) <span class=\"comment\">#调整数据类型与标签一致</span></span><br><span class=\"line\">correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)</span><br><span class=\"line\">correct = tf.reduce_sum (correct) <span class=\"comment\"># 将每个batch的correct数加起来</span></span><br><span class=\"line\">total_correct += <span class=\"built_in\">int</span> (correct) <span class=\"comment\"># 将所有batch中的correct数加起来</span></span><br><span class=\"line\">total_number += x_test.shape [<span class=\"number\">0</span>]</span><br><span class=\"line\">acc = total_correct / total_number</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;test_acc:&quot;</span>, acc)</span><br></pre></td></tr></table></figure>\n<h3 id=\"accloss可视化\"><a class=\"markdownIt-Anchor\" href=\"#accloss可视化\">#</a> acc/loss 可视化</h3>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#acc / loss可视化</span></span><br><span class=\"line\">plt.title(<span class=\"string\">&#x27;Acc Curve&#x27;</span>) <span class=\"comment\"># 图片标题</span></span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;Epoch&#x27;</span>) <span class=\"comment\"># x轴名称</span></span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&#x27;Acc&#x27;</span>) <span class=\"comment\"># y轴名称</span></span><br><span class=\"line\">plt.plot(test_acc, label=<span class=\"string\">&quot;$Accuracy$&quot;</span>) <span class=\"comment\"># 逐点画出test_acc值并连线</span></span><br><span class=\"line\">plt.legend()</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n",
            "tags": [
                "机器学习"
            ]
        }
    ]
}